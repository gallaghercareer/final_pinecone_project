[youtube] Extracting URL: https://www.youtube.com/watch?v=CnAgB3A5OlU
[youtube] CnAgB3A5OlU: Downloading webpage
[youtube] CnAgB3A5OlU: Downloading android player API JSON
[info] CnAgB3A5OlU: Downloading 1 format(s): 251
[dashsegments] Total fragments: 3
[download] Destination: C:\users\jp\desktop\pinecone_project\final_pinecone_project\video
[download]   0.0% of ~  30.00MiB at    2.46KiB/s ETA 03:27:49 (frag 0/3)[download]   0.0% of ~  30.00MiB at    7.37KiB/s ETA 01:09:26 (frag 0/3)[download]   0.0% of ~  30.00MiB at   17.20KiB/s ETA 29:45 (frag 0/3)   [download]   0.0% of ~  30.00MiB at   36.77KiB/s ETA 13:55 (frag 0/3)[download]   0.1% of ~  30.00MiB at   74.89KiB/s ETA 06:49 (frag 0/3)[download]   0.2% of ~  30.00MiB at  143.56KiB/s ETA 03:33 (frag 0/3)[download]   0.4% of ~  30.00MiB at  274.42KiB/s ETA 01:51 (frag 0/3)[download]   0.8% of ~  30.00MiB at  521.80KiB/s ETA 00:58 (frag 0/3)[download]   1.7% of ~  30.00MiB at  963.04KiB/s ETA 00:31 (frag 0/3)[download]   3.3% of ~  30.00MiB at    1.78MiB/s ETA 00:16 (frag 0/3)[download]   6.7% of ~  30.00MiB at    3.35MiB/s ETA 00:08 (frag 0/3)[download]  13.3% of ~  30.00MiB at    6.03MiB/s ETA 00:04 (frag 0/3)[download]  26.7% of ~  30.00MiB at   10.19MiB/s ETA 00:02 (frag 0/3)[download]  33.3% of ~  30.00MiB at   11.94MiB/s ETA 00:01 (frag 0/3)[download]  33.3% of ~  30.00MiB at   11.91MiB/s ETA 00:01 (frag 1/3)[download]  33.3% of ~  30.00MiB at    5.37KiB/s ETA 00:02 (frag 1/3)[download]  33.3% of ~  30.00MiB at   16.11KiB/s ETA 00:02 (frag 1/3)[download]  33.4% of ~  30.00MiB at   37.59KiB/s ETA 00:02 (frag 1/3)[download]  33.4% of ~  30.00MiB at   80.10KiB/s ETA 00:02 (frag 1/3)[download]  33.4% of ~  30.00MiB at  160.45KiB/s ETA 00:02 (frag 1/3)[download]  33.5% of ~  30.00MiB at  284.04KiB/s ETA 00:02 (frag 1/3)[download]  33.7% of ~  30.00MiB at  522.35KiB/s ETA 00:02 (frag 1/3)[download]  34.2% of ~  30.00MiB at  949.96KiB/s ETA 00:02 (frag 1/3)[download]  35.0% of ~  30.00MiB at    1.56MiB/s ETA 00:02 (frag 1/3)[download]  36.7% of ~  30.00MiB at    2.82MiB/s ETA 00:02 (frag 1/3)[download]  40.0% of ~  30.00MiB at    5.17MiB/s ETA 00:01 (frag 1/3)[download]  46.7% of ~  30.00MiB at    8.95MiB/s ETA 00:01 (frag 1/3)[download]  60.0% of ~  30.00MiB at   14.41MiB/s ETA 00:00 (frag 1/3)[download]  66.7% of ~  30.00MiB at   16.46MiB/s ETA 00:00 (frag 1/3)[download]  66.7% of ~  30.00MiB at   16.44MiB/s ETA 00:00 (frag 2/3)[download]  95.4% of ~  20.97MiB at    2.67KiB/s ETA 00:00 (frag 2/3)[download]  95.4% of ~  20.97MiB at    7.99KiB/s ETA 00:00 (frag 2/3)[download]  95.4% of ~  20.97MiB at   18.64KiB/s ETA 00:00 (frag 2/3)[download]  95.5% of ~  20.97MiB at   39.84KiB/s ETA 00:00 (frag 2/3)[download]  95.5% of ~  20.97MiB at   79.60KiB/s ETA 00:00 (frag 2/3)[download]  95.7% of ~  20.97MiB at  154.27KiB/s ETA 00:00 (frag 2/3)[download]  96.0% of ~  20.97MiB at  297.89KiB/s ETA 00:00 (frag 2/3)[download]  96.6% of ~  20.97MiB at  568.84KiB/s ETA 00:00 (frag 2/3)[download]  97.8% of ~  20.97MiB at    1.03MiB/s ETA 00:00 (frag 2/3)[download] 100.0% of ~  20.97MiB at    1.87MiB/s ETA 00:00 (frag 2/3)[download] 100.0% of ~  20.97MiB at    1.86MiB/s ETA 00:00 (frag 3/3)[download] 100% of   20.97MiB in 00:00:01 at 10.62MiB/s              
[ExtractAudio] Destination: C:\users\jp\desktop\pinecone_project\final_pinecone_project\video.mp3
Deleting original file C:\users\jp\desktop\pinecone_project\final_pinecone_project\video (pass -k to keep)
None
outer reached
checking filename of files in directory tree
checking filename of files in directory tree
checking filename of files in directory tree
checking filename of files in directory tree
checking filename of files in directory tree
checking filename of files in directory tree
{'text': " With the introduction of OpenAI's new chat GPT endpoints, the line chain library have very quickly, unsurprisingly, added a ton of new support for chat. The reason for this is that unlike previous large language model endpoints, the new chat GPT endpoint is slightly different. It takes multiple inputs and therefore with line chain, this new sort of approach to calling large language models has been supported with its own set of objects and functions. So the new chat completion endpoint from OpenAI, it differs in the typical large language model endpoints in that you can essentially pass in three types of inputs that are defined or distinguished by these three different role types. These three different role types are system, user and assistant. The system or system message acts as the initial prompt to the model in order to set up its behavior for the rest of the interaction. So for example, with chat GPT, what you would find, before we even write anything, OpenAI have already passed in a system message to chat GPT, kind of telling it how to behave. Then after that, we have the user messages. So user messages is like what we write. So in chat GPT, we write something that's a user message. And then the other one is the assistant message. Those are the responses that we get from chat GPT. So the assistant is what chat GPT is producing. Now, when we use the endpoint, for every new interaction, we're feeding in a history of previous interactions as well. So we're always gonna have that system message at the top. We're going to have the user message followed by an assistant message, followed by user message, and so on and so on. So there is some difference with this new endpoint. And therefore, how we interact with chat GPT via line chain is also different. So let's just jump straight into it. Okay, so we get started with a pip install. Here we're doing line chain and OpenAI. There's only two libraries we use for this. Once those have been installed or updated, okay, so this is the latest version of OpenAI and line chain. So you do need to update if you haven't updated them very recently. Now, what we'll do is start by initializing the chat OpenAI object. For that, we do need an OpenAI API key. So you can click this link. There will be a link to this notebook. So you can follow along at the top of the video somewhere right now. But this will take us across to this page here. So this is platform.openai.com. And what we'll do is we'll go to view API keys. We go to here, create a new secret key, and then you copy that secret key. And what you do is run this cell. And you can see at the top here, it says, tells me OpenAI API key. If you're on Colab, it will appear just below the cell. And you just paste your API key into there. Okay, that sorts the API key into here. And then we come down here. And what we're going to do is initialize the chat OpenAI object. So for this, we're going to be using the chat GPT model. Now, by using this, we're essentially going to default to the latest version of chat GPT. So right now, the latest version is actually this here. Okay, so if you want to follow this video and the exact same responses in the future, you need to write this, but I will leave it like this. Basically, as they release new versions of this model, this will just default to the latest one. Now, when setting temperature to zero, that would make the completions fully deterministic as far as I could tell. So like running the same prompt twice, you would get the same output. Now, we've seen this. So the chats with chat GPT are kind of structured like this. So we have system, user, assistant, user, assistant. That final empty assistant prompt there is kind of telling the model, like now it's your time to respond. Right, so the model is just completing the end of this conversation. And the way that we format that is like this, okay? In line chain, they kind of mirror this format. It's very similar, but slightly different. So we have these system message objects, a human message object, and an AI message object. So to create this up here, we would write this, okay? So we have these messages, and it's just a list of these, okay, in the order that they have been passed in the conversation. Okay, so we're just passing, we are stopping user for human message and assistant for AI message. A system message is still a system message. And let's run this, okay? And let's run this. So this is going to generate a response from the chat GPT model, right? And I get this, so we have AI message, and it's pretty long, so what we can do is just print it out, and we get this. It's still pretty long, but we can go along like so. All right, cool. Now, if we take a look up here at the initial response before printing out the response content, come to the start and we can see that it's an AI message. So it's the same type of object as this here. So that means that we can actually just append this AI message, our response, directly to messages here, and that will create the full conversation, including the latest response, all right? So that's what we're doing here. Then from there, we can just continue the conversation. So we will create a new human message prompt, we'll add that to our messages, and then we'll send all of those to chat GPT. Okay, so now what was the next question I asked? Why do physicists believe it can produce a unified theory? This is talking about string theory up here. And then it goes in and starts explaining that they believe that string theory has potential to produce a unified theory because, you know, so on and so on. Okay, cool. Now, that is, I suppose, a core functionality of Linechain's new chat features, but there are a few other things that they've introduced alongside these. So we have a few new prompt templates. So these new prompt templates, we have like a AI message, human message, and system message prompt template. And these are kind of just an extension of the original prompt templates in Linechain. But when you use them, you have a couple of functions that will allow you to create your prompt template and output it as a system message, AI message, or user message. And you can also kind of like link them all together to create a list of messages that you then just pass straight into your chat endpoint. Now, I'm not super aware of like a huge number of reasons to use these right now, but these are part of the new features in Linechain for chat. So I figure it is important to share these. And if it seems like something that would actually help you with whatever it is you're building, then that's great. You now know how, or you will know how to use them. So we'll come down to here. What I'm doing is I'm making sure I'm using the March model here. So we're going to set up our first system message, and we're going to create a human message, our first input. Now, within this system message, I'm saying I want the responses to be no more than 100 characters long, including white space. And I want it to sign off every message with a random name like robot or bot rub, okay? We're just giving it tasks to do to see how well it follows these instructions. So run this, and now we make our first completion from this and let's see how it does with those instructions. Okay, so the length is way out. We asked for 100 at maximum, it's 154. And it also didn't give us a sign off there as well. Now, this is kind of just an issue with the current version of ChatGPT, okay? So with this version here, it's not very good at following system messages apparently, and it's kind of better to pass these instructions into your human message. But we might not want a user to have to specify these things. So maybe this is where we can use one of these prompt templates. So let's try. What we're gonna do is for every human message, we're gonna pass it into here, right? So we had that question before, hi AI, how are you? What is quantum physics? We'd pass it into input here. And what I'm going to do is after the question, I'm gonna say, can you keep the response to no more than 100 characters, including white space and sign off with a random name? So we create our prompt like this. So we have this line chain prompts chat, and we have human message prompt template. And we also need to use this chat prompt template. I feel like this is a little bit convoluted at the moment, but this is just how it is. So we're gonna go through anyway. So we have human message prompt template, and we're gonna have this, okay? This is just like a typical prompt template in line chain. Then once we have that human template, we need to pass it to this chat prompt template and from messages, right? And then in there, we pass in like a list of whatever messages we want, right? So I will give you another example soon, but we can also pass multiple messages here, like system message, human message, AI message, and so on, which I found some way of kind of using that. So I mean, I think that's kind of interesting at least. So we format that with some input. So we pass in this input here, how AI, how you, what is quantum physics? And let's see what we get from that. So we get this chat prompt value object, and it has a list of messages in there. First message and the only message is, hi AI, how are you, what is quantum physics? Right, so that's our input. And then we have, can you keep the response to no more than 100 characters including white space, sign off, so on and so on, right? So that is our template that is being applied based on this. All right, cool. Now, and we come down to here, and to use our human message prompt template as a typical message or human message, we actually need to use this here, right? So we take our chat prompt value, which we create here, and we can see here, and we can either pass it as two messages, that will give us the format that we need in order to pass it to chat GPT, or we can just create a string out of it, okay? So this would, I suppose, be pretty much the same as, like, using an F string. The only thing that's added onto there is we have this, like, human, right? Otherwise, it's literally just, like, taking this and converting it into a string. Okay, so let's see if this approach works. Here, I'm just kind of throwing it all together. So we have the chat prompt, the input, hi, hi, how are you doing? That's going to create this, and then I'm going to convert two messages and take the first message, which is the only message in there, which is essentially going to give us this human message, okay? And did I, can you keep the response to no more than 100 characters? And then here, I put 60 characters, so maybe I just put 100 here, and we'll try 60 later as well. So let's run that. All right, so you can see now it's listening. So we said 100 characters here, didn't really work, but then we did, we've also added it into this user or human message here, and now it's sticking to that, right? So length is good, let's keep going, and we also have this signed off with botrub. So that is working. By adding those instructions into the user message, we're getting better results. Okay, cool. In my last attempt, I actually got slightly over the character limit apparently. So I mean, we can run this again, and okay, so we've set the time to zero here, and because of that, we would expect the output to be the same every single time, so it's deterministic. So quantum physics is very small scale, I think it's every time it's outputting the same. Okay, cool. And then let's continue with this. So I want to show you, we can use this prompt templating method in order to build a initial set of messages that we can basically use as like examples, like few shot training for our chat model. So what we can do like here, we've done a hundred characters, right? Maybe we can go even lower, but maybe in that case, we might need to give some examples to the system, right? So let's do that. We're going to have this character limit, and we're going to have this sign off inputs or variables. For the human message, we're just going to pass in the input there, right? So for this first, we're not going to pass in those instructions, because we're actually going to create this human message, and we're also going to create following AI message as an example to the chat bot as to how it should respond, okay? And we put all of these together. So we have the system template, the human template, and the AI template. Like know that we're using AI message prompt template, human message prompt template, and system message prompt template for each of those. And what we do is create a list of messages. So it goes off to the system message first, the human message for second, and the AI message third. And these are the templates, right? So what we then do is we take our chat prompt, which is a list of these, and we format that prompt with our inputs. So we have the character limit, which we're going to set to 50, so half of what we had before, making it harder. We're going to say the sign off has to be this robot robot, and the input is going to be the same as before. And then we're giving an example response, right? So good is physics, small things. That example response is going to automatically have the sign off added to it. All right, so let's run this. Let's see what we get. So system message, you are helpful assistant. You keep responses, no one's 50 characters long. You sign off every message with robot robot, so we can see where those are being added there. Human message, how yeah, AI, how you, what is quantum physics? So it's, you know, because we're just passing the input in there. And then we have the AI message, good. It's physics of small things, robot robot, okay? Very short answer. And let's just see if that helps the system produce just very short answers. So we run this and we get atoms, electrons, photons, and then it does a sign off. So I think that's a pretty good response. Let's try again, right? So here we go slightly over. So we get like four characters over there. So maybe we can be more strict again. So what we can do is we add in that template that we used before where we add in the answer in less than the character limit, including white space. Okay, we're going to add that to our human message. So we're gonna create the human message like this. So the chat prompt template and so on and so on. Okay, cool. So is it like particle physics as far as before? Yeah. So we're asking the same question, but we're adding that on to the end. So is it like particle physics answering less than 50 characters, including white space? Then what I'm going to do is so within the messages right now, we have this query that we created before where we need to replace that query with our new modified query. So I'm going to remove the most recent message in messages. And now I'm going to send it with this new human prompt value, which is this kind of new version with those instructions added to the end. So let's have a look, make sure we have the right form. So system, human, AI, human, AI. That's the last correct response we got from the AI. And now we have the new modified human message. Okay, cool. So is it like this answering less than 50 characters? And now we pass that through our chat system again, and we get way shorter. So 28 is like, yes, similar. Because we're saying, we're telling it in the most recent query again, like you need to answer in less than 50 characters. All right, so what I mentioned before is that maybe this is a little bit convoluted. And that's not to say that there aren't use cases for this. It's just that it would be unfair of me to tell you all of this and be like, this is how you use it. And then like just miss something that could make things much easier in most, at least most use cases or simply use cases or something along those lines. Right, so I would say it's arguable as to whether all of the above that we just did would be any simpler than using an F string. So we have this input. Okay, cool, is it like particle physics? That's our most recent question, right? And we can just use an F string, right? So we have the F string here, and we have the human message, the content. And then we just say answering less than the character limit, which is set here, characters, including white space, right? And the result of that is basically the same that we have this, right? That's the same as all of this code here. So now all of this code, is that right? Yeah, plus this. So it depends, I don't know, it depends on your use case, like what you're doing, how you prefer to write this. But just be aware that you can also do this and you get the same result. So we can see again, popping the last message to remove the one that we created using the prompt template. And then I'm adding the one that we created using the F string approach, and we get this. All right, it's the same thing. There's no difference there. We can process it through chat GPT again, and we'll get the same response. Okay, so just wanted to make you aware of that. But yeah, that's it for this video. We've covered, I think the vast majority of the new chat features within Limechain. And naturally, like we saw at the end there, we don't need to use all of them, like the prompt templates. You can use, of course, if you have reason to, but it isn't needed if you have a simpler approach to doing these things. But yeah, it's cool to see this being implemented in Limechain, and although I haven't been through it yet, I'm hoping that there will be good integrations of these new chat features with like their conversation memory, their retrieval augmentation, and everything else within Limechain, which is that that's where the value of this sort of thing will come in. Right now, it's kind of like a simple wrapper on top of OpenAI's chat completion endpoint, but hopefully with all of the agent's conversation memory and retrieval augmentation components that Limechain offers, we'll get a tight integration between those, and that's why this will be useful. So that's it for this video. I hope all of this has been useful and interesting. But for now, thank you very much for watching, and I will see you again in the next one. Bye.", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 4.24, 'text': " With the introduction of OpenAI's new chat GPT endpoints,", 'tokens': [50364, 2022, 264, 9339, 295, 7238, 48698, 311, 777, 5081, 26039, 51, 917, 20552, 11, 50576], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 1, 'seek': 0, 'start': 4.24, 'end': 6.92, 'text': ' the line chain library have very quickly,', 'tokens': [50576, 264, 1622, 5021, 6405, 362, 588, 2661, 11, 50710], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 2, 'seek': 0, 'start': 6.92, 'end': 11.32, 'text': ' unsurprisingly, added a ton of new support for chat.', 'tokens': [50710, 2693, 374, 34408, 11, 3869, 257, 2952, 295, 777, 1406, 337, 5081, 13, 50930], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 3, 'seek': 0, 'start': 11.32, 'end': 14.24, 'text': ' The reason for this is that unlike previous', 'tokens': [50930, 440, 1778, 337, 341, 307, 300, 8343, 3894, 51076], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 4, 'seek': 0, 'start': 14.24, 'end': 16.080000000000002, 'text': ' large language model endpoints,', 'tokens': [51076, 2416, 2856, 2316, 917, 20552, 11, 51168], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 5, 'seek': 0, 'start': 16.080000000000002, 'end': 20.6, 'text': ' the new chat GPT endpoint is slightly different.', 'tokens': [51168, 264, 777, 5081, 26039, 51, 35795, 307, 4748, 819, 13, 51394], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 6, 'seek': 0, 'start': 20.6, 'end': 24.96, 'text': ' It takes multiple inputs and therefore with line chain,', 'tokens': [51394, 467, 2516, 3866, 15743, 293, 4412, 365, 1622, 5021, 11, 51612], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 7, 'seek': 0, 'start': 24.96, 'end': 29.66, 'text': ' this new sort of approach to calling large language models', 'tokens': [51612, 341, 777, 1333, 295, 3109, 281, 5141, 2416, 2856, 5245, 51847], 'temperature': 0.0, 'avg_logprob': -0.1339528978485422, 'compression_ratio': 1.6824034334763949, 'no_speech_prob': 0.0015718506183475256}, {'id': 8, 'seek': 2966, 'start': 29.66, 'end': 33.82, 'text': ' has been supported with its own set of objects', 'tokens': [50364, 575, 668, 8104, 365, 1080, 1065, 992, 295, 6565, 50572], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 9, 'seek': 2966, 'start': 33.82, 'end': 35.22, 'text': ' and functions.', 'tokens': [50572, 293, 6828, 13, 50642], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 10, 'seek': 2966, 'start': 35.22, 'end': 38.06, 'text': ' So the new chat completion endpoint from OpenAI,', 'tokens': [50642, 407, 264, 777, 5081, 19372, 35795, 490, 7238, 48698, 11, 50784], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 11, 'seek': 2966, 'start': 38.06, 'end': 41.74, 'text': ' it differs in the typical large language model endpoints', 'tokens': [50784, 309, 37761, 294, 264, 7476, 2416, 2856, 2316, 917, 20552, 50968], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 12, 'seek': 2966, 'start': 41.74, 'end': 46.379999999999995, 'text': ' in that you can essentially pass in three types of inputs', 'tokens': [50968, 294, 300, 291, 393, 4476, 1320, 294, 1045, 3467, 295, 15743, 51200], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 13, 'seek': 2966, 'start': 46.379999999999995, 'end': 49.06, 'text': ' that are defined or distinguished', 'tokens': [51200, 300, 366, 7642, 420, 21702, 51334], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 14, 'seek': 2966, 'start': 49.06, 'end': 50.980000000000004, 'text': ' by these three different role types.', 'tokens': [51334, 538, 613, 1045, 819, 3090, 3467, 13, 51430], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 15, 'seek': 2966, 'start': 50.980000000000004, 'end': 53.42, 'text': ' These three different role types are', 'tokens': [51430, 1981, 1045, 819, 3090, 3467, 366, 51552], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 16, 'seek': 2966, 'start': 53.42, 'end': 55.900000000000006, 'text': ' system, user and assistant.', 'tokens': [51552, 1185, 11, 4195, 293, 10994, 13, 51676], 'temperature': 0.0, 'avg_logprob': -0.1042501624973341, 'compression_ratio': 1.6869158878504673, 'no_speech_prob': 0.00019101161160506308}, {'id': 17, 'seek': 5590, 'start': 55.9, 'end': 60.9, 'text': ' The system or system message acts as the initial prompt', 'tokens': [50364, 440, 1185, 420, 1185, 3636, 10672, 382, 264, 5883, 12391, 50614], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 18, 'seek': 5590, 'start': 61.1, 'end': 64.7, 'text': ' to the model in order to set up its behavior', 'tokens': [50624, 281, 264, 2316, 294, 1668, 281, 992, 493, 1080, 5223, 50804], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 19, 'seek': 5590, 'start': 64.7, 'end': 66.22, 'text': ' for the rest of the interaction.', 'tokens': [50804, 337, 264, 1472, 295, 264, 9285, 13, 50880], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 20, 'seek': 5590, 'start': 66.22, 'end': 70.36, 'text': ' So for example, with chat GPT, what you would find,', 'tokens': [50880, 407, 337, 1365, 11, 365, 5081, 26039, 51, 11, 437, 291, 576, 915, 11, 51087], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 21, 'seek': 5590, 'start': 70.36, 'end': 73.22, 'text': ' before we even write anything,', 'tokens': [51087, 949, 321, 754, 2464, 1340, 11, 51230], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 22, 'seek': 5590, 'start': 73.22, 'end': 76.52, 'text': ' OpenAI have already passed in a system message', 'tokens': [51230, 7238, 48698, 362, 1217, 4678, 294, 257, 1185, 3636, 51395], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 23, 'seek': 5590, 'start': 76.52, 'end': 80.5, 'text': ' to chat GPT, kind of telling it how to behave.', 'tokens': [51395, 281, 5081, 26039, 51, 11, 733, 295, 3585, 309, 577, 281, 15158, 13, 51594], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 24, 'seek': 5590, 'start': 80.5, 'end': 84.06, 'text': ' Then after that, we have the user messages.', 'tokens': [51594, 1396, 934, 300, 11, 321, 362, 264, 4195, 7897, 13, 51772], 'temperature': 0.0, 'avg_logprob': -0.10973610828832253, 'compression_ratio': 1.6164383561643836, 'no_speech_prob': 0.0010001526679843664}, {'id': 25, 'seek': 8406, 'start': 84.10000000000001, 'end': 87.22, 'text': ' So user messages is like what we write.', 'tokens': [50366, 407, 4195, 7897, 307, 411, 437, 321, 2464, 13, 50522], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 26, 'seek': 8406, 'start': 87.22, 'end': 90.46000000000001, 'text': " So in chat GPT, we write something that's a user message.", 'tokens': [50522, 407, 294, 5081, 26039, 51, 11, 321, 2464, 746, 300, 311, 257, 4195, 3636, 13, 50684], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 27, 'seek': 8406, 'start': 90.46000000000001, 'end': 92.60000000000001, 'text': ' And then the other one is the assistant message.', 'tokens': [50684, 400, 550, 264, 661, 472, 307, 264, 10994, 3636, 13, 50791], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 28, 'seek': 8406, 'start': 92.60000000000001, 'end': 95.98, 'text': ' Those are the responses that we get from chat GPT.', 'tokens': [50791, 3950, 366, 264, 13019, 300, 321, 483, 490, 5081, 26039, 51, 13, 50960], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 29, 'seek': 8406, 'start': 95.98, 'end': 99.54, 'text': ' So the assistant is what chat GPT is producing.', 'tokens': [50960, 407, 264, 10994, 307, 437, 5081, 26039, 51, 307, 10501, 13, 51138], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 30, 'seek': 8406, 'start': 99.54, 'end': 102.02000000000001, 'text': ' Now, when we use the endpoint,', 'tokens': [51138, 823, 11, 562, 321, 764, 264, 35795, 11, 51262], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 31, 'seek': 8406, 'start': 102.02000000000001, 'end': 103.34, 'text': ' for every new interaction,', 'tokens': [51262, 337, 633, 777, 9285, 11, 51328], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 32, 'seek': 8406, 'start': 103.34, 'end': 106.22, 'text': " we're feeding in a history of previous interactions as well.", 'tokens': [51328, 321, 434, 12919, 294, 257, 2503, 295, 3894, 13280, 382, 731, 13, 51472], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 33, 'seek': 8406, 'start': 106.22, 'end': 109.34, 'text': " So we're always gonna have that system message at the top.", 'tokens': [51472, 407, 321, 434, 1009, 799, 362, 300, 1185, 3636, 412, 264, 1192, 13, 51628], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 34, 'seek': 8406, 'start': 109.34, 'end': 111.74000000000001, 'text': " We're going to have the user message", 'tokens': [51628, 492, 434, 516, 281, 362, 264, 4195, 3636, 51748], 'temperature': 0.0, 'avg_logprob': -0.12916896057128907, 'compression_ratio': 1.84, 'no_speech_prob': 0.0009694746695458889}, {'id': 35, 'seek': 11174, 'start': 111.74, 'end': 113.1, 'text': ' followed by an assistant message,', 'tokens': [50364, 6263, 538, 364, 10994, 3636, 11, 50432], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 36, 'seek': 11174, 'start': 113.1, 'end': 115.25999999999999, 'text': ' followed by user message, and so on and so on.', 'tokens': [50432, 6263, 538, 4195, 3636, 11, 293, 370, 322, 293, 370, 322, 13, 50540], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 37, 'seek': 11174, 'start': 115.25999999999999, 'end': 119.14, 'text': ' So there is some difference with this new endpoint.', 'tokens': [50540, 407, 456, 307, 512, 2649, 365, 341, 777, 35795, 13, 50734], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 38, 'seek': 11174, 'start': 119.14, 'end': 123.82, 'text': ' And therefore, how we interact with chat GPT', 'tokens': [50734, 400, 4412, 11, 577, 321, 4648, 365, 5081, 26039, 51, 50968], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 39, 'seek': 11174, 'start': 123.82, 'end': 126.17999999999999, 'text': ' via line chain is also different.', 'tokens': [50968, 5766, 1622, 5021, 307, 611, 819, 13, 51086], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 40, 'seek': 11174, 'start': 126.17999999999999, 'end': 129.45999999999998, 'text': " So let's just jump straight into it.", 'tokens': [51086, 407, 718, 311, 445, 3012, 2997, 666, 309, 13, 51250], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 41, 'seek': 11174, 'start': 129.45999999999998, 'end': 132.26, 'text': ' Okay, so we get started with a pip install.', 'tokens': [51250, 1033, 11, 370, 321, 483, 1409, 365, 257, 8489, 3625, 13, 51390], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 42, 'seek': 11174, 'start': 132.26, 'end': 134.5, 'text': " Here we're doing line chain and OpenAI.", 'tokens': [51390, 1692, 321, 434, 884, 1622, 5021, 293, 7238, 48698, 13, 51502], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 43, 'seek': 11174, 'start': 134.5, 'end': 136.42, 'text': " There's only two libraries we use for this.", 'tokens': [51502, 821, 311, 787, 732, 15148, 321, 764, 337, 341, 13, 51598], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 44, 'seek': 11174, 'start': 136.42, 'end': 139.06, 'text': ' Once those have been installed or updated,', 'tokens': [51598, 3443, 729, 362, 668, 8899, 420, 10588, 11, 51730], 'temperature': 0.0, 'avg_logprob': -0.16253227565599523, 'compression_ratio': 1.6431372549019607, 'no_speech_prob': 0.0015475900145247579}, {'id': 45, 'seek': 13906, 'start': 139.1, 'end': 142.26, 'text': ' okay, so this is the latest version of OpenAI and line chain.', 'tokens': [50366, 1392, 11, 370, 341, 307, 264, 6792, 3037, 295, 7238, 48698, 293, 1622, 5021, 13, 50524], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 46, 'seek': 13906, 'start': 142.26, 'end': 144.42000000000002, 'text': ' So you do need to update', 'tokens': [50524, 407, 291, 360, 643, 281, 5623, 50632], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 47, 'seek': 13906, 'start': 144.42000000000002, 'end': 147.42000000000002, 'text': " if you haven't updated them very recently.", 'tokens': [50632, 498, 291, 2378, 380, 10588, 552, 588, 3938, 13, 50782], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 48, 'seek': 13906, 'start': 147.42000000000002, 'end': 150.3, 'text': " Now, what we'll do is start by initializing", 'tokens': [50782, 823, 11, 437, 321, 603, 360, 307, 722, 538, 5883, 3319, 50926], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 49, 'seek': 13906, 'start': 150.3, 'end': 152.46, 'text': ' the chat OpenAI object.', 'tokens': [50926, 264, 5081, 7238, 48698, 2657, 13, 51034], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 50, 'seek': 13906, 'start': 152.46, 'end': 155.54, 'text': ' For that, we do need an OpenAI API key.', 'tokens': [51034, 1171, 300, 11, 321, 360, 643, 364, 7238, 48698, 9362, 2141, 13, 51188], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 51, 'seek': 13906, 'start': 155.54, 'end': 157.1, 'text': ' So you can click this link.', 'tokens': [51188, 407, 291, 393, 2052, 341, 2113, 13, 51266], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 52, 'seek': 13906, 'start': 157.1, 'end': 159.34, 'text': ' There will be a link to this notebook.', 'tokens': [51266, 821, 486, 312, 257, 2113, 281, 341, 21060, 13, 51378], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 53, 'seek': 13906, 'start': 159.34, 'end': 161.36, 'text': ' So you can follow along at the top of the video', 'tokens': [51378, 407, 291, 393, 1524, 2051, 412, 264, 1192, 295, 264, 960, 51479], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 54, 'seek': 13906, 'start': 161.36, 'end': 162.54, 'text': ' somewhere right now.', 'tokens': [51479, 4079, 558, 586, 13, 51538], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 55, 'seek': 13906, 'start': 162.54, 'end': 166.82, 'text': ' But this will take us across to this page here.', 'tokens': [51538, 583, 341, 486, 747, 505, 2108, 281, 341, 3028, 510, 13, 51752], 'temperature': 0.0, 'avg_logprob': -0.11299411010742187, 'compression_ratio': 1.6317829457364341, 'no_speech_prob': 0.000519171473570168}, {'id': 56, 'seek': 16682, 'start': 166.85999999999999, 'end': 170.38, 'text': ' So this is platform.openai.com.', 'tokens': [50366, 407, 341, 307, 3663, 13, 15752, 1301, 13, 1112, 13, 50542], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 57, 'seek': 16682, 'start': 170.38, 'end': 172.94, 'text': " And what we'll do is we'll go to view API keys.", 'tokens': [50542, 400, 437, 321, 603, 360, 307, 321, 603, 352, 281, 1910, 9362, 9317, 13, 50670], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 58, 'seek': 16682, 'start': 172.94, 'end': 175.57999999999998, 'text': ' We go to here, create a new secret key,', 'tokens': [50670, 492, 352, 281, 510, 11, 1884, 257, 777, 4054, 2141, 11, 50802], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 59, 'seek': 16682, 'start': 175.57999999999998, 'end': 177.57999999999998, 'text': ' and then you copy that secret key.', 'tokens': [50802, 293, 550, 291, 5055, 300, 4054, 2141, 13, 50902], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 60, 'seek': 16682, 'start': 177.57999999999998, 'end': 180.38, 'text': ' And what you do is run this cell.', 'tokens': [50902, 400, 437, 291, 360, 307, 1190, 341, 2815, 13, 51042], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 61, 'seek': 16682, 'start': 180.38, 'end': 181.82, 'text': ' And you can see at the top here,', 'tokens': [51042, 400, 291, 393, 536, 412, 264, 1192, 510, 11, 51114], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 62, 'seek': 16682, 'start': 181.82, 'end': 183.9, 'text': ' it says, tells me OpenAI API key.', 'tokens': [51114, 309, 1619, 11, 5112, 385, 7238, 48698, 9362, 2141, 13, 51218], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 63, 'seek': 16682, 'start': 183.9, 'end': 186.74, 'text': " If you're on Colab, it will appear just below the cell.", 'tokens': [51218, 759, 291, 434, 322, 4004, 455, 11, 309, 486, 4204, 445, 2507, 264, 2815, 13, 51360], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 64, 'seek': 16682, 'start': 186.74, 'end': 189.5, 'text': ' And you just paste your API key into there.', 'tokens': [51360, 400, 291, 445, 9163, 428, 9362, 2141, 666, 456, 13, 51498], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 65, 'seek': 16682, 'start': 189.5, 'end': 192.01999999999998, 'text': ' Okay, that sorts the API key into here.', 'tokens': [51498, 1033, 11, 300, 7527, 264, 9362, 2141, 666, 510, 13, 51624], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 66, 'seek': 16682, 'start': 192.01999999999998, 'end': 194.62, 'text': ' And then we come down here.', 'tokens': [51624, 400, 550, 321, 808, 760, 510, 13, 51754], 'temperature': 0.0, 'avg_logprob': -0.16450948784821226, 'compression_ratio': 1.7698744769874477, 'no_speech_prob': 0.0036477313842624426}, {'id': 67, 'seek': 19462, 'start': 194.62, 'end': 196.58, 'text': " And what we're going to do is initialize", 'tokens': [50364, 400, 437, 321, 434, 516, 281, 360, 307, 5883, 1125, 50462], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 68, 'seek': 19462, 'start': 196.58, 'end': 198.9, 'text': ' the chat OpenAI object.', 'tokens': [50462, 264, 5081, 7238, 48698, 2657, 13, 50578], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 69, 'seek': 19462, 'start': 198.9, 'end': 203.9, 'text': " So for this, we're going to be using the chat GPT model.", 'tokens': [50578, 407, 337, 341, 11, 321, 434, 516, 281, 312, 1228, 264, 5081, 26039, 51, 2316, 13, 50828], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 70, 'seek': 19462, 'start': 204.14000000000001, 'end': 207.42000000000002, 'text': " Now, by using this, we're essentially going to default", 'tokens': [50840, 823, 11, 538, 1228, 341, 11, 321, 434, 4476, 516, 281, 7576, 51004], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 71, 'seek': 19462, 'start': 207.42000000000002, 'end': 211.78, 'text': ' to the latest version of chat GPT.', 'tokens': [51004, 281, 264, 6792, 3037, 295, 5081, 26039, 51, 13, 51222], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 72, 'seek': 19462, 'start': 211.78, 'end': 216.78, 'text': ' So right now, the latest version is actually this here.', 'tokens': [51222, 407, 558, 586, 11, 264, 6792, 3037, 307, 767, 341, 510, 13, 51472], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 73, 'seek': 19462, 'start': 216.82, 'end': 220.58, 'text': ' Okay, so if you want to follow this video', 'tokens': [51474, 1033, 11, 370, 498, 291, 528, 281, 1524, 341, 960, 51662], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 74, 'seek': 19462, 'start': 220.58, 'end': 223.66, 'text': ' and the exact same responses in the future,', 'tokens': [51662, 293, 264, 1900, 912, 13019, 294, 264, 2027, 11, 51816], 'temperature': 0.0, 'avg_logprob': -0.10124399147781671, 'compression_ratio': 1.6729857819905214, 'no_speech_prob': 0.0005526318564079702}, {'id': 75, 'seek': 22366, 'start': 223.66, 'end': 227.34, 'text': ' you need to write this, but I will leave it like this.', 'tokens': [50364, 291, 643, 281, 2464, 341, 11, 457, 286, 486, 1856, 309, 411, 341, 13, 50548], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 76, 'seek': 22366, 'start': 227.34, 'end': 231.94, 'text': ' Basically, as they release new versions of this model,', 'tokens': [50548, 8537, 11, 382, 436, 4374, 777, 9606, 295, 341, 2316, 11, 50778], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 77, 'seek': 22366, 'start': 231.94, 'end': 233.94, 'text': ' this will just default to the latest one.', 'tokens': [50778, 341, 486, 445, 7576, 281, 264, 6792, 472, 13, 50878], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 78, 'seek': 22366, 'start': 233.94, 'end': 238.22, 'text': ' Now, when setting temperature to zero,', 'tokens': [50878, 823, 11, 562, 3287, 4292, 281, 4018, 11, 51092], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 79, 'seek': 22366, 'start': 238.22, 'end': 241.62, 'text': ' that would make the completions fully deterministic', 'tokens': [51092, 300, 576, 652, 264, 1557, 626, 4498, 15957, 3142, 51262], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 80, 'seek': 22366, 'start': 241.62, 'end': 243.22, 'text': ' as far as I could tell.', 'tokens': [51262, 382, 1400, 382, 286, 727, 980, 13, 51342], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 81, 'seek': 22366, 'start': 243.22, 'end': 245.57999999999998, 'text': ' So like running the same prompt twice,', 'tokens': [51342, 407, 411, 2614, 264, 912, 12391, 6091, 11, 51460], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 82, 'seek': 22366, 'start': 245.57999999999998, 'end': 247.1, 'text': ' you would get the same output.', 'tokens': [51460, 291, 576, 483, 264, 912, 5598, 13, 51536], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 83, 'seek': 22366, 'start': 247.1, 'end': 248.57999999999998, 'text': " Now, we've seen this.", 'tokens': [51536, 823, 11, 321, 600, 1612, 341, 13, 51610], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 84, 'seek': 22366, 'start': 248.57999999999998, 'end': 253.38, 'text': ' So the chats with chat GPT are kind of structured like this.', 'tokens': [51610, 407, 264, 38057, 365, 5081, 26039, 51, 366, 733, 295, 18519, 411, 341, 13, 51850], 'temperature': 0.0, 'avg_logprob': -0.12825488222056422, 'compression_ratio': 1.6303501945525292, 'no_speech_prob': 7.367765647359192e-05}, {'id': 85, 'seek': 25338, 'start': 253.38, 'end': 257.3, 'text': ' So we have system, user, assistant, user, assistant.', 'tokens': [50364, 407, 321, 362, 1185, 11, 4195, 11, 10994, 11, 4195, 11, 10994, 13, 50560], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 86, 'seek': 25338, 'start': 257.3, 'end': 260.58, 'text': ' That final empty assistant prompt there', 'tokens': [50560, 663, 2572, 6707, 10994, 12391, 456, 50724], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 87, 'seek': 25338, 'start': 260.58, 'end': 264.42, 'text': ' is kind of telling the model,', 'tokens': [50724, 307, 733, 295, 3585, 264, 2316, 11, 50916], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 88, 'seek': 25338, 'start': 264.42, 'end': 267.58, 'text': " like now it's your time to respond.", 'tokens': [50916, 411, 586, 309, 311, 428, 565, 281, 4196, 13, 51074], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 89, 'seek': 25338, 'start': 267.58, 'end': 269.9, 'text': ' Right, so the model is just completing', 'tokens': [51074, 1779, 11, 370, 264, 2316, 307, 445, 19472, 51190], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 90, 'seek': 25338, 'start': 269.9, 'end': 272.78, 'text': ' the end of this conversation.', 'tokens': [51190, 264, 917, 295, 341, 3761, 13, 51334], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 91, 'seek': 25338, 'start': 272.78, 'end': 276.82, 'text': ' And the way that we format that is like this, okay?', 'tokens': [51334, 400, 264, 636, 300, 321, 7877, 300, 307, 411, 341, 11, 1392, 30, 51536], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 92, 'seek': 25338, 'start': 276.82, 'end': 280.5, 'text': ' In line chain, they kind of mirror this format.', 'tokens': [51536, 682, 1622, 5021, 11, 436, 733, 295, 8013, 341, 7877, 13, 51720], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 93, 'seek': 25338, 'start': 280.5, 'end': 282.42, 'text': " It's very similar, but slightly different.", 'tokens': [51720, 467, 311, 588, 2531, 11, 457, 4748, 819, 13, 51816], 'temperature': 0.0, 'avg_logprob': -0.14156181672040155, 'compression_ratio': 1.6444444444444444, 'no_speech_prob': 0.00011233933764742687}, {'id': 94, 'seek': 28242, 'start': 282.42, 'end': 284.86, 'text': ' So we have these system message objects,', 'tokens': [50364, 407, 321, 362, 613, 1185, 3636, 6565, 11, 50486], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 95, 'seek': 28242, 'start': 284.86, 'end': 289.1, 'text': ' a human message object, and an AI message object.', 'tokens': [50486, 257, 1952, 3636, 2657, 11, 293, 364, 7318, 3636, 2657, 13, 50698], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 96, 'seek': 28242, 'start': 289.1, 'end': 293.5, 'text': ' So to create this up here, we would write this, okay?', 'tokens': [50698, 407, 281, 1884, 341, 493, 510, 11, 321, 576, 2464, 341, 11, 1392, 30, 50918], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 97, 'seek': 28242, 'start': 293.5, 'end': 296.58000000000004, 'text': " So we have these messages, and it's just a list of these,", 'tokens': [50918, 407, 321, 362, 613, 7897, 11, 293, 309, 311, 445, 257, 1329, 295, 613, 11, 51072], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 98, 'seek': 28242, 'start': 296.58000000000004, 'end': 299.90000000000003, 'text': ' okay, in the order that they have been passed', 'tokens': [51072, 1392, 11, 294, 264, 1668, 300, 436, 362, 668, 4678, 51238], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 99, 'seek': 28242, 'start': 299.90000000000003, 'end': 301.3, 'text': ' in the conversation.', 'tokens': [51238, 294, 264, 3761, 13, 51308], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 100, 'seek': 28242, 'start': 301.3, 'end': 304.22, 'text': " Okay, so we're just passing, we are stopping user", 'tokens': [51308, 1033, 11, 370, 321, 434, 445, 8437, 11, 321, 366, 12767, 4195, 51454], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 101, 'seek': 28242, 'start': 304.22, 'end': 307.18, 'text': ' for human message and assistant for AI message.', 'tokens': [51454, 337, 1952, 3636, 293, 10994, 337, 7318, 3636, 13, 51602], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 102, 'seek': 28242, 'start': 307.18, 'end': 309.86, 'text': ' A system message is still a system message.', 'tokens': [51602, 316, 1185, 3636, 307, 920, 257, 1185, 3636, 13, 51736], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 103, 'seek': 28242, 'start': 309.86, 'end': 312.02000000000004, 'text': " And let's run this, okay?", 'tokens': [51736, 400, 718, 311, 1190, 341, 11, 1392, 30, 51844], 'temperature': 0.0, 'avg_logprob': -0.13730823016557536, 'compression_ratio': 1.9251101321585904, 'no_speech_prob': 0.0005613137036561966}, {'id': 104, 'seek': 31202, 'start': 312.02, 'end': 313.9, 'text': " And let's run this.", 'tokens': [50364, 400, 718, 311, 1190, 341, 13, 50458], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 105, 'seek': 31202, 'start': 313.9, 'end': 316.38, 'text': ' So this is going to generate a response', 'tokens': [50458, 407, 341, 307, 516, 281, 8460, 257, 4134, 50582], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 106, 'seek': 31202, 'start': 316.38, 'end': 319.06, 'text': ' from the chat GPT model, right?', 'tokens': [50582, 490, 264, 5081, 26039, 51, 2316, 11, 558, 30, 50716], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 107, 'seek': 31202, 'start': 319.06, 'end': 322.14, 'text': ' And I get this, so we have AI message,', 'tokens': [50716, 400, 286, 483, 341, 11, 370, 321, 362, 7318, 3636, 11, 50870], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 108, 'seek': 31202, 'start': 322.14, 'end': 324.29999999999995, 'text': " and it's pretty long, so what we can do", 'tokens': [50870, 293, 309, 311, 1238, 938, 11, 370, 437, 321, 393, 360, 50978], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 109, 'seek': 31202, 'start': 324.29999999999995, 'end': 327.29999999999995, 'text': ' is just print it out, and we get this.', 'tokens': [50978, 307, 445, 4482, 309, 484, 11, 293, 321, 483, 341, 13, 51128], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 110, 'seek': 31202, 'start': 327.29999999999995, 'end': 331.58, 'text': " It's still pretty long, but we can go along like so.", 'tokens': [51128, 467, 311, 920, 1238, 938, 11, 457, 321, 393, 352, 2051, 411, 370, 13, 51342], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 111, 'seek': 31202, 'start': 331.58, 'end': 332.78, 'text': ' All right, cool.', 'tokens': [51342, 1057, 558, 11, 1627, 13, 51402], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 112, 'seek': 31202, 'start': 332.78, 'end': 337.74, 'text': ' Now, if we take a look up here at the initial response', 'tokens': [51402, 823, 11, 498, 321, 747, 257, 574, 493, 510, 412, 264, 5883, 4134, 51650], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 113, 'seek': 31202, 'start': 337.74, 'end': 340.29999999999995, 'text': ' before printing out the response content,', 'tokens': [51650, 949, 14699, 484, 264, 4134, 2701, 11, 51778], 'temperature': 0.0, 'avg_logprob': -0.13735120724409056, 'compression_ratio': 1.6277056277056277, 'no_speech_prob': 4.1979437810368836e-05}, {'id': 114, 'seek': 34030, 'start': 340.3, 'end': 343.38, 'text': " come to the start and we can see that it's an AI message.", 'tokens': [50364, 808, 281, 264, 722, 293, 321, 393, 536, 300, 309, 311, 364, 7318, 3636, 13, 50518], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 115, 'seek': 34030, 'start': 343.38, 'end': 346.82, 'text': " So it's the same type of object as this here.", 'tokens': [50518, 407, 309, 311, 264, 912, 2010, 295, 2657, 382, 341, 510, 13, 50690], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 116, 'seek': 34030, 'start': 346.82, 'end': 349.86, 'text': ' So that means that we can actually just append', 'tokens': [50690, 407, 300, 1355, 300, 321, 393, 767, 445, 34116, 50842], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 117, 'seek': 34030, 'start': 349.86, 'end': 354.62, 'text': ' this AI message, our response, directly to messages here,', 'tokens': [50842, 341, 7318, 3636, 11, 527, 4134, 11, 3838, 281, 7897, 510, 11, 51080], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 118, 'seek': 34030, 'start': 354.62, 'end': 358.66, 'text': ' and that will create the full conversation,', 'tokens': [51080, 293, 300, 486, 1884, 264, 1577, 3761, 11, 51282], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 119, 'seek': 34030, 'start': 358.66, 'end': 360.66, 'text': ' including the latest response, all right?', 'tokens': [51282, 3009, 264, 6792, 4134, 11, 439, 558, 30, 51382], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 120, 'seek': 34030, 'start': 360.66, 'end': 362.06, 'text': " So that's what we're doing here.", 'tokens': [51382, 407, 300, 311, 437, 321, 434, 884, 510, 13, 51452], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 121, 'seek': 34030, 'start': 362.06, 'end': 364.66, 'text': ' Then from there, we can just continue the conversation.', 'tokens': [51452, 1396, 490, 456, 11, 321, 393, 445, 2354, 264, 3761, 13, 51582], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 122, 'seek': 34030, 'start': 364.66, 'end': 368.14, 'text': ' So we will create a new human message prompt,', 'tokens': [51582, 407, 321, 486, 1884, 257, 777, 1952, 3636, 12391, 11, 51756], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 123, 'seek': 34030, 'start': 368.14, 'end': 369.86, 'text': " we'll add that to our messages,", 'tokens': [51756, 321, 603, 909, 300, 281, 527, 7897, 11, 51842], 'temperature': 0.0, 'avg_logprob': -0.0921643780123803, 'compression_ratio': 1.8514056224899598, 'no_speech_prob': 0.00020018834038637578}, {'id': 124, 'seek': 36986, 'start': 369.86, 'end': 372.66, 'text': " and then we'll send all of those to chat GPT.", 'tokens': [50364, 293, 550, 321, 603, 2845, 439, 295, 729, 281, 5081, 26039, 51, 13, 50504], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 125, 'seek': 36986, 'start': 372.66, 'end': 376.38, 'text': ' Okay, so now what was the next question I asked?', 'tokens': [50504, 1033, 11, 370, 586, 437, 390, 264, 958, 1168, 286, 2351, 30, 50690], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 126, 'seek': 36986, 'start': 376.38, 'end': 380.94, 'text': ' Why do physicists believe it can produce a unified theory?', 'tokens': [50690, 1545, 360, 48716, 1697, 309, 393, 5258, 257, 26787, 5261, 30, 50918], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 127, 'seek': 36986, 'start': 380.94, 'end': 383.54, 'text': ' This is talking about string theory up here.', 'tokens': [50918, 639, 307, 1417, 466, 6798, 5261, 493, 510, 13, 51048], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 128, 'seek': 36986, 'start': 383.54, 'end': 386.62, 'text': ' And then it goes in and starts explaining that they believe', 'tokens': [51048, 400, 550, 309, 1709, 294, 293, 3719, 13468, 300, 436, 1697, 51202], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 129, 'seek': 36986, 'start': 386.62, 'end': 389.3, 'text': ' that string theory has potential to produce a unified theory', 'tokens': [51202, 300, 6798, 5261, 575, 3995, 281, 5258, 257, 26787, 5261, 51336], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 130, 'seek': 36986, 'start': 389.3, 'end': 391.06, 'text': ' because, you know, so on and so on.', 'tokens': [51336, 570, 11, 291, 458, 11, 370, 322, 293, 370, 322, 13, 51424], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 131, 'seek': 36986, 'start': 391.06, 'end': 392.06, 'text': ' Okay, cool.', 'tokens': [51424, 1033, 11, 1627, 13, 51474], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 132, 'seek': 36986, 'start': 392.06, 'end': 395.26, 'text': ' Now, that is, I suppose, a core functionality', 'tokens': [51474, 823, 11, 300, 307, 11, 286, 7297, 11, 257, 4965, 14980, 51634], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 133, 'seek': 36986, 'start': 395.26, 'end': 398.74, 'text': " of Linechain's new chat features,", 'tokens': [51634, 295, 14670, 11509, 311, 777, 5081, 4122, 11, 51808], 'temperature': 0.0, 'avg_logprob': -0.1645130407614786, 'compression_ratio': 1.6679104477611941, 'no_speech_prob': 7.140810339478776e-05}, {'id': 134, 'seek': 39874, 'start': 398.74, 'end': 400.58, 'text': ' but there are a few other things', 'tokens': [50364, 457, 456, 366, 257, 1326, 661, 721, 50456], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 135, 'seek': 39874, 'start': 400.58, 'end': 403.06, 'text': " that they've introduced alongside these.", 'tokens': [50456, 300, 436, 600, 7268, 12385, 613, 13, 50580], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 136, 'seek': 39874, 'start': 403.06, 'end': 406.46000000000004, 'text': ' So we have a few new prompt templates.', 'tokens': [50580, 407, 321, 362, 257, 1326, 777, 12391, 21165, 13, 50750], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 137, 'seek': 39874, 'start': 406.46000000000004, 'end': 410.98, 'text': ' So these new prompt templates, we have like a AI message,', 'tokens': [50750, 407, 613, 777, 12391, 21165, 11, 321, 362, 411, 257, 7318, 3636, 11, 50976], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 138, 'seek': 39874, 'start': 410.98, 'end': 414.82, 'text': ' human message, and system message prompt template.', 'tokens': [50976, 1952, 3636, 11, 293, 1185, 3636, 12391, 12379, 13, 51168], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 139, 'seek': 39874, 'start': 414.82, 'end': 417.3, 'text': ' And these are kind of just an extension', 'tokens': [51168, 400, 613, 366, 733, 295, 445, 364, 10320, 51292], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 140, 'seek': 39874, 'start': 417.3, 'end': 420.74, 'text': ' of the original prompt templates in Linechain.', 'tokens': [51292, 295, 264, 3380, 12391, 21165, 294, 14670, 11509, 13, 51464], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 141, 'seek': 39874, 'start': 420.74, 'end': 423.90000000000003, 'text': ' But when you use them, you have a couple of functions', 'tokens': [51464, 583, 562, 291, 764, 552, 11, 291, 362, 257, 1916, 295, 6828, 51622], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 142, 'seek': 39874, 'start': 423.90000000000003, 'end': 427.5, 'text': ' that will allow you to create your prompt template', 'tokens': [51622, 300, 486, 2089, 291, 281, 1884, 428, 12391, 12379, 51802], 'temperature': 0.0, 'avg_logprob': -0.07799130967519816, 'compression_ratio': 1.84375, 'no_speech_prob': 5.736729144700803e-05}, {'id': 143, 'seek': 42750, 'start': 427.5, 'end': 430.7, 'text': ' and output it as a system message, AI message,', 'tokens': [50364, 293, 5598, 309, 382, 257, 1185, 3636, 11, 7318, 3636, 11, 50524], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 144, 'seek': 42750, 'start': 430.7, 'end': 432.14, 'text': ' or user message.', 'tokens': [50524, 420, 4195, 3636, 13, 50596], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 145, 'seek': 42750, 'start': 432.14, 'end': 434.94, 'text': ' And you can also kind of like link them all together', 'tokens': [50596, 400, 291, 393, 611, 733, 295, 411, 2113, 552, 439, 1214, 50736], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 146, 'seek': 42750, 'start': 434.94, 'end': 437.9, 'text': ' to create a list of messages that you then just pass', 'tokens': [50736, 281, 1884, 257, 1329, 295, 7897, 300, 291, 550, 445, 1320, 50884], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 147, 'seek': 42750, 'start': 437.9, 'end': 440.34, 'text': ' straight into your chat endpoint.', 'tokens': [50884, 2997, 666, 428, 5081, 35795, 13, 51006], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 148, 'seek': 42750, 'start': 440.34, 'end': 445.34, 'text': " Now, I'm not super aware of like a huge number", 'tokens': [51006, 823, 11, 286, 478, 406, 1687, 3650, 295, 411, 257, 2603, 1230, 51256], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 149, 'seek': 42750, 'start': 445.58, 'end': 448.9, 'text': ' of reasons to use these right now,', 'tokens': [51268, 295, 4112, 281, 764, 613, 558, 586, 11, 51434], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 150, 'seek': 42750, 'start': 448.9, 'end': 451.7, 'text': ' but these are part of the new features', 'tokens': [51434, 457, 613, 366, 644, 295, 264, 777, 4122, 51574], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 151, 'seek': 42750, 'start': 451.7, 'end': 452.94, 'text': ' in Linechain for chat.', 'tokens': [51574, 294, 14670, 11509, 337, 5081, 13, 51636], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 152, 'seek': 42750, 'start': 452.94, 'end': 456.02, 'text': ' So I figure it is important to share these.', 'tokens': [51636, 407, 286, 2573, 309, 307, 1021, 281, 2073, 613, 13, 51790], 'temperature': 0.0, 'avg_logprob': -0.08692009077159636, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.0005975324893370271}, {'id': 153, 'seek': 45602, 'start': 456.02, 'end': 461.02, 'text': ' And if it seems like something that would actually help you', 'tokens': [50364, 400, 498, 309, 2544, 411, 746, 300, 576, 767, 854, 291, 50614], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 154, 'seek': 45602, 'start': 461.09999999999997, 'end': 463.97999999999996, 'text': " with whatever it is you're building, then that's great.", 'tokens': [50618, 365, 2035, 309, 307, 291, 434, 2390, 11, 550, 300, 311, 869, 13, 50762], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 155, 'seek': 45602, 'start': 463.97999999999996, 'end': 467.14, 'text': ' You now know how, or you will know how to use them.', 'tokens': [50762, 509, 586, 458, 577, 11, 420, 291, 486, 458, 577, 281, 764, 552, 13, 50920], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 156, 'seek': 45602, 'start': 467.14, 'end': 469.14, 'text': " So we'll come down to here.", 'tokens': [50920, 407, 321, 603, 808, 760, 281, 510, 13, 51020], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 157, 'seek': 45602, 'start': 469.14, 'end': 471.09999999999997, 'text': " What I'm doing is I'm making sure", 'tokens': [51020, 708, 286, 478, 884, 307, 286, 478, 1455, 988, 51118], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 158, 'seek': 45602, 'start': 471.09999999999997, 'end': 473.58, 'text': " I'm using the March model here.", 'tokens': [51118, 286, 478, 1228, 264, 6129, 2316, 510, 13, 51242], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 159, 'seek': 45602, 'start': 473.58, 'end': 477.26, 'text': " So we're going to set up our first system message,", 'tokens': [51242, 407, 321, 434, 516, 281, 992, 493, 527, 700, 1185, 3636, 11, 51426], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 160, 'seek': 45602, 'start': 477.26, 'end': 480.09999999999997, 'text': " and we're going to create a human message,", 'tokens': [51426, 293, 321, 434, 516, 281, 1884, 257, 1952, 3636, 11, 51568], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 161, 'seek': 45602, 'start': 480.09999999999997, 'end': 481.82, 'text': ' our first input.', 'tokens': [51568, 527, 700, 4846, 13, 51654], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 162, 'seek': 45602, 'start': 481.82, 'end': 485.02, 'text': ' Now, within this system message,', 'tokens': [51654, 823, 11, 1951, 341, 1185, 3636, 11, 51814], 'temperature': 0.0, 'avg_logprob': -0.10846059605226678, 'compression_ratio': 1.7016806722689075, 'no_speech_prob': 0.00010070046118926257}, {'id': 163, 'seek': 48502, 'start': 485.02, 'end': 487.78, 'text': " I'm saying I want the responses to be no more", 'tokens': [50364, 286, 478, 1566, 286, 528, 264, 13019, 281, 312, 572, 544, 50502], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 164, 'seek': 48502, 'start': 487.78, 'end': 490.97999999999996, 'text': ' than 100 characters long, including white space.', 'tokens': [50502, 813, 2319, 4342, 938, 11, 3009, 2418, 1901, 13, 50662], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 165, 'seek': 48502, 'start': 490.97999999999996, 'end': 492.85999999999996, 'text': ' And I want it to sign off every message', 'tokens': [50662, 400, 286, 528, 309, 281, 1465, 766, 633, 3636, 50756], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 166, 'seek': 48502, 'start': 492.85999999999996, 'end': 497.21999999999997, 'text': ' with a random name like robot or bot rub, okay?', 'tokens': [50756, 365, 257, 4974, 1315, 411, 7881, 420, 10592, 5915, 11, 1392, 30, 50974], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 167, 'seek': 48502, 'start': 497.21999999999997, 'end': 498.94, 'text': " We're just giving it tasks to do", 'tokens': [50974, 492, 434, 445, 2902, 309, 9608, 281, 360, 51060], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 168, 'seek': 48502, 'start': 498.94, 'end': 501.21999999999997, 'text': ' to see how well it follows these instructions.', 'tokens': [51060, 281, 536, 577, 731, 309, 10002, 613, 9415, 13, 51174], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 169, 'seek': 48502, 'start': 501.21999999999997, 'end': 506.21999999999997, 'text': ' So run this, and now we make our first completion from this', 'tokens': [51174, 407, 1190, 341, 11, 293, 586, 321, 652, 527, 700, 19372, 490, 341, 51424], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 170, 'seek': 48502, 'start': 506.74, 'end': 509.21999999999997, 'text': " and let's see how it does with those instructions.", 'tokens': [51450, 293, 718, 311, 536, 577, 309, 775, 365, 729, 9415, 13, 51574], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 171, 'seek': 48502, 'start': 509.21999999999997, 'end': 512.42, 'text': ' Okay, so the length is way out.', 'tokens': [51574, 1033, 11, 370, 264, 4641, 307, 636, 484, 13, 51734], 'temperature': 0.0, 'avg_logprob': -0.14222474141163868, 'compression_ratio': 1.569767441860465, 'no_speech_prob': 9.914357360685244e-05}, {'id': 172, 'seek': 51242, 'start': 512.42, 'end': 515.54, 'text': " We asked for 100 at maximum, it's 154.", 'tokens': [50364, 492, 2351, 337, 2319, 412, 6674, 11, 309, 311, 2119, 19, 13, 50520], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 173, 'seek': 51242, 'start': 515.54, 'end': 520.4599999999999, 'text': " And it also didn't give us a sign off there as well.", 'tokens': [50520, 400, 309, 611, 994, 380, 976, 505, 257, 1465, 766, 456, 382, 731, 13, 50766], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 174, 'seek': 51242, 'start': 520.4599999999999, 'end': 523.9, 'text': ' Now, this is kind of just an issue', 'tokens': [50766, 823, 11, 341, 307, 733, 295, 445, 364, 2734, 50938], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 175, 'seek': 51242, 'start': 523.9, 'end': 528.26, 'text': ' with the current version of ChatGPT, okay?', 'tokens': [50938, 365, 264, 2190, 3037, 295, 27503, 38, 47, 51, 11, 1392, 30, 51156], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 176, 'seek': 51242, 'start': 528.26, 'end': 529.8199999999999, 'text': ' So with this version here,', 'tokens': [51156, 407, 365, 341, 3037, 510, 11, 51234], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 177, 'seek': 51242, 'start': 529.8199999999999, 'end': 532.78, 'text': " it's not very good at following system messages apparently,", 'tokens': [51234, 309, 311, 406, 588, 665, 412, 3480, 1185, 7897, 7970, 11, 51382], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 178, 'seek': 51242, 'start': 532.78, 'end': 535.26, 'text': " and it's kind of better to pass these instructions", 'tokens': [51382, 293, 309, 311, 733, 295, 1101, 281, 1320, 613, 9415, 51506], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 179, 'seek': 51242, 'start': 535.26, 'end': 536.5799999999999, 'text': ' into your human message.', 'tokens': [51506, 666, 428, 1952, 3636, 13, 51572], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 180, 'seek': 51242, 'start': 536.5799999999999, 'end': 539.2199999999999, 'text': ' But we might not want a user', 'tokens': [51572, 583, 321, 1062, 406, 528, 257, 4195, 51704], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 181, 'seek': 51242, 'start': 539.2199999999999, 'end': 541.62, 'text': ' to have to specify these things.', 'tokens': [51704, 281, 362, 281, 16500, 613, 721, 13, 51824], 'temperature': 0.0, 'avg_logprob': -0.12395047726838485, 'compression_ratio': 1.5212355212355213, 'no_speech_prob': 0.0008293407154269516}, {'id': 182, 'seek': 54162, 'start': 541.62, 'end': 543.82, 'text': ' So maybe this is where we can use', 'tokens': [50364, 407, 1310, 341, 307, 689, 321, 393, 764, 50474], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 183, 'seek': 54162, 'start': 543.82, 'end': 545.86, 'text': ' one of these prompt templates.', 'tokens': [50474, 472, 295, 613, 12391, 21165, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 184, 'seek': 54162, 'start': 545.86, 'end': 546.74, 'text': " So let's try.", 'tokens': [50576, 407, 718, 311, 853, 13, 50620], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 185, 'seek': 54162, 'start': 546.74, 'end': 551.18, 'text': " What we're gonna do is for every human message,", 'tokens': [50620, 708, 321, 434, 799, 360, 307, 337, 633, 1952, 3636, 11, 50842], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 186, 'seek': 54162, 'start': 551.18, 'end': 552.94, 'text': " we're gonna pass it into here, right?", 'tokens': [50842, 321, 434, 799, 1320, 309, 666, 510, 11, 558, 30, 50930], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 187, 'seek': 54162, 'start': 552.94, 'end': 554.78, 'text': ' So we had that question before,', 'tokens': [50930, 407, 321, 632, 300, 1168, 949, 11, 51022], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 188, 'seek': 54162, 'start': 554.78, 'end': 556.22, 'text': ' hi AI, how are you?', 'tokens': [51022, 4879, 7318, 11, 577, 366, 291, 30, 51094], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 189, 'seek': 54162, 'start': 556.22, 'end': 557.46, 'text': ' What is quantum physics?', 'tokens': [51094, 708, 307, 13018, 10649, 30, 51156], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 190, 'seek': 54162, 'start': 557.46, 'end': 559.18, 'text': " We'd pass it into input here.", 'tokens': [51156, 492, 1116, 1320, 309, 666, 4846, 510, 13, 51242], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 191, 'seek': 54162, 'start': 559.18, 'end': 561.9, 'text': " And what I'm going to do is after the question,", 'tokens': [51242, 400, 437, 286, 478, 516, 281, 360, 307, 934, 264, 1168, 11, 51378], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 192, 'seek': 54162, 'start': 561.9, 'end': 564.62, 'text': " I'm gonna say, can you keep the response", 'tokens': [51378, 286, 478, 799, 584, 11, 393, 291, 1066, 264, 4134, 51514], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 193, 'seek': 54162, 'start': 564.62, 'end': 569.38, 'text': ' to no more than 100 characters, including white space', 'tokens': [51514, 281, 572, 544, 813, 2319, 4342, 11, 3009, 2418, 1901, 51752], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 194, 'seek': 54162, 'start': 569.38, 'end': 571.22, 'text': ' and sign off with a random name?', 'tokens': [51752, 293, 1465, 766, 365, 257, 4974, 1315, 30, 51844], 'temperature': 0.0, 'avg_logprob': -0.10191343095567491, 'compression_ratio': 1.5964285714285715, 'no_speech_prob': 0.0001794969866750762}, {'id': 195, 'seek': 57122, 'start': 571.22, 'end': 574.14, 'text': ' So we create our prompt like this.', 'tokens': [50364, 407, 321, 1884, 527, 12391, 411, 341, 13, 50510], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 196, 'seek': 57122, 'start': 574.14, 'end': 576.4200000000001, 'text': ' So we have this line chain prompts chat,', 'tokens': [50510, 407, 321, 362, 341, 1622, 5021, 41095, 5081, 11, 50624], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 197, 'seek': 57122, 'start': 576.4200000000001, 'end': 578.38, 'text': ' and we have human message prompt template.', 'tokens': [50624, 293, 321, 362, 1952, 3636, 12391, 12379, 13, 50722], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 198, 'seek': 57122, 'start': 578.38, 'end': 580.86, 'text': ' And we also need to use this chat prompt template.', 'tokens': [50722, 400, 321, 611, 643, 281, 764, 341, 5081, 12391, 12379, 13, 50846], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 199, 'seek': 57122, 'start': 580.86, 'end': 585.86, 'text': ' I feel like this is a little bit convoluted at the moment,', 'tokens': [50846, 286, 841, 411, 341, 307, 257, 707, 857, 3754, 2308, 292, 412, 264, 1623, 11, 51096], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 200, 'seek': 57122, 'start': 586.34, 'end': 587.7, 'text': ' but this is just how it is.', 'tokens': [51120, 457, 341, 307, 445, 577, 309, 307, 13, 51188], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 201, 'seek': 57122, 'start': 587.7, 'end': 590.94, 'text': " So we're gonna go through anyway.", 'tokens': [51188, 407, 321, 434, 799, 352, 807, 4033, 13, 51350], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 202, 'seek': 57122, 'start': 590.94, 'end': 593.0600000000001, 'text': ' So we have human message prompt template,', 'tokens': [51350, 407, 321, 362, 1952, 3636, 12391, 12379, 11, 51456], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 203, 'seek': 57122, 'start': 593.0600000000001, 'end': 595.6600000000001, 'text': " and we're gonna have this, okay?", 'tokens': [51456, 293, 321, 434, 799, 362, 341, 11, 1392, 30, 51586], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 204, 'seek': 57122, 'start': 595.6600000000001, 'end': 599.26, 'text': ' This is just like a typical prompt template in line chain.', 'tokens': [51586, 639, 307, 445, 411, 257, 7476, 12391, 12379, 294, 1622, 5021, 13, 51766], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 205, 'seek': 57122, 'start': 599.26, 'end': 600.98, 'text': ' Then once we have that human template,', 'tokens': [51766, 1396, 1564, 321, 362, 300, 1952, 12379, 11, 51852], 'temperature': 0.0, 'avg_logprob': -0.12178943306207657, 'compression_ratio': 2.057777777777778, 'no_speech_prob': 4.539538349490613e-05}, {'id': 206, 'seek': 60098, 'start': 600.98, 'end': 605.02, 'text': ' we need to pass it to this chat prompt template', 'tokens': [50364, 321, 643, 281, 1320, 309, 281, 341, 5081, 12391, 12379, 50566], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 207, 'seek': 60098, 'start': 605.02, 'end': 607.14, 'text': ' and from messages, right?', 'tokens': [50566, 293, 490, 7897, 11, 558, 30, 50672], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 208, 'seek': 60098, 'start': 607.14, 'end': 610.82, 'text': ' And then in there, we pass in like a list', 'tokens': [50672, 400, 550, 294, 456, 11, 321, 1320, 294, 411, 257, 1329, 50856], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 209, 'seek': 60098, 'start': 610.82, 'end': 612.98, 'text': ' of whatever messages we want, right?', 'tokens': [50856, 295, 2035, 7897, 321, 528, 11, 558, 30, 50964], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 210, 'seek': 60098, 'start': 612.98, 'end': 615.5, 'text': ' So I will give you another example soon,', 'tokens': [50964, 407, 286, 486, 976, 291, 1071, 1365, 2321, 11, 51090], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 211, 'seek': 60098, 'start': 615.5, 'end': 618.86, 'text': ' but we can also pass multiple messages here,', 'tokens': [51090, 457, 321, 393, 611, 1320, 3866, 7897, 510, 11, 51258], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 212, 'seek': 60098, 'start': 618.86, 'end': 622.1800000000001, 'text': ' like system message, human message, AI message, and so on,', 'tokens': [51258, 411, 1185, 3636, 11, 1952, 3636, 11, 7318, 3636, 11, 293, 370, 322, 11, 51424], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 213, 'seek': 60098, 'start': 622.1800000000001, 'end': 627.1, 'text': ' which I found some way of kind of using that.', 'tokens': [51424, 597, 286, 1352, 512, 636, 295, 733, 295, 1228, 300, 13, 51670], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 214, 'seek': 60098, 'start': 627.1, 'end': 630.26, 'text': " So I mean, I think that's kind of interesting at least.", 'tokens': [51670, 407, 286, 914, 11, 286, 519, 300, 311, 733, 295, 1880, 412, 1935, 13, 51828], 'temperature': 0.0, 'avg_logprob': -0.09939396381378174, 'compression_ratio': 1.7347826086956522, 'no_speech_prob': 5.062967829871923e-05}, {'id': 215, 'seek': 63026, 'start': 630.26, 'end': 631.9, 'text': ' So we format that with some input.', 'tokens': [50364, 407, 321, 7877, 300, 365, 512, 4846, 13, 50446], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 216, 'seek': 63026, 'start': 631.9, 'end': 634.66, 'text': ' So we pass in this input here,', 'tokens': [50446, 407, 321, 1320, 294, 341, 4846, 510, 11, 50584], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 217, 'seek': 63026, 'start': 634.66, 'end': 636.9399999999999, 'text': ' how AI, how you, what is quantum physics?', 'tokens': [50584, 577, 7318, 11, 577, 291, 11, 437, 307, 13018, 10649, 30, 50698], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 218, 'seek': 63026, 'start': 636.9399999999999, 'end': 639.06, 'text': " And let's see what we get from that.", 'tokens': [50698, 400, 718, 311, 536, 437, 321, 483, 490, 300, 13, 50804], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 219, 'seek': 63026, 'start': 639.06, 'end': 642.42, 'text': ' So we get this chat prompt value object,', 'tokens': [50804, 407, 321, 483, 341, 5081, 12391, 2158, 2657, 11, 50972], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 220, 'seek': 63026, 'start': 642.42, 'end': 646.14, 'text': ' and it has a list of messages in there.', 'tokens': [50972, 293, 309, 575, 257, 1329, 295, 7897, 294, 456, 13, 51158], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 221, 'seek': 63026, 'start': 646.14, 'end': 648.8199999999999, 'text': ' First message and the only message is,', 'tokens': [51158, 2386, 3636, 293, 264, 787, 3636, 307, 11, 51292], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 222, 'seek': 63026, 'start': 648.8199999999999, 'end': 651.3, 'text': ' hi AI, how are you, what is quantum physics?', 'tokens': [51292, 4879, 7318, 11, 577, 366, 291, 11, 437, 307, 13018, 10649, 30, 51416], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 223, 'seek': 63026, 'start': 651.3, 'end': 652.74, 'text': " Right, so that's our input.", 'tokens': [51416, 1779, 11, 370, 300, 311, 527, 4846, 13, 51488], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 224, 'seek': 63026, 'start': 652.74, 'end': 654.34, 'text': ' And then we have, can you keep the response', 'tokens': [51488, 400, 550, 321, 362, 11, 393, 291, 1066, 264, 4134, 51568], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 225, 'seek': 63026, 'start': 654.34, 'end': 656.86, 'text': ' to no more than 100 characters including white space,', 'tokens': [51568, 281, 572, 544, 813, 2319, 4342, 3009, 2418, 1901, 11, 51694], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 226, 'seek': 63026, 'start': 656.86, 'end': 658.7, 'text': ' sign off, so on and so on, right?', 'tokens': [51694, 1465, 766, 11, 370, 322, 293, 370, 322, 11, 558, 30, 51786], 'temperature': 0.0, 'avg_logprob': -0.147181991144275, 'compression_ratio': 1.763157894736842, 'no_speech_prob': 8.746422827243805e-05}, {'id': 227, 'seek': 65870, 'start': 658.7, 'end': 663.22, 'text': ' So that is our template that is being applied based on this.', 'tokens': [50364, 407, 300, 307, 527, 12379, 300, 307, 885, 6456, 2361, 322, 341, 13, 50590], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 228, 'seek': 65870, 'start': 663.22, 'end': 664.5400000000001, 'text': ' All right, cool.', 'tokens': [50590, 1057, 558, 11, 1627, 13, 50656], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 229, 'seek': 65870, 'start': 665.38, 'end': 667.26, 'text': ' Now, and we come down to here,', 'tokens': [50698, 823, 11, 293, 321, 808, 760, 281, 510, 11, 50792], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 230, 'seek': 65870, 'start': 667.26, 'end': 671.46, 'text': ' and to use our human message prompt template', 'tokens': [50792, 293, 281, 764, 527, 1952, 3636, 12391, 12379, 51002], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 231, 'seek': 65870, 'start': 671.46, 'end': 676.1, 'text': ' as a typical message or human message,', 'tokens': [51002, 382, 257, 7476, 3636, 420, 1952, 3636, 11, 51234], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 232, 'seek': 65870, 'start': 676.1, 'end': 679.94, 'text': ' we actually need to use this here, right?', 'tokens': [51234, 321, 767, 643, 281, 764, 341, 510, 11, 558, 30, 51426], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 233, 'seek': 65870, 'start': 679.94, 'end': 683.9000000000001, 'text': ' So we take our chat prompt value, which we create here,', 'tokens': [51426, 407, 321, 747, 527, 5081, 12391, 2158, 11, 597, 321, 1884, 510, 11, 51624], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 234, 'seek': 65870, 'start': 683.9000000000001, 'end': 685.1800000000001, 'text': ' and we can see here,', 'tokens': [51624, 293, 321, 393, 536, 510, 11, 51688], 'temperature': 0.0, 'avg_logprob': -0.1615022341410319, 'compression_ratio': 1.7374301675977655, 'no_speech_prob': 0.000206588621949777}, {'id': 235, 'seek': 68518, 'start': 685.18, 'end': 690.02, 'text': ' and we can either pass it as two messages,', 'tokens': [50364, 293, 321, 393, 2139, 1320, 309, 382, 732, 7897, 11, 50606], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 236, 'seek': 68518, 'start': 690.02, 'end': 692.6999999999999, 'text': ' that will give us the format that we need', 'tokens': [50606, 300, 486, 976, 505, 264, 7877, 300, 321, 643, 50740], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 237, 'seek': 68518, 'start': 692.6999999999999, 'end': 695.06, 'text': ' in order to pass it to chat GPT,', 'tokens': [50740, 294, 1668, 281, 1320, 309, 281, 5081, 26039, 51, 11, 50858], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 238, 'seek': 68518, 'start': 695.06, 'end': 698.2199999999999, 'text': ' or we can just create a string out of it, okay?', 'tokens': [50858, 420, 321, 393, 445, 1884, 257, 6798, 484, 295, 309, 11, 1392, 30, 51016], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 239, 'seek': 68518, 'start': 698.2199999999999, 'end': 702.62, 'text': ' So this would, I suppose, be pretty much the same as,', 'tokens': [51016, 407, 341, 576, 11, 286, 7297, 11, 312, 1238, 709, 264, 912, 382, 11, 51236], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 240, 'seek': 68518, 'start': 702.62, 'end': 704.42, 'text': ' like, using an F string.', 'tokens': [51236, 411, 11, 1228, 364, 479, 6798, 13, 51326], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 241, 'seek': 68518, 'start': 704.42, 'end': 705.9, 'text': " The only thing that's added onto there", 'tokens': [51326, 440, 787, 551, 300, 311, 3869, 3911, 456, 51400], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 242, 'seek': 68518, 'start': 705.9, 'end': 708.8599999999999, 'text': ' is we have this, like, human, right?', 'tokens': [51400, 307, 321, 362, 341, 11, 411, 11, 1952, 11, 558, 30, 51548], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 243, 'seek': 68518, 'start': 708.8599999999999, 'end': 712.06, 'text': " Otherwise, it's literally just, like,", 'tokens': [51548, 10328, 11, 309, 311, 3736, 445, 11, 411, 11, 51708], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 244, 'seek': 68518, 'start': 712.06, 'end': 714.3, 'text': ' taking this and converting it into a string.', 'tokens': [51708, 1940, 341, 293, 29942, 309, 666, 257, 6798, 13, 51820], 'temperature': 0.0, 'avg_logprob': -0.1375761188444544, 'compression_ratio': 1.625, 'no_speech_prob': 0.0009691251907497644}, {'id': 245, 'seek': 71430, 'start': 714.3, 'end': 717.06, 'text': " Okay, so let's see if this approach works.", 'tokens': [50364, 1033, 11, 370, 718, 311, 536, 498, 341, 3109, 1985, 13, 50502], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 246, 'seek': 71430, 'start': 717.06, 'end': 719.6999999999999, 'text': " Here, I'm just kind of throwing it all together.", 'tokens': [50502, 1692, 11, 286, 478, 445, 733, 295, 10238, 309, 439, 1214, 13, 50634], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 247, 'seek': 71430, 'start': 719.6999999999999, 'end': 723.4599999999999, 'text': ' So we have the chat prompt, the input, hi, hi,', 'tokens': [50634, 407, 321, 362, 264, 5081, 12391, 11, 264, 4846, 11, 4879, 11, 4879, 11, 50822], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 248, 'seek': 71430, 'start': 723.4599999999999, 'end': 724.3, 'text': ' how are you doing?', 'tokens': [50822, 577, 366, 291, 884, 30, 50864], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 249, 'seek': 71430, 'start': 724.3, 'end': 727.54, 'text': " That's going to create this,", 'tokens': [50864, 663, 311, 516, 281, 1884, 341, 11, 51026], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 250, 'seek': 71430, 'start': 727.54, 'end': 729.66, 'text': " and then I'm going to convert two messages", 'tokens': [51026, 293, 550, 286, 478, 516, 281, 7620, 732, 7897, 51132], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 251, 'seek': 71430, 'start': 729.66, 'end': 731.02, 'text': ' and take the first message,', 'tokens': [51132, 293, 747, 264, 700, 3636, 11, 51200], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 252, 'seek': 71430, 'start': 731.02, 'end': 732.6999999999999, 'text': ' which is the only message in there,', 'tokens': [51200, 597, 307, 264, 787, 3636, 294, 456, 11, 51284], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 253, 'seek': 71430, 'start': 732.6999999999999, 'end': 736.4599999999999, 'text': ' which is essentially going to give us this human message,', 'tokens': [51284, 597, 307, 4476, 516, 281, 976, 505, 341, 1952, 3636, 11, 51472], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 254, 'seek': 71430, 'start': 736.4599999999999, 'end': 737.3, 'text': ' okay?', 'tokens': [51472, 1392, 30, 51514], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 255, 'seek': 71430, 'start': 737.3, 'end': 740.38, 'text': ' And did I, can you keep the response', 'tokens': [51514, 400, 630, 286, 11, 393, 291, 1066, 264, 4134, 51668], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 256, 'seek': 71430, 'start': 740.38, 'end': 743.18, 'text': ' to no more than 100 characters?', 'tokens': [51668, 281, 572, 544, 813, 2319, 4342, 30, 51808], 'temperature': 0.0, 'avg_logprob': -0.09692959522637795, 'compression_ratio': 1.6575875486381324, 'no_speech_prob': 5.1435737987048924e-05}, {'id': 257, 'seek': 74318, 'start': 743.18, 'end': 744.8199999999999, 'text': ' And then here, I put 60 characters,', 'tokens': [50364, 400, 550, 510, 11, 286, 829, 4060, 4342, 11, 50446], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 258, 'seek': 74318, 'start': 744.8199999999999, 'end': 748.9399999999999, 'text': ' so maybe I just put 100 here,', 'tokens': [50446, 370, 1310, 286, 445, 829, 2319, 510, 11, 50652], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 259, 'seek': 74318, 'start': 748.9399999999999, 'end': 750.9399999999999, 'text': " and we'll try 60 later as well.", 'tokens': [50652, 293, 321, 603, 853, 4060, 1780, 382, 731, 13, 50752], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 260, 'seek': 74318, 'start': 750.9399999999999, 'end': 752.42, 'text': " So let's run that.", 'tokens': [50752, 407, 718, 311, 1190, 300, 13, 50826], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 261, 'seek': 74318, 'start': 752.42, 'end': 755.02, 'text': " All right, so you can see now it's listening.", 'tokens': [50826, 1057, 558, 11, 370, 291, 393, 536, 586, 309, 311, 4764, 13, 50956], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 262, 'seek': 74318, 'start': 755.02, 'end': 757.8199999999999, 'text': " So we said 100 characters here, didn't really work,", 'tokens': [50956, 407, 321, 848, 2319, 4342, 510, 11, 994, 380, 534, 589, 11, 51096], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 263, 'seek': 74318, 'start': 757.8199999999999, 'end': 761.62, 'text': " but then we did, we've also added it into this user", 'tokens': [51096, 457, 550, 321, 630, 11, 321, 600, 611, 3869, 309, 666, 341, 4195, 51286], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 264, 'seek': 74318, 'start': 761.62, 'end': 763.18, 'text': ' or human message here,', 'tokens': [51286, 420, 1952, 3636, 510, 11, 51364], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 265, 'seek': 74318, 'start': 763.18, 'end': 765.4599999999999, 'text': " and now it's sticking to that, right?", 'tokens': [51364, 293, 586, 309, 311, 13465, 281, 300, 11, 558, 30, 51478], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 266, 'seek': 74318, 'start': 765.4599999999999, 'end': 767.2199999999999, 'text': " So length is good, let's keep going,", 'tokens': [51478, 407, 4641, 307, 665, 11, 718, 311, 1066, 516, 11, 51566], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 267, 'seek': 74318, 'start': 767.2199999999999, 'end': 770.02, 'text': ' and we also have this signed off with botrub.', 'tokens': [51566, 293, 321, 611, 362, 341, 8175, 766, 365, 10592, 81, 836, 13, 51706], 'temperature': 0.0, 'avg_logprob': -0.14156527886023887, 'compression_ratio': 1.653225806451613, 'no_speech_prob': 0.00029130157781764865}, {'id': 268, 'seek': 77002, 'start': 770.02, 'end': 771.9, 'text': ' So that is working.', 'tokens': [50364, 407, 300, 307, 1364, 13, 50458], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 269, 'seek': 77002, 'start': 771.9, 'end': 774.86, 'text': ' By adding those instructions into the user message,', 'tokens': [50458, 3146, 5127, 729, 9415, 666, 264, 4195, 3636, 11, 50606], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 270, 'seek': 77002, 'start': 774.86, 'end': 776.22, 'text': " we're getting better results.", 'tokens': [50606, 321, 434, 1242, 1101, 3542, 13, 50674], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 271, 'seek': 77002, 'start': 776.22, 'end': 777.54, 'text': ' Okay, cool.', 'tokens': [50674, 1033, 11, 1627, 13, 50740], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 272, 'seek': 77002, 'start': 777.54, 'end': 778.78, 'text': ' In my last attempt,', 'tokens': [50740, 682, 452, 1036, 5217, 11, 50802], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 273, 'seek': 77002, 'start': 778.78, 'end': 781.98, 'text': ' I actually got slightly over the character limit apparently.', 'tokens': [50802, 286, 767, 658, 4748, 670, 264, 2517, 4948, 7970, 13, 50962], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 274, 'seek': 77002, 'start': 781.98, 'end': 783.8199999999999, 'text': ' So I mean, we can run this again,', 'tokens': [50962, 407, 286, 914, 11, 321, 393, 1190, 341, 797, 11, 51054], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 275, 'seek': 77002, 'start': 783.8199999999999, 'end': 787.02, 'text': " and okay, so we've set the time to zero here,", 'tokens': [51054, 293, 1392, 11, 370, 321, 600, 992, 264, 565, 281, 4018, 510, 11, 51214], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 276, 'seek': 77002, 'start': 787.02, 'end': 788.34, 'text': ' and because of that,', 'tokens': [51214, 293, 570, 295, 300, 11, 51280], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 277, 'seek': 77002, 'start': 788.34, 'end': 791.6999999999999, 'text': ' we would expect the output to be the same every single time,', 'tokens': [51280, 321, 576, 2066, 264, 5598, 281, 312, 264, 912, 633, 2167, 565, 11, 51448], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 278, 'seek': 77002, 'start': 791.6999999999999, 'end': 793.1, 'text': " so it's deterministic.", 'tokens': [51448, 370, 309, 311, 15957, 3142, 13, 51518], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 279, 'seek': 77002, 'start': 793.1, 'end': 795.54, 'text': ' So quantum physics is very small scale,', 'tokens': [51518, 407, 13018, 10649, 307, 588, 1359, 4373, 11, 51640], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 280, 'seek': 77002, 'start': 795.54, 'end': 797.9399999999999, 'text': " I think it's every time it's outputting the same.", 'tokens': [51640, 286, 519, 309, 311, 633, 565, 309, 311, 5598, 783, 264, 912, 13, 51760], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 281, 'seek': 77002, 'start': 797.9399999999999, 'end': 798.78, 'text': ' Okay, cool.', 'tokens': [51760, 1033, 11, 1627, 13, 51802], 'temperature': 0.0, 'avg_logprob': -0.14211988787279062, 'compression_ratio': 1.693661971830986, 'no_speech_prob': 0.0007551517337560654}, {'id': 282, 'seek': 79878, 'start': 798.78, 'end': 801.98, 'text': " And then let's continue with this.", 'tokens': [50364, 400, 550, 718, 311, 2354, 365, 341, 13, 50524], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 283, 'seek': 79878, 'start': 801.98, 'end': 803.4599999999999, 'text': ' So I want to show you,', 'tokens': [50524, 407, 286, 528, 281, 855, 291, 11, 50598], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 284, 'seek': 79878, 'start': 803.4599999999999, 'end': 805.98, 'text': ' we can use this prompt templating method', 'tokens': [50598, 321, 393, 764, 341, 12391, 9100, 990, 3170, 50724], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 285, 'seek': 79878, 'start': 805.98, 'end': 808.62, 'text': ' in order to build a initial set of messages', 'tokens': [50724, 294, 1668, 281, 1322, 257, 5883, 992, 295, 7897, 50856], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 286, 'seek': 79878, 'start': 808.62, 'end': 812.06, 'text': ' that we can basically use as like examples,', 'tokens': [50856, 300, 321, 393, 1936, 764, 382, 411, 5110, 11, 51028], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 287, 'seek': 79878, 'start': 812.06, 'end': 816.02, 'text': ' like few shot training for our chat model.', 'tokens': [51028, 411, 1326, 3347, 3097, 337, 527, 5081, 2316, 13, 51226], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 288, 'seek': 79878, 'start': 816.02, 'end': 818.54, 'text': ' So what we can do like here,', 'tokens': [51226, 407, 437, 321, 393, 360, 411, 510, 11, 51352], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 289, 'seek': 79878, 'start': 818.54, 'end': 822.06, 'text': " we've done a hundred characters, right?", 'tokens': [51352, 321, 600, 1096, 257, 3262, 4342, 11, 558, 30, 51528], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 290, 'seek': 79878, 'start': 822.06, 'end': 824.1, 'text': ' Maybe we can go even lower,', 'tokens': [51528, 2704, 321, 393, 352, 754, 3126, 11, 51630], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 291, 'seek': 79878, 'start': 824.1, 'end': 825.4599999999999, 'text': ' but maybe in that case,', 'tokens': [51630, 457, 1310, 294, 300, 1389, 11, 51698], 'temperature': 0.0, 'avg_logprob': -0.08150953405043658, 'compression_ratio': 1.5695067264573992, 'no_speech_prob': 0.00020986171148251742}, {'id': 292, 'seek': 82546, 'start': 825.46, 'end': 829.38, 'text': ' we might need to give some examples to the system, right?', 'tokens': [50364, 321, 1062, 643, 281, 976, 512, 5110, 281, 264, 1185, 11, 558, 30, 50560], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 293, 'seek': 82546, 'start': 829.38, 'end': 831.26, 'text': " So let's do that.", 'tokens': [50560, 407, 718, 311, 360, 300, 13, 50654], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 294, 'seek': 82546, 'start': 831.26, 'end': 833.9000000000001, 'text': " We're going to have this character limit,", 'tokens': [50654, 492, 434, 516, 281, 362, 341, 2517, 4948, 11, 50786], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 295, 'seek': 82546, 'start': 833.9000000000001, 'end': 837.34, 'text': " and we're going to have this sign off inputs or variables.", 'tokens': [50786, 293, 321, 434, 516, 281, 362, 341, 1465, 766, 15743, 420, 9102, 13, 50958], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 296, 'seek': 82546, 'start': 837.34, 'end': 838.5400000000001, 'text': ' For the human message,', 'tokens': [50958, 1171, 264, 1952, 3636, 11, 51018], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 297, 'seek': 82546, 'start': 838.5400000000001, 'end': 843.5400000000001, 'text': " we're just going to pass in the input there, right?", 'tokens': [51018, 321, 434, 445, 516, 281, 1320, 294, 264, 4846, 456, 11, 558, 30, 51268], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 298, 'seek': 82546, 'start': 843.74, 'end': 844.74, 'text': ' So for this first,', 'tokens': [51278, 407, 337, 341, 700, 11, 51328], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 299, 'seek': 82546, 'start': 844.74, 'end': 846.9000000000001, 'text': " we're not going to pass in those instructions,", 'tokens': [51328, 321, 434, 406, 516, 281, 1320, 294, 729, 9415, 11, 51436], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 300, 'seek': 82546, 'start': 846.9000000000001, 'end': 849.38, 'text': " because we're actually going to create this human message,", 'tokens': [51436, 570, 321, 434, 767, 516, 281, 1884, 341, 1952, 3636, 11, 51560], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 301, 'seek': 82546, 'start': 849.38, 'end': 851.58, 'text': " and we're also going to create following AI message", 'tokens': [51560, 293, 321, 434, 611, 516, 281, 1884, 3480, 7318, 3636, 51670], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 302, 'seek': 82546, 'start': 851.58, 'end': 854.6600000000001, 'text': ' as an example to the chat bot', 'tokens': [51670, 382, 364, 1365, 281, 264, 5081, 10592, 51824], 'temperature': 0.0, 'avg_logprob': -0.12128528716072204, 'compression_ratio': 1.9324894514767932, 'no_speech_prob': 0.007112193387001753}, {'id': 303, 'seek': 85466, 'start': 854.66, 'end': 858.2199999999999, 'text': ' as to how it should respond, okay?', 'tokens': [50364, 382, 281, 577, 309, 820, 4196, 11, 1392, 30, 50542], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 304, 'seek': 85466, 'start': 858.2199999999999, 'end': 859.98, 'text': ' And we put all of these together.', 'tokens': [50542, 400, 321, 829, 439, 295, 613, 1214, 13, 50630], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 305, 'seek': 85466, 'start': 859.98, 'end': 862.26, 'text': ' So we have the system template,', 'tokens': [50630, 407, 321, 362, 264, 1185, 12379, 11, 50744], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 306, 'seek': 85466, 'start': 862.26, 'end': 864.54, 'text': ' the human template, and the AI template.', 'tokens': [50744, 264, 1952, 12379, 11, 293, 264, 7318, 12379, 13, 50858], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 307, 'seek': 85466, 'start': 864.54, 'end': 867.02, 'text': " Like know that we're using AI message prompt template,", 'tokens': [50858, 1743, 458, 300, 321, 434, 1228, 7318, 3636, 12391, 12379, 11, 50982], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 308, 'seek': 85466, 'start': 867.02, 'end': 868.3, 'text': ' human message prompt template,', 'tokens': [50982, 1952, 3636, 12391, 12379, 11, 51046], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 309, 'seek': 85466, 'start': 868.3, 'end': 870.98, 'text': ' and system message prompt template for each of those.', 'tokens': [51046, 293, 1185, 3636, 12391, 12379, 337, 1184, 295, 729, 13, 51180], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 310, 'seek': 85466, 'start': 870.98, 'end': 874.8199999999999, 'text': ' And what we do is create a list of messages.', 'tokens': [51180, 400, 437, 321, 360, 307, 1884, 257, 1329, 295, 7897, 13, 51372], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 311, 'seek': 85466, 'start': 874.8199999999999, 'end': 876.8199999999999, 'text': ' So it goes off to the system message first,', 'tokens': [51372, 407, 309, 1709, 766, 281, 264, 1185, 3636, 700, 11, 51472], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 312, 'seek': 85466, 'start': 876.8199999999999, 'end': 878.54, 'text': ' the human message for second,', 'tokens': [51472, 264, 1952, 3636, 337, 1150, 11, 51558], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 313, 'seek': 85466, 'start': 878.54, 'end': 880.8199999999999, 'text': ' and the AI message third.', 'tokens': [51558, 293, 264, 7318, 3636, 2636, 13, 51672], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 314, 'seek': 85466, 'start': 880.8199999999999, 'end': 883.22, 'text': ' And these are the templates, right?', 'tokens': [51672, 400, 613, 366, 264, 21165, 11, 558, 30, 51792], 'temperature': 0.0, 'avg_logprob': -0.11736528457157196, 'compression_ratio': 2.008695652173913, 'no_speech_prob': 0.0003405149618629366}, {'id': 315, 'seek': 88322, 'start': 883.22, 'end': 886.26, 'text': ' So what we then do is we take our chat prompt,', 'tokens': [50364, 407, 437, 321, 550, 360, 307, 321, 747, 527, 5081, 12391, 11, 50516], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 316, 'seek': 88322, 'start': 886.26, 'end': 887.98, 'text': ' which is a list of these,', 'tokens': [50516, 597, 307, 257, 1329, 295, 613, 11, 50602], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 317, 'seek': 88322, 'start': 887.98, 'end': 890.78, 'text': ' and we format that prompt with our inputs.', 'tokens': [50602, 293, 321, 7877, 300, 12391, 365, 527, 15743, 13, 50742], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 318, 'seek': 88322, 'start': 890.78, 'end': 892.14, 'text': ' So we have the character limit,', 'tokens': [50742, 407, 321, 362, 264, 2517, 4948, 11, 50810], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 319, 'seek': 88322, 'start': 892.14, 'end': 893.46, 'text': " which we're going to set to 50,", 'tokens': [50810, 597, 321, 434, 516, 281, 992, 281, 2625, 11, 50876], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 320, 'seek': 88322, 'start': 893.46, 'end': 896.46, 'text': ' so half of what we had before, making it harder.', 'tokens': [50876, 370, 1922, 295, 437, 321, 632, 949, 11, 1455, 309, 6081, 13, 51026], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 321, 'seek': 88322, 'start': 896.46, 'end': 897.3000000000001, 'text': " We're going to say the sign off", 'tokens': [51026, 492, 434, 516, 281, 584, 264, 1465, 766, 51068], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 322, 'seek': 88322, 'start': 897.3000000000001, 'end': 900.1800000000001, 'text': ' has to be this robot robot,', 'tokens': [51068, 575, 281, 312, 341, 7881, 7881, 11, 51212], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 323, 'seek': 88322, 'start': 900.1800000000001, 'end': 904.22, 'text': ' and the input is going to be the same as before.', 'tokens': [51212, 293, 264, 4846, 307, 516, 281, 312, 264, 912, 382, 949, 13, 51414], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 324, 'seek': 88322, 'start': 904.22, 'end': 907.22, 'text': " And then we're giving an example response, right?", 'tokens': [51414, 400, 550, 321, 434, 2902, 364, 1365, 4134, 11, 558, 30, 51564], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 325, 'seek': 88322, 'start': 907.22, 'end': 909.82, 'text': ' So good is physics, small things.', 'tokens': [51564, 407, 665, 307, 10649, 11, 1359, 721, 13, 51694], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 326, 'seek': 88322, 'start': 909.82, 'end': 913.14, 'text': ' That example response is going to automatically', 'tokens': [51694, 663, 1365, 4134, 307, 516, 281, 6772, 51860], 'temperature': 0.0, 'avg_logprob': -0.14521831936306423, 'compression_ratio': 1.817829457364341, 'no_speech_prob': 4.2642455809982494e-05}, {'id': 327, 'seek': 91314, 'start': 914.06, 'end': 914.9, 'text': ' have the sign off added to it.', 'tokens': [50410, 362, 264, 1465, 766, 3869, 281, 309, 13, 50452], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 328, 'seek': 91314, 'start': 914.9, 'end': 917.06, 'text': " All right, so let's run this.", 'tokens': [50452, 1057, 558, 11, 370, 718, 311, 1190, 341, 13, 50560], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 329, 'seek': 91314, 'start': 917.06, 'end': 917.9, 'text': " Let's see what we get.", 'tokens': [50560, 961, 311, 536, 437, 321, 483, 13, 50602], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 330, 'seek': 91314, 'start': 917.9, 'end': 921.1, 'text': ' So system message, you are helpful assistant.', 'tokens': [50602, 407, 1185, 3636, 11, 291, 366, 4961, 10994, 13, 50762], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 331, 'seek': 91314, 'start': 921.1, 'end': 924.14, 'text': " You keep responses, no one's 50 characters long.", 'tokens': [50762, 509, 1066, 13019, 11, 572, 472, 311, 2625, 4342, 938, 13, 50914], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 332, 'seek': 91314, 'start': 924.14, 'end': 926.78, 'text': ' You sign off every message with robot robot,', 'tokens': [50914, 509, 1465, 766, 633, 3636, 365, 7881, 7881, 11, 51046], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 333, 'seek': 91314, 'start': 926.78, 'end': 930.26, 'text': ' so we can see where those are being added there.', 'tokens': [51046, 370, 321, 393, 536, 689, 729, 366, 885, 3869, 456, 13, 51220], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 334, 'seek': 91314, 'start': 930.26, 'end': 933.1, 'text': ' Human message, how yeah, AI, how you,', 'tokens': [51220, 10294, 3636, 11, 577, 1338, 11, 7318, 11, 577, 291, 11, 51362], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 335, 'seek': 91314, 'start': 933.1, 'end': 934.38, 'text': ' what is quantum physics?', 'tokens': [51362, 437, 307, 13018, 10649, 30, 51426], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 336, 'seek': 91314, 'start': 934.38, 'end': 935.54, 'text': " So it's, you know,", 'tokens': [51426, 407, 309, 311, 11, 291, 458, 11, 51484], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 337, 'seek': 91314, 'start': 935.54, 'end': 937.8199999999999, 'text': " because we're just passing the input in there.", 'tokens': [51484, 570, 321, 434, 445, 8437, 264, 4846, 294, 456, 13, 51598], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 338, 'seek': 91314, 'start': 937.8199999999999, 'end': 939.74, 'text': ' And then we have the AI message, good.', 'tokens': [51598, 400, 550, 321, 362, 264, 7318, 3636, 11, 665, 13, 51694], 'temperature': 0.0, 'avg_logprob': -0.2550339662939086, 'compression_ratio': 1.6417910447761195, 'no_speech_prob': 3.9410788303939626e-05}, {'id': 339, 'seek': 93974, 'start': 939.74, 'end': 944.74, 'text': " It's physics of small things, robot robot, okay?", 'tokens': [50364, 467, 311, 10649, 295, 1359, 721, 11, 7881, 7881, 11, 1392, 30, 50614], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 340, 'seek': 93974, 'start': 944.94, 'end': 945.9, 'text': ' Very short answer.', 'tokens': [50624, 4372, 2099, 1867, 13, 50672], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 341, 'seek': 93974, 'start': 945.9, 'end': 950.34, 'text': " And let's just see if that helps the system", 'tokens': [50672, 400, 718, 311, 445, 536, 498, 300, 3665, 264, 1185, 50894], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 342, 'seek': 93974, 'start': 950.34, 'end': 952.54, 'text': ' produce just very short answers.', 'tokens': [50894, 5258, 445, 588, 2099, 6338, 13, 51004], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 343, 'seek': 93974, 'start': 952.54, 'end': 957.54, 'text': ' So we run this and we get atoms, electrons, photons,', 'tokens': [51004, 407, 321, 1190, 341, 293, 321, 483, 16871, 11, 14265, 11, 40209, 11, 51254], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 344, 'seek': 93974, 'start': 957.66, 'end': 958.82, 'text': ' and then it does a sign off.', 'tokens': [51260, 293, 550, 309, 775, 257, 1465, 766, 13, 51318], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 345, 'seek': 93974, 'start': 958.82, 'end': 961.22, 'text': " So I think that's a pretty good response.", 'tokens': [51318, 407, 286, 519, 300, 311, 257, 1238, 665, 4134, 13, 51438], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 346, 'seek': 93974, 'start': 961.22, 'end': 963.26, 'text': " Let's try again, right?", 'tokens': [51438, 961, 311, 853, 797, 11, 558, 30, 51540], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 347, 'seek': 93974, 'start': 963.26, 'end': 965.86, 'text': ' So here we go slightly over.', 'tokens': [51540, 407, 510, 321, 352, 4748, 670, 13, 51670], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 348, 'seek': 93974, 'start': 965.86, 'end': 969.42, 'text': ' So we get like four characters over there.', 'tokens': [51670, 407, 321, 483, 411, 1451, 4342, 670, 456, 13, 51848], 'temperature': 0.0, 'avg_logprob': -0.15128423549510814, 'compression_ratio': 1.5964912280701755, 'no_speech_prob': 0.002114634495228529}, {'id': 349, 'seek': 96942, 'start': 969.42, 'end': 971.78, 'text': ' So maybe we can be more strict again.', 'tokens': [50364, 407, 1310, 321, 393, 312, 544, 10910, 797, 13, 50482], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 350, 'seek': 96942, 'start': 971.78, 'end': 976.3399999999999, 'text': ' So what we can do is we add in that template', 'tokens': [50482, 407, 437, 321, 393, 360, 307, 321, 909, 294, 300, 12379, 50710], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 351, 'seek': 96942, 'start': 976.3399999999999, 'end': 978.9, 'text': ' that we used before where we add in the answer', 'tokens': [50710, 300, 321, 1143, 949, 689, 321, 909, 294, 264, 1867, 50838], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 352, 'seek': 96942, 'start': 978.9, 'end': 983.0999999999999, 'text': ' in less than the character limit, including white space.', 'tokens': [50838, 294, 1570, 813, 264, 2517, 4948, 11, 3009, 2418, 1901, 13, 51048], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 353, 'seek': 96942, 'start': 983.0999999999999, 'end': 987.02, 'text': " Okay, we're going to add that to our human message.", 'tokens': [51048, 1033, 11, 321, 434, 516, 281, 909, 300, 281, 527, 1952, 3636, 13, 51244], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 354, 'seek': 96942, 'start': 987.02, 'end': 990.02, 'text': " So we're gonna create the human message like this.", 'tokens': [51244, 407, 321, 434, 799, 1884, 264, 1952, 3636, 411, 341, 13, 51394], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 355, 'seek': 96942, 'start': 990.02, 'end': 993.5799999999999, 'text': ' So the chat prompt template and so on and so on.', 'tokens': [51394, 407, 264, 5081, 12391, 12379, 293, 370, 322, 293, 370, 322, 13, 51572], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 356, 'seek': 96942, 'start': 993.5799999999999, 'end': 994.42, 'text': ' Okay, cool.', 'tokens': [51572, 1033, 11, 1627, 13, 51614], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 357, 'seek': 96942, 'start': 994.42, 'end': 997.26, 'text': ' So is it like particle physics as far as before?', 'tokens': [51614, 407, 307, 309, 411, 12359, 10649, 382, 1400, 382, 949, 30, 51756], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 358, 'seek': 96942, 'start': 997.26, 'end': 998.0999999999999, 'text': ' Yeah.', 'tokens': [51756, 865, 13, 51798], 'temperature': 0.0, 'avg_logprob': -0.11094532341792665, 'compression_ratio': 1.7920353982300885, 'no_speech_prob': 6.301607209024951e-05}, {'id': 359, 'seek': 99810, 'start': 998.1, 'end': 998.94, 'text': " So we're asking the same question,", 'tokens': [50364, 407, 321, 434, 3365, 264, 912, 1168, 11, 50406], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 360, 'seek': 99810, 'start': 998.94, 'end': 1001.02, 'text': " but we're adding that on to the end.", 'tokens': [50406, 457, 321, 434, 5127, 300, 322, 281, 264, 917, 13, 50510], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 361, 'seek': 99810, 'start': 1001.02, 'end': 1003.46, 'text': ' So is it like particle physics answering less than 50', 'tokens': [50510, 407, 307, 309, 411, 12359, 10649, 13430, 1570, 813, 2625, 50632], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 362, 'seek': 99810, 'start': 1003.46, 'end': 1005.1800000000001, 'text': ' characters, including white space?', 'tokens': [50632, 4342, 11, 3009, 2418, 1901, 30, 50718], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 363, 'seek': 99810, 'start': 1005.1800000000001, 'end': 1008.5, 'text': " Then what I'm going to do is so within the messages", 'tokens': [50718, 1396, 437, 286, 478, 516, 281, 360, 307, 370, 1951, 264, 7897, 50884], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 364, 'seek': 99810, 'start': 1008.5, 'end': 1012.82, 'text': ' right now, we have this query that we created before', 'tokens': [50884, 558, 586, 11, 321, 362, 341, 14581, 300, 321, 2942, 949, 51100], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 365, 'seek': 99810, 'start': 1012.82, 'end': 1014.46, 'text': ' where we need to replace that query', 'tokens': [51100, 689, 321, 643, 281, 7406, 300, 14581, 51182], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 366, 'seek': 99810, 'start': 1014.46, 'end': 1016.5400000000001, 'text': ' with our new modified query.', 'tokens': [51182, 365, 527, 777, 15873, 14581, 13, 51286], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 367, 'seek': 99810, 'start': 1016.5400000000001, 'end': 1020.66, 'text': " So I'm going to remove the most recent message in messages.", 'tokens': [51286, 407, 286, 478, 516, 281, 4159, 264, 881, 5162, 3636, 294, 7897, 13, 51492], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 368, 'seek': 99810, 'start': 1020.66, 'end': 1024.3, 'text': " And now I'm going to send it with this new human prompt", 'tokens': [51492, 400, 586, 286, 478, 516, 281, 2845, 309, 365, 341, 777, 1952, 12391, 51674], 'temperature': 0.0, 'avg_logprob': -0.1642158637612553, 'compression_ratio': 1.702290076335878, 'no_speech_prob': 0.00035691820085048676}, {'id': 369, 'seek': 102430, 'start': 1024.34, 'end': 1028.6599999999999, 'text': ' value, which is this kind of new version', 'tokens': [50366, 2158, 11, 597, 307, 341, 733, 295, 777, 3037, 50582], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 370, 'seek': 102430, 'start': 1028.6599999999999, 'end': 1030.62, 'text': ' with those instructions added to the end.', 'tokens': [50582, 365, 729, 9415, 3869, 281, 264, 917, 13, 50680], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 371, 'seek': 102430, 'start': 1030.62, 'end': 1033.22, 'text': " So let's have a look, make sure we have the right form.", 'tokens': [50680, 407, 718, 311, 362, 257, 574, 11, 652, 988, 321, 362, 264, 558, 1254, 13, 50810], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 372, 'seek': 102430, 'start': 1033.22, 'end': 1036.7, 'text': ' So system, human, AI, human, AI.', 'tokens': [50810, 407, 1185, 11, 1952, 11, 7318, 11, 1952, 11, 7318, 13, 50984], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 373, 'seek': 102430, 'start': 1036.7, 'end': 1039.34, 'text': " That's the last correct response we got from the AI.", 'tokens': [50984, 663, 311, 264, 1036, 3006, 4134, 321, 658, 490, 264, 7318, 13, 51116], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 374, 'seek': 102430, 'start': 1039.34, 'end': 1042.26, 'text': ' And now we have the new modified human message.', 'tokens': [51116, 400, 586, 321, 362, 264, 777, 15873, 1952, 3636, 13, 51262], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 375, 'seek': 102430, 'start': 1042.26, 'end': 1043.1, 'text': ' Okay, cool.', 'tokens': [51262, 1033, 11, 1627, 13, 51304], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 376, 'seek': 102430, 'start': 1043.1, 'end': 1046.26, 'text': ' So is it like this answering less than 50 characters?', 'tokens': [51304, 407, 307, 309, 411, 341, 13430, 1570, 813, 2625, 4342, 30, 51462], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 377, 'seek': 102430, 'start': 1046.26, 'end': 1050.3, 'text': ' And now we pass that through our chat system again,', 'tokens': [51462, 400, 586, 321, 1320, 300, 807, 527, 5081, 1185, 797, 11, 51664], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 378, 'seek': 102430, 'start': 1050.3, 'end': 1052.94, 'text': ' and we get way shorter.', 'tokens': [51664, 293, 321, 483, 636, 11639, 13, 51796], 'temperature': 0.0, 'avg_logprob': -0.1454317910330636, 'compression_ratio': 1.6363636363636365, 'no_speech_prob': 0.01910032145678997}, {'id': 379, 'seek': 105294, 'start': 1052.94, 'end': 1055.22, 'text': ' So 28 is like, yes, similar.', 'tokens': [50364, 407, 7562, 307, 411, 11, 2086, 11, 2531, 13, 50478], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 380, 'seek': 105294, 'start': 1055.22, 'end': 1058.06, 'text': " Because we're saying, we're telling it", 'tokens': [50478, 1436, 321, 434, 1566, 11, 321, 434, 3585, 309, 50620], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 381, 'seek': 105294, 'start': 1058.06, 'end': 1060.54, 'text': ' in the most recent query again,', 'tokens': [50620, 294, 264, 881, 5162, 14581, 797, 11, 50744], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 382, 'seek': 105294, 'start': 1060.54, 'end': 1062.8200000000002, 'text': ' like you need to answer in less than 50 characters.', 'tokens': [50744, 411, 291, 643, 281, 1867, 294, 1570, 813, 2625, 4342, 13, 50858], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 383, 'seek': 105294, 'start': 1062.8200000000002, 'end': 1065.8600000000001, 'text': ' All right, so what I mentioned before is that maybe', 'tokens': [50858, 1057, 558, 11, 370, 437, 286, 2835, 949, 307, 300, 1310, 51010], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 384, 'seek': 105294, 'start': 1065.8600000000001, 'end': 1067.42, 'text': ' this is a little bit convoluted.', 'tokens': [51010, 341, 307, 257, 707, 857, 3754, 2308, 292, 13, 51088], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 385, 'seek': 105294, 'start': 1067.42, 'end': 1071.18, 'text': " And that's not to say that there aren't use cases for this.", 'tokens': [51088, 400, 300, 311, 406, 281, 584, 300, 456, 3212, 380, 764, 3331, 337, 341, 13, 51276], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 386, 'seek': 105294, 'start': 1071.18, 'end': 1075.54, 'text': " It's just that it would be unfair of me to tell you", 'tokens': [51276, 467, 311, 445, 300, 309, 576, 312, 17019, 295, 385, 281, 980, 291, 51494], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 387, 'seek': 105294, 'start': 1075.54, 'end': 1077.66, 'text': ' all of this and be like, this is how you use it.', 'tokens': [51494, 439, 295, 341, 293, 312, 411, 11, 341, 307, 577, 291, 764, 309, 13, 51600], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 388, 'seek': 105294, 'start': 1077.66, 'end': 1081.06, 'text': ' And then like just miss something that could make things', 'tokens': [51600, 400, 550, 411, 445, 1713, 746, 300, 727, 652, 721, 51770], 'temperature': 0.0, 'avg_logprob': -0.1504584679236779, 'compression_ratio': 1.6509090909090909, 'no_speech_prob': 0.0007316223345696926}, {'id': 389, 'seek': 108106, 'start': 1081.06, 'end': 1084.46, 'text': ' much easier in most, at least most use cases', 'tokens': [50364, 709, 3571, 294, 881, 11, 412, 1935, 881, 764, 3331, 50534], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 390, 'seek': 108106, 'start': 1084.46, 'end': 1087.62, 'text': ' or simply use cases or something along those lines.', 'tokens': [50534, 420, 2935, 764, 3331, 420, 746, 2051, 729, 3876, 13, 50692], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 391, 'seek': 108106, 'start': 1087.62, 'end': 1089.8999999999999, 'text': " Right, so I would say it's arguable as to whether", 'tokens': [50692, 1779, 11, 370, 286, 576, 584, 309, 311, 10171, 712, 382, 281, 1968, 50806], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 392, 'seek': 108106, 'start': 1089.8999999999999, 'end': 1091.8999999999999, 'text': ' all of the above that we just did', 'tokens': [50806, 439, 295, 264, 3673, 300, 321, 445, 630, 50906], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 393, 'seek': 108106, 'start': 1091.8999999999999, 'end': 1094.62, 'text': ' would be any simpler than using an F string.', 'tokens': [50906, 576, 312, 604, 18587, 813, 1228, 364, 479, 6798, 13, 51042], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 394, 'seek': 108106, 'start': 1094.62, 'end': 1096.7, 'text': ' So we have this input.', 'tokens': [51042, 407, 321, 362, 341, 4846, 13, 51146], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 395, 'seek': 108106, 'start': 1096.7, 'end': 1099.1, 'text': ' Okay, cool, is it like particle physics?', 'tokens': [51146, 1033, 11, 1627, 11, 307, 309, 411, 12359, 10649, 30, 51266], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 396, 'seek': 108106, 'start': 1099.1, 'end': 1101.5, 'text': " That's our most recent question, right?", 'tokens': [51266, 663, 311, 527, 881, 5162, 1168, 11, 558, 30, 51386], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 397, 'seek': 108106, 'start': 1101.5, 'end': 1103.06, 'text': ' And we can just use an F string, right?', 'tokens': [51386, 400, 321, 393, 445, 764, 364, 479, 6798, 11, 558, 30, 51464], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 398, 'seek': 108106, 'start': 1103.06, 'end': 1104.8999999999999, 'text': ' So we have the F string here,', 'tokens': [51464, 407, 321, 362, 264, 479, 6798, 510, 11, 51556], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 399, 'seek': 108106, 'start': 1104.8999999999999, 'end': 1108.22, 'text': ' and we have the human message, the content.', 'tokens': [51556, 293, 321, 362, 264, 1952, 3636, 11, 264, 2701, 13, 51722], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 400, 'seek': 108106, 'start': 1108.22, 'end': 1110.82, 'text': ' And then we just say answering less than', 'tokens': [51722, 400, 550, 321, 445, 584, 13430, 1570, 813, 51852], 'temperature': 0.0, 'avg_logprob': -0.1376307645290018, 'compression_ratio': 1.7410071942446044, 'no_speech_prob': 0.0028865470085293055}, {'id': 401, 'seek': 111082, 'start': 1110.86, 'end': 1112.8999999999999, 'text': ' the character limit, which is set here,', 'tokens': [50366, 264, 2517, 4948, 11, 597, 307, 992, 510, 11, 50468], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 402, 'seek': 111082, 'start': 1112.8999999999999, 'end': 1115.9399999999998, 'text': ' characters, including white space, right?', 'tokens': [50468, 4342, 11, 3009, 2418, 1901, 11, 558, 30, 50620], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 403, 'seek': 111082, 'start': 1115.9399999999998, 'end': 1119.3799999999999, 'text': ' And the result of that is basically the same', 'tokens': [50620, 400, 264, 1874, 295, 300, 307, 1936, 264, 912, 50792], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 404, 'seek': 111082, 'start': 1119.3799999999999, 'end': 1121.4199999999998, 'text': ' that we have this, right?', 'tokens': [50792, 300, 321, 362, 341, 11, 558, 30, 50894], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 405, 'seek': 111082, 'start': 1121.4199999999998, 'end': 1125.5, 'text': " That's the same as all of this code here.", 'tokens': [50894, 663, 311, 264, 912, 382, 439, 295, 341, 3089, 510, 13, 51098], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 406, 'seek': 111082, 'start': 1125.5, 'end': 1129.62, 'text': ' So now all of this code, is that right?', 'tokens': [51098, 407, 586, 439, 295, 341, 3089, 11, 307, 300, 558, 30, 51304], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 407, 'seek': 111082, 'start': 1129.62, 'end': 1132.06, 'text': ' Yeah, plus this.', 'tokens': [51304, 865, 11, 1804, 341, 13, 51426], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 408, 'seek': 111082, 'start': 1132.06, 'end': 1135.9399999999998, 'text': " So it depends, I don't know, it depends on your use case,", 'tokens': [51426, 407, 309, 5946, 11, 286, 500, 380, 458, 11, 309, 5946, 322, 428, 764, 1389, 11, 51620], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 409, 'seek': 111082, 'start': 1135.9399999999998, 'end': 1138.34, 'text': " like what you're doing, how you prefer to write this.", 'tokens': [51620, 411, 437, 291, 434, 884, 11, 577, 291, 4382, 281, 2464, 341, 13, 51740], 'temperature': 0.0, 'avg_logprob': -0.14830577045405677, 'compression_ratio': 1.6962616822429906, 'no_speech_prob': 0.00011589897621888667}, {'id': 410, 'seek': 113834, 'start': 1138.3799999999999, 'end': 1141.1799999999998, 'text': ' But just be aware that you can also do this', 'tokens': [50366, 583, 445, 312, 3650, 300, 291, 393, 611, 360, 341, 50506], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 411, 'seek': 113834, 'start': 1141.1799999999998, 'end': 1142.8999999999999, 'text': ' and you get the same result.', 'tokens': [50506, 293, 291, 483, 264, 912, 1874, 13, 50592], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 412, 'seek': 113834, 'start': 1142.8999999999999, 'end': 1146.5, 'text': ' So we can see again, popping the last message', 'tokens': [50592, 407, 321, 393, 536, 797, 11, 18374, 264, 1036, 3636, 50772], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 413, 'seek': 113834, 'start': 1146.5, 'end': 1150.86, 'text': ' to remove the one that we created using the prompt template.', 'tokens': [50772, 281, 4159, 264, 472, 300, 321, 2942, 1228, 264, 12391, 12379, 13, 50990], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 414, 'seek': 113834, 'start': 1150.86, 'end': 1152.3, 'text': " And then I'm adding the one that we created", 'tokens': [50990, 400, 550, 286, 478, 5127, 264, 472, 300, 321, 2942, 51062], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 415, 'seek': 113834, 'start': 1152.3, 'end': 1155.1399999999999, 'text': ' using the F string approach, and we get this.', 'tokens': [51062, 1228, 264, 479, 6798, 3109, 11, 293, 321, 483, 341, 13, 51204], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 416, 'seek': 113834, 'start': 1155.1399999999999, 'end': 1156.5, 'text': " All right, it's the same thing.", 'tokens': [51204, 1057, 558, 11, 309, 311, 264, 912, 551, 13, 51272], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 417, 'seek': 113834, 'start': 1156.5, 'end': 1157.98, 'text': " There's no difference there.", 'tokens': [51272, 821, 311, 572, 2649, 456, 13, 51346], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 418, 'seek': 113834, 'start': 1157.98, 'end': 1160.58, 'text': ' We can process it through chat GPT again,', 'tokens': [51346, 492, 393, 1399, 309, 807, 5081, 26039, 51, 797, 11, 51476], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 419, 'seek': 113834, 'start': 1160.58, 'end': 1162.78, 'text': " and we'll get the same response.", 'tokens': [51476, 293, 321, 603, 483, 264, 912, 4134, 13, 51586], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 420, 'seek': 113834, 'start': 1162.78, 'end': 1165.6599999999999, 'text': ' Okay, so just wanted to make you aware of that.', 'tokens': [51586, 1033, 11, 370, 445, 1415, 281, 652, 291, 3650, 295, 300, 13, 51730], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 421, 'seek': 113834, 'start': 1165.6599999999999, 'end': 1168.26, 'text': " But yeah, that's it for this video.", 'tokens': [51730, 583, 1338, 11, 300, 311, 309, 337, 341, 960, 13, 51860], 'temperature': 0.0, 'avg_logprob': -0.15951130914349929, 'compression_ratio': 1.7717391304347827, 'no_speech_prob': 0.0013441999908536673}, {'id': 422, 'seek': 116826, 'start': 1169.1, 'end': 1170.66, 'text': " We've covered, I think the vast majority", 'tokens': [50406, 492, 600, 5343, 11, 286, 519, 264, 8369, 6286, 50484], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 423, 'seek': 116826, 'start': 1170.66, 'end': 1174.78, 'text': ' of the new chat features within Limechain.', 'tokens': [50484, 295, 264, 777, 5081, 4122, 1951, 441, 1312, 11509, 13, 50690], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 424, 'seek': 116826, 'start': 1174.78, 'end': 1177.3799999999999, 'text': ' And naturally, like we saw at the end there,', 'tokens': [50690, 400, 8195, 11, 411, 321, 1866, 412, 264, 917, 456, 11, 50820], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 425, 'seek': 116826, 'start': 1177.3799999999999, 'end': 1178.98, 'text': " we don't need to use all of them,", 'tokens': [50820, 321, 500, 380, 643, 281, 764, 439, 295, 552, 11, 50900], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 426, 'seek': 116826, 'start': 1178.98, 'end': 1180.54, 'text': ' like the prompt templates.', 'tokens': [50900, 411, 264, 12391, 21165, 13, 50978], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 427, 'seek': 116826, 'start': 1180.54, 'end': 1183.02, 'text': ' You can use, of course, if you have reason to,', 'tokens': [50978, 509, 393, 764, 11, 295, 1164, 11, 498, 291, 362, 1778, 281, 11, 51102], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 428, 'seek': 116826, 'start': 1183.02, 'end': 1186.14, 'text': " but it isn't needed if you have a simpler approach", 'tokens': [51102, 457, 309, 1943, 380, 2978, 498, 291, 362, 257, 18587, 3109, 51258], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 429, 'seek': 116826, 'start': 1186.14, 'end': 1187.5, 'text': ' to doing these things.', 'tokens': [51258, 281, 884, 613, 721, 13, 51326], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 430, 'seek': 116826, 'start': 1187.5, 'end': 1191.98, 'text': " But yeah, it's cool to see this being implemented", 'tokens': [51326, 583, 1338, 11, 309, 311, 1627, 281, 536, 341, 885, 12270, 51550], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 431, 'seek': 116826, 'start': 1191.98, 'end': 1194.86, 'text': " in Limechain, and although I haven't been through it yet,", 'tokens': [51550, 294, 441, 1312, 11509, 11, 293, 4878, 286, 2378, 380, 668, 807, 309, 1939, 11, 51694], 'temperature': 0.0, 'avg_logprob': -0.16772332738657467, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0001970092998817563}, {'id': 432, 'seek': 119486, 'start': 1194.86, 'end': 1198.8999999999999, 'text': " I'm hoping that there will be good integrations", 'tokens': [50364, 286, 478, 7159, 300, 456, 486, 312, 665, 3572, 763, 50566], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 433, 'seek': 119486, 'start': 1198.8999999999999, 'end': 1201.82, 'text': ' of these new chat features with like', 'tokens': [50566, 295, 613, 777, 5081, 4122, 365, 411, 50712], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 434, 'seek': 119486, 'start': 1201.82, 'end': 1204.5, 'text': ' their conversation memory, their retrieval augmentation,', 'tokens': [50712, 641, 3761, 4675, 11, 641, 19817, 3337, 14501, 19631, 11, 50846], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 435, 'seek': 119486, 'start': 1204.5, 'end': 1206.26, 'text': ' and everything else within Limechain,', 'tokens': [50846, 293, 1203, 1646, 1951, 441, 1312, 11509, 11, 50934], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 436, 'seek': 119486, 'start': 1206.26, 'end': 1208.62, 'text': " which is that that's where the value", 'tokens': [50934, 597, 307, 300, 300, 311, 689, 264, 2158, 51052], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 437, 'seek': 119486, 'start': 1208.62, 'end': 1209.82, 'text': ' of this sort of thing will come in.', 'tokens': [51052, 295, 341, 1333, 295, 551, 486, 808, 294, 13, 51112], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 438, 'seek': 119486, 'start': 1209.82, 'end': 1211.82, 'text': " Right now, it's kind of like a simple wrapper", 'tokens': [51112, 1779, 586, 11, 309, 311, 733, 295, 411, 257, 2199, 46906, 51212], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 439, 'seek': 119486, 'start': 1211.82, 'end': 1215.3, 'text': " on top of OpenAI's chat completion endpoint,", 'tokens': [51212, 322, 1192, 295, 7238, 48698, 311, 5081, 19372, 35795, 11, 51386], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 440, 'seek': 119486, 'start': 1215.3, 'end': 1220.26, 'text': " but hopefully with all of the agent's conversation memory", 'tokens': [51386, 457, 4696, 365, 439, 295, 264, 9461, 311, 3761, 4675, 51634], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 441, 'seek': 119486, 'start': 1220.26, 'end': 1223.6599999999999, 'text': ' and retrieval augmentation components that Limechain offers,', 'tokens': [51634, 293, 19817, 3337, 14501, 19631, 6677, 300, 441, 1312, 11509, 7736, 11, 51804], 'temperature': 0.0, 'avg_logprob': -0.14149706587832198, 'compression_ratio': 1.7906976744186047, 'no_speech_prob': 0.007563426624983549}, {'id': 442, 'seek': 122366, 'start': 1223.66, 'end': 1226.02, 'text': " we'll get a tight integration between those,", 'tokens': [50364, 321, 603, 483, 257, 4524, 10980, 1296, 729, 11, 50482], 'temperature': 0.0, 'avg_logprob': -0.14264678955078125, 'compression_ratio': 1.5178571428571428, 'no_speech_prob': 0.0002990570792462677}, {'id': 443, 'seek': 122366, 'start': 1226.02, 'end': 1227.94, 'text': " and that's why this will be useful.", 'tokens': [50482, 293, 300, 311, 983, 341, 486, 312, 4420, 13, 50578], 'temperature': 0.0, 'avg_logprob': -0.14264678955078125, 'compression_ratio': 1.5178571428571428, 'no_speech_prob': 0.0002990570792462677}, {'id': 444, 'seek': 122366, 'start': 1227.94, 'end': 1231.5, 'text': " So that's it for this video.", 'tokens': [50578, 407, 300, 311, 309, 337, 341, 960, 13, 50756], 'temperature': 0.0, 'avg_logprob': -0.14264678955078125, 'compression_ratio': 1.5178571428571428, 'no_speech_prob': 0.0002990570792462677}, {'id': 445, 'seek': 122366, 'start': 1231.5, 'end': 1233.94, 'text': ' I hope all of this has been useful and interesting.', 'tokens': [50756, 286, 1454, 439, 295, 341, 575, 668, 4420, 293, 1880, 13, 50878], 'temperature': 0.0, 'avg_logprob': -0.14264678955078125, 'compression_ratio': 1.5178571428571428, 'no_speech_prob': 0.0002990570792462677}, {'id': 446, 'seek': 122366, 'start': 1233.94, 'end': 1236.3400000000001, 'text': ' But for now, thank you very much for watching,', 'tokens': [50878, 583, 337, 586, 11, 1309, 291, 588, 709, 337, 1976, 11, 50998], 'temperature': 0.0, 'avg_logprob': -0.14264678955078125, 'compression_ratio': 1.5178571428571428, 'no_speech_prob': 0.0002990570792462677}, {'id': 447, 'seek': 122366, 'start': 1236.3400000000001, 'end': 1238.9, 'text': ' and I will see you again in the next one.', 'tokens': [50998, 293, 286, 486, 536, 291, 797, 294, 264, 958, 472, 13, 51126], 'temperature': 0.0, 'avg_logprob': -0.14264678955078125, 'compression_ratio': 1.5178571428571428, 'no_speech_prob': 0.0002990570792462677}, {'id': 448, 'seek': 122366, 'start': 1238.9, 'end': 1239.74, 'text': ' Bye.', 'tokens': [51126, 4621, 13, 51168], 'temperature': 0.0, 'avg_logprob': -0.14264678955078125, 'compression_ratio': 1.5178571428571428, 'no_speech_prob': 0.0002990570792462677}], 'language': 'en'}
checking filename of files in directory tree
checking filename of files in directory tree
checking filename of files in directory tree
checking filename of files in directory tree
