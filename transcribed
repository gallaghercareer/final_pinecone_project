{"0": {"start": 0.0, "end": 4.24, "text": " With the introduction of OpenAI's new chat GPT endpoints,", "tokens": [50364, 2022, 264, 9339, 295, 7238, 48698, 311, 777, 5081, 26039, 51, 917, 20552, 11, 50576], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "1": {"start": 4.24, "end": 6.92, "text": " the line chain library have very quickly,", "tokens": [50576, 264, 1622, 5021, 6405, 362, 588, 2661, 11, 50710], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "2": {"start": 6.92, "end": 11.32, "text": " unsurprisingly, added a ton of new support for chat.", "tokens": [50710, 2693, 374, 34408, 11, 3869, 257, 2952, 295, 777, 1406, 337, 5081, 13, 50930], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "3": {"start": 11.32, "end": 14.24, "text": " The reason for this is that unlike previous", "tokens": [50930, 440, 1778, 337, 341, 307, 300, 8343, 3894, 51076], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "4": {"start": 14.24, "end": 16.080000000000002, "text": " large language model endpoints,", "tokens": [51076, 2416, 2856, 2316, 917, 20552, 11, 51168], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "5": {"start": 16.080000000000002, "end": 20.6, "text": " the new chat GPT endpoint is slightly different.", "tokens": [51168, 264, 777, 5081, 26039, 51, 35795, 307, 4748, 819, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "6": {"start": 20.6, "end": 24.96, "text": " It takes multiple inputs and therefore with line chain,", "tokens": [51394, 467, 2516, 3866, 15743, 293, 4412, 365, 1622, 5021, 11, 51612], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "7": {"start": 24.96, "end": 29.66, "text": " this new sort of approach to calling large language models", "tokens": [51612, 341, 777, 1333, 295, 3109, 281, 5141, 2416, 2856, 5245, 51847], "temperature": 0.0, "avg_logprob": -0.1339528978485422, "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0015718506183475256}, "8": {"start": 29.66, "end": 33.82, "text": " has been supported with its own set of objects", "tokens": [50364, 575, 668, 8104, 365, 1080, 1065, 992, 295, 6565, 50572], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "9": {"start": 33.82, "end": 35.22, "text": " and functions.", "tokens": [50572, 293, 6828, 13, 50642], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "10": {"start": 35.22, "end": 38.06, "text": " So the new chat completion endpoint from OpenAI,", "tokens": [50642, 407, 264, 777, 5081, 19372, 35795, 490, 7238, 48698, 11, 50784], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "11": {"start": 38.06, "end": 41.74, "text": " it differs in the typical large language model endpoints", "tokens": [50784, 309, 37761, 294, 264, 7476, 2416, 2856, 2316, 917, 20552, 50968], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "12": {"start": 41.74, "end": 46.379999999999995, "text": " in that you can essentially pass in three types of inputs", "tokens": [50968, 294, 300, 291, 393, 4476, 1320, 294, 1045, 3467, 295, 15743, 51200], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "13": {"start": 46.379999999999995, "end": 49.06, "text": " that are defined or distinguished", "tokens": [51200, 300, 366, 7642, 420, 21702, 51334], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "14": {"start": 49.06, "end": 50.980000000000004, "text": " by these three different role types.", "tokens": [51334, 538, 613, 1045, 819, 3090, 3467, 13, 51430], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "15": {"start": 50.980000000000004, "end": 53.42, "text": " These three different role types are", "tokens": [51430, 1981, 1045, 819, 3090, 3467, 366, 51552], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "16": {"start": 53.42, "end": 55.900000000000006, "text": " system, user and assistant.", "tokens": [51552, 1185, 11, 4195, 293, 10994, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1042501624973341, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00019101161160506308}, "17": {"start": 55.9, "end": 60.9, "text": " The system or system message acts as the initial prompt", "tokens": [50364, 440, 1185, 420, 1185, 3636, 10672, 382, 264, 5883, 12391, 50614], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "18": {"start": 61.1, "end": 64.7, "text": " to the model in order to set up its behavior", "tokens": [50624, 281, 264, 2316, 294, 1668, 281, 992, 493, 1080, 5223, 50804], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "19": {"start": 64.7, "end": 66.22, "text": " for the rest of the interaction.", "tokens": [50804, 337, 264, 1472, 295, 264, 9285, 13, 50880], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "20": {"start": 66.22, "end": 70.36, "text": " So for example, with chat GPT, what you would find,", "tokens": [50880, 407, 337, 1365, 11, 365, 5081, 26039, 51, 11, 437, 291, 576, 915, 11, 51087], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "21": {"start": 70.36, "end": 73.22, "text": " before we even write anything,", "tokens": [51087, 949, 321, 754, 2464, 1340, 11, 51230], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "22": {"start": 73.22, "end": 76.52, "text": " OpenAI have already passed in a system message", "tokens": [51230, 7238, 48698, 362, 1217, 4678, 294, 257, 1185, 3636, 51395], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "23": {"start": 76.52, "end": 80.5, "text": " to chat GPT, kind of telling it how to behave.", "tokens": [51395, 281, 5081, 26039, 51, 11, 733, 295, 3585, 309, 577, 281, 15158, 13, 51594], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "24": {"start": 80.5, "end": 84.06, "text": " Then after that, we have the user messages.", "tokens": [51594, 1396, 934, 300, 11, 321, 362, 264, 4195, 7897, 13, 51772], "temperature": 0.0, "avg_logprob": -0.10973610828832253, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.0010001526679843664}, "25": {"start": 84.10000000000001, "end": 87.22, "text": " So user messages is like what we write.", "tokens": [50366, 407, 4195, 7897, 307, 411, 437, 321, 2464, 13, 50522], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "26": {"start": 87.22, "end": 90.46000000000001, "text": " So in chat GPT, we write something that's a user message.", "tokens": [50522, 407, 294, 5081, 26039, 51, 11, 321, 2464, 746, 300, 311, 257, 4195, 3636, 13, 50684], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "27": {"start": 90.46000000000001, "end": 92.60000000000001, "text": " And then the other one is the assistant message.", "tokens": [50684, 400, 550, 264, 661, 472, 307, 264, 10994, 3636, 13, 50791], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "28": {"start": 92.60000000000001, "end": 95.98, "text": " Those are the responses that we get from chat GPT.", "tokens": [50791, 3950, 366, 264, 13019, 300, 321, 483, 490, 5081, 26039, 51, 13, 50960], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "29": {"start": 95.98, "end": 99.54, "text": " So the assistant is what chat GPT is producing.", "tokens": [50960, 407, 264, 10994, 307, 437, 5081, 26039, 51, 307, 10501, 13, 51138], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "30": {"start": 99.54, "end": 102.02000000000001, "text": " Now, when we use the endpoint,", "tokens": [51138, 823, 11, 562, 321, 764, 264, 35795, 11, 51262], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "31": {"start": 102.02000000000001, "end": 103.34, "text": " for every new interaction,", "tokens": [51262, 337, 633, 777, 9285, 11, 51328], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "32": {"start": 103.34, "end": 106.22, "text": " we're feeding in a history of previous interactions as well.", "tokens": [51328, 321, 434, 12919, 294, 257, 2503, 295, 3894, 13280, 382, 731, 13, 51472], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "33": {"start": 106.22, "end": 109.34, "text": " So we're always gonna have that system message at the top.", "tokens": [51472, 407, 321, 434, 1009, 799, 362, 300, 1185, 3636, 412, 264, 1192, 13, 51628], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "34": {"start": 109.34, "end": 111.74000000000001, "text": " We're going to have the user message", "tokens": [51628, 492, 434, 516, 281, 362, 264, 4195, 3636, 51748], "temperature": 0.0, "avg_logprob": -0.12916896057128907, "compression_ratio": 1.84, "no_speech_prob": 0.0009694746695458889}, "35": {"start": 111.74, "end": 113.1, "text": " followed by an assistant message,", "tokens": [50364, 6263, 538, 364, 10994, 3636, 11, 50432], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "36": {"start": 113.1, "end": 115.25999999999999, "text": " followed by user message, and so on and so on.", "tokens": [50432, 6263, 538, 4195, 3636, 11, 293, 370, 322, 293, 370, 322, 13, 50540], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "37": {"start": 115.25999999999999, "end": 119.14, "text": " So there is some difference with this new endpoint.", "tokens": [50540, 407, 456, 307, 512, 2649, 365, 341, 777, 35795, 13, 50734], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "38": {"start": 119.14, "end": 123.82, "text": " And therefore, how we interact with chat GPT", "tokens": [50734, 400, 4412, 11, 577, 321, 4648, 365, 5081, 26039, 51, 50968], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "39": {"start": 123.82, "end": 126.17999999999999, "text": " via line chain is also different.", "tokens": [50968, 5766, 1622, 5021, 307, 611, 819, 13, 51086], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "40": {"start": 126.17999999999999, "end": 129.45999999999998, "text": " So let's just jump straight into it.", "tokens": [51086, 407, 718, 311, 445, 3012, 2997, 666, 309, 13, 51250], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "41": {"start": 129.45999999999998, "end": 132.26, "text": " Okay, so we get started with a pip install.", "tokens": [51250, 1033, 11, 370, 321, 483, 1409, 365, 257, 8489, 3625, 13, 51390], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "42": {"start": 132.26, "end": 134.5, "text": " Here we're doing line chain and OpenAI.", "tokens": [51390, 1692, 321, 434, 884, 1622, 5021, 293, 7238, 48698, 13, 51502], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "43": {"start": 134.5, "end": 136.42, "text": " There's only two libraries we use for this.", "tokens": [51502, 821, 311, 787, 732, 15148, 321, 764, 337, 341, 13, 51598], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "44": {"start": 136.42, "end": 139.06, "text": " Once those have been installed or updated,", "tokens": [51598, 3443, 729, 362, 668, 8899, 420, 10588, 11, 51730], "temperature": 0.0, "avg_logprob": -0.16253227565599523, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.0015475900145247579}, "45": {"start": 139.1, "end": 142.26, "text": " okay, so this is the latest version of OpenAI and line chain.", "tokens": [50366, 1392, 11, 370, 341, 307, 264, 6792, 3037, 295, 7238, 48698, 293, 1622, 5021, 13, 50524], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "46": {"start": 142.26, "end": 144.42000000000002, "text": " So you do need to update", "tokens": [50524, 407, 291, 360, 643, 281, 5623, 50632], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "47": {"start": 144.42000000000002, "end": 147.42000000000002, "text": " if you haven't updated them very recently.", "tokens": [50632, 498, 291, 2378, 380, 10588, 552, 588, 3938, 13, 50782], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "48": {"start": 147.42000000000002, "end": 150.3, "text": " Now, what we'll do is start by initializing", "tokens": [50782, 823, 11, 437, 321, 603, 360, 307, 722, 538, 5883, 3319, 50926], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "49": {"start": 150.3, "end": 152.46, "text": " the chat OpenAI object.", "tokens": [50926, 264, 5081, 7238, 48698, 2657, 13, 51034], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "50": {"start": 152.46, "end": 155.54, "text": " For that, we do need an OpenAI API key.", "tokens": [51034, 1171, 300, 11, 321, 360, 643, 364, 7238, 48698, 9362, 2141, 13, 51188], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "51": {"start": 155.54, "end": 157.1, "text": " So you can click this link.", "tokens": [51188, 407, 291, 393, 2052, 341, 2113, 13, 51266], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "52": {"start": 157.1, "end": 159.34, "text": " There will be a link to this notebook.", "tokens": [51266, 821, 486, 312, 257, 2113, 281, 341, 21060, 13, 51378], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "53": {"start": 159.34, "end": 161.36, "text": " So you can follow along at the top of the video", "tokens": [51378, 407, 291, 393, 1524, 2051, 412, 264, 1192, 295, 264, 960, 51479], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "54": {"start": 161.36, "end": 162.54, "text": " somewhere right now.", "tokens": [51479, 4079, 558, 586, 13, 51538], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "55": {"start": 162.54, "end": 166.82, "text": " But this will take us across to this page here.", "tokens": [51538, 583, 341, 486, 747, 505, 2108, 281, 341, 3028, 510, 13, 51752], "temperature": 0.0, "avg_logprob": -0.11299411010742187, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000519171473570168}, "56": {"start": 166.85999999999999, "end": 170.38, "text": " So this is platform.openai.com.", "tokens": [50366, 407, 341, 307, 3663, 13, 15752, 1301, 13, 1112, 13, 50542], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "57": {"start": 170.38, "end": 172.94, "text": " And what we'll do is we'll go to view API keys.", "tokens": [50542, 400, 437, 321, 603, 360, 307, 321, 603, 352, 281, 1910, 9362, 9317, 13, 50670], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "58": {"start": 172.94, "end": 175.57999999999998, "text": " We go to here, create a new secret key,", "tokens": [50670, 492, 352, 281, 510, 11, 1884, 257, 777, 4054, 2141, 11, 50802], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "59": {"start": 175.57999999999998, "end": 177.57999999999998, "text": " and then you copy that secret key.", "tokens": [50802, 293, 550, 291, 5055, 300, 4054, 2141, 13, 50902], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "60": {"start": 177.57999999999998, "end": 180.38, "text": " And what you do is run this cell.", "tokens": [50902, 400, 437, 291, 360, 307, 1190, 341, 2815, 13, 51042], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "61": {"start": 180.38, "end": 181.82, "text": " And you can see at the top here,", "tokens": [51042, 400, 291, 393, 536, 412, 264, 1192, 510, 11, 51114], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "62": {"start": 181.82, "end": 183.9, "text": " it says, tells me OpenAI API key.", "tokens": [51114, 309, 1619, 11, 5112, 385, 7238, 48698, 9362, 2141, 13, 51218], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "63": {"start": 183.9, "end": 186.74, "text": " If you're on Colab, it will appear just below the cell.", "tokens": [51218, 759, 291, 434, 322, 4004, 455, 11, 309, 486, 4204, 445, 2507, 264, 2815, 13, 51360], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "64": {"start": 186.74, "end": 189.5, "text": " And you just paste your API key into there.", "tokens": [51360, 400, 291, 445, 9163, 428, 9362, 2141, 666, 456, 13, 51498], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "65": {"start": 189.5, "end": 192.01999999999998, "text": " Okay, that sorts the API key into here.", "tokens": [51498, 1033, 11, 300, 7527, 264, 9362, 2141, 666, 510, 13, 51624], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "66": {"start": 192.01999999999998, "end": 194.62, "text": " And then we come down here.", "tokens": [51624, 400, 550, 321, 808, 760, 510, 13, 51754], "temperature": 0.0, "avg_logprob": -0.16450948784821226, "compression_ratio": 1.7698744769874477, "no_speech_prob": 0.0036477313842624426}, "67": {"start": 194.62, "end": 196.58, "text": " And what we're going to do is initialize", "tokens": [50364, 400, 437, 321, 434, 516, 281, 360, 307, 5883, 1125, 50462], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "68": {"start": 196.58, "end": 198.9, "text": " the chat OpenAI object.", "tokens": [50462, 264, 5081, 7238, 48698, 2657, 13, 50578], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "69": {"start": 198.9, "end": 203.9, "text": " So for this, we're going to be using the chat GPT model.", "tokens": [50578, 407, 337, 341, 11, 321, 434, 516, 281, 312, 1228, 264, 5081, 26039, 51, 2316, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "70": {"start": 204.14000000000001, "end": 207.42000000000002, "text": " Now, by using this, we're essentially going to default", "tokens": [50840, 823, 11, 538, 1228, 341, 11, 321, 434, 4476, 516, 281, 7576, 51004], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "71": {"start": 207.42000000000002, "end": 211.78, "text": " to the latest version of chat GPT.", "tokens": [51004, 281, 264, 6792, 3037, 295, 5081, 26039, 51, 13, 51222], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "72": {"start": 211.78, "end": 216.78, "text": " So right now, the latest version is actually this here.", "tokens": [51222, 407, 558, 586, 11, 264, 6792, 3037, 307, 767, 341, 510, 13, 51472], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "73": {"start": 216.82, "end": 220.58, "text": " Okay, so if you want to follow this video", "tokens": [51474, 1033, 11, 370, 498, 291, 528, 281, 1524, 341, 960, 51662], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "74": {"start": 220.58, "end": 223.66, "text": " and the exact same responses in the future,", "tokens": [51662, 293, 264, 1900, 912, 13019, 294, 264, 2027, 11, 51816], "temperature": 0.0, "avg_logprob": -0.10124399147781671, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.0005526318564079702}, "75": {"start": 223.66, "end": 227.34, "text": " you need to write this, but I will leave it like this.", "tokens": [50364, 291, 643, 281, 2464, 341, 11, 457, 286, 486, 1856, 309, 411, 341, 13, 50548], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "76": {"start": 227.34, "end": 231.94, "text": " Basically, as they release new versions of this model,", "tokens": [50548, 8537, 11, 382, 436, 4374, 777, 9606, 295, 341, 2316, 11, 50778], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "77": {"start": 231.94, "end": 233.94, "text": " this will just default to the latest one.", "tokens": [50778, 341, 486, 445, 7576, 281, 264, 6792, 472, 13, 50878], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "78": {"start": 233.94, "end": 238.22, "text": " Now, when setting temperature to zero,", "tokens": [50878, 823, 11, 562, 3287, 4292, 281, 4018, 11, 51092], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "79": {"start": 238.22, "end": 241.62, "text": " that would make the completions fully deterministic", "tokens": [51092, 300, 576, 652, 264, 1557, 626, 4498, 15957, 3142, 51262], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "80": {"start": 241.62, "end": 243.22, "text": " as far as I could tell.", "tokens": [51262, 382, 1400, 382, 286, 727, 980, 13, 51342], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "81": {"start": 243.22, "end": 245.57999999999998, "text": " So like running the same prompt twice,", "tokens": [51342, 407, 411, 2614, 264, 912, 12391, 6091, 11, 51460], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "82": {"start": 245.57999999999998, "end": 247.1, "text": " you would get the same output.", "tokens": [51460, 291, 576, 483, 264, 912, 5598, 13, 51536], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "83": {"start": 247.1, "end": 248.57999999999998, "text": " Now, we've seen this.", "tokens": [51536, 823, 11, 321, 600, 1612, 341, 13, 51610], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "84": {"start": 248.57999999999998, "end": 253.38, "text": " So the chats with chat GPT are kind of structured like this.", "tokens": [51610, 407, 264, 38057, 365, 5081, 26039, 51, 366, 733, 295, 18519, 411, 341, 13, 51850], "temperature": 0.0, "avg_logprob": -0.12825488222056422, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.367765647359192e-05}, "85": {"start": 253.38, "end": 257.3, "text": " So we have system, user, assistant, user, assistant.", "tokens": [50364, 407, 321, 362, 1185, 11, 4195, 11, 10994, 11, 4195, 11, 10994, 13, 50560], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "86": {"start": 257.3, "end": 260.58, "text": " That final empty assistant prompt there", "tokens": [50560, 663, 2572, 6707, 10994, 12391, 456, 50724], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "87": {"start": 260.58, "end": 264.42, "text": " is kind of telling the model,", "tokens": [50724, 307, 733, 295, 3585, 264, 2316, 11, 50916], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "88": {"start": 264.42, "end": 267.58, "text": " like now it's your time to respond.", "tokens": [50916, 411, 586, 309, 311, 428, 565, 281, 4196, 13, 51074], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "89": {"start": 267.58, "end": 269.9, "text": " Right, so the model is just completing", "tokens": [51074, 1779, 11, 370, 264, 2316, 307, 445, 19472, 51190], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "90": {"start": 269.9, "end": 272.78, "text": " the end of this conversation.", "tokens": [51190, 264, 917, 295, 341, 3761, 13, 51334], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "91": {"start": 272.78, "end": 276.82, "text": " And the way that we format that is like this, okay?", "tokens": [51334, 400, 264, 636, 300, 321, 7877, 300, 307, 411, 341, 11, 1392, 30, 51536], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "92": {"start": 276.82, "end": 280.5, "text": " In line chain, they kind of mirror this format.", "tokens": [51536, 682, 1622, 5021, 11, 436, 733, 295, 8013, 341, 7877, 13, 51720], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "93": {"start": 280.5, "end": 282.42, "text": " It's very similar, but slightly different.", "tokens": [51720, 467, 311, 588, 2531, 11, 457, 4748, 819, 13, 51816], "temperature": 0.0, "avg_logprob": -0.14156181672040155, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00011233933764742687}, "94": {"start": 282.42, "end": 284.86, "text": " So we have these system message objects,", "tokens": [50364, 407, 321, 362, 613, 1185, 3636, 6565, 11, 50486], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "95": {"start": 284.86, "end": 289.1, "text": " a human message object, and an AI message object.", "tokens": [50486, 257, 1952, 3636, 2657, 11, 293, 364, 7318, 3636, 2657, 13, 50698], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "96": {"start": 289.1, "end": 293.5, "text": " So to create this up here, we would write this, okay?", "tokens": [50698, 407, 281, 1884, 341, 493, 510, 11, 321, 576, 2464, 341, 11, 1392, 30, 50918], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "97": {"start": 293.5, "end": 296.58000000000004, "text": " So we have these messages, and it's just a list of these,", "tokens": [50918, 407, 321, 362, 613, 7897, 11, 293, 309, 311, 445, 257, 1329, 295, 613, 11, 51072], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "98": {"start": 296.58000000000004, "end": 299.90000000000003, "text": " okay, in the order that they have been passed", "tokens": [51072, 1392, 11, 294, 264, 1668, 300, 436, 362, 668, 4678, 51238], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "99": {"start": 299.90000000000003, "end": 301.3, "text": " in the conversation.", "tokens": [51238, 294, 264, 3761, 13, 51308], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "100": {"start": 301.3, "end": 304.22, "text": " Okay, so we're just passing, we are stopping user", "tokens": [51308, 1033, 11, 370, 321, 434, 445, 8437, 11, 321, 366, 12767, 4195, 51454], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "101": {"start": 304.22, "end": 307.18, "text": " for human message and assistant for AI message.", "tokens": [51454, 337, 1952, 3636, 293, 10994, 337, 7318, 3636, 13, 51602], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "102": {"start": 307.18, "end": 309.86, "text": " A system message is still a system message.", "tokens": [51602, 316, 1185, 3636, 307, 920, 257, 1185, 3636, 13, 51736], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "103": {"start": 309.86, "end": 312.02000000000004, "text": " And let's run this, okay?", "tokens": [51736, 400, 718, 311, 1190, 341, 11, 1392, 30, 51844], "temperature": 0.0, "avg_logprob": -0.13730823016557536, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0005613137036561966}, "104": {"start": 312.02, "end": 313.9, "text": " And let's run this.", "tokens": [50364, 400, 718, 311, 1190, 341, 13, 50458], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "105": {"start": 313.9, "end": 316.38, "text": " So this is going to generate a response", "tokens": [50458, 407, 341, 307, 516, 281, 8460, 257, 4134, 50582], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "106": {"start": 316.38, "end": 319.06, "text": " from the chat GPT model, right?", "tokens": [50582, 490, 264, 5081, 26039, 51, 2316, 11, 558, 30, 50716], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "107": {"start": 319.06, "end": 322.14, "text": " And I get this, so we have AI message,", "tokens": [50716, 400, 286, 483, 341, 11, 370, 321, 362, 7318, 3636, 11, 50870], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "108": {"start": 322.14, "end": 324.29999999999995, "text": " and it's pretty long, so what we can do", "tokens": [50870, 293, 309, 311, 1238, 938, 11, 370, 437, 321, 393, 360, 50978], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "109": {"start": 324.29999999999995, "end": 327.29999999999995, "text": " is just print it out, and we get this.", "tokens": [50978, 307, 445, 4482, 309, 484, 11, 293, 321, 483, 341, 13, 51128], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "110": {"start": 327.29999999999995, "end": 331.58, "text": " It's still pretty long, but we can go along like so.", "tokens": [51128, 467, 311, 920, 1238, 938, 11, 457, 321, 393, 352, 2051, 411, 370, 13, 51342], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "111": {"start": 331.58, "end": 332.78, "text": " All right, cool.", "tokens": [51342, 1057, 558, 11, 1627, 13, 51402], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "112": {"start": 332.78, "end": 337.74, "text": " Now, if we take a look up here at the initial response", "tokens": [51402, 823, 11, 498, 321, 747, 257, 574, 493, 510, 412, 264, 5883, 4134, 51650], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "113": {"start": 337.74, "end": 340.29999999999995, "text": " before printing out the response content,", "tokens": [51650, 949, 14699, 484, 264, 4134, 2701, 11, 51778], "temperature": 0.0, "avg_logprob": -0.13735120724409056, "compression_ratio": 1.6277056277056277, "no_speech_prob": 4.1979437810368836e-05}, "114": {"start": 340.3, "end": 343.38, "text": " come to the start and we can see that it's an AI message.", "tokens": [50364, 808, 281, 264, 722, 293, 321, 393, 536, 300, 309, 311, 364, 7318, 3636, 13, 50518], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "115": {"start": 343.38, "end": 346.82, "text": " So it's the same type of object as this here.", "tokens": [50518, 407, 309, 311, 264, 912, 2010, 295, 2657, 382, 341, 510, 13, 50690], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "116": {"start": 346.82, "end": 349.86, "text": " So that means that we can actually just append", "tokens": [50690, 407, 300, 1355, 300, 321, 393, 767, 445, 34116, 50842], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "117": {"start": 349.86, "end": 354.62, "text": " this AI message, our response, directly to messages here,", "tokens": [50842, 341, 7318, 3636, 11, 527, 4134, 11, 3838, 281, 7897, 510, 11, 51080], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "118": {"start": 354.62, "end": 358.66, "text": " and that will create the full conversation,", "tokens": [51080, 293, 300, 486, 1884, 264, 1577, 3761, 11, 51282], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "119": {"start": 358.66, "end": 360.66, "text": " including the latest response, all right?", "tokens": [51282, 3009, 264, 6792, 4134, 11, 439, 558, 30, 51382], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "120": {"start": 360.66, "end": 362.06, "text": " So that's what we're doing here.", "tokens": [51382, 407, 300, 311, 437, 321, 434, 884, 510, 13, 51452], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "121": {"start": 362.06, "end": 364.66, "text": " Then from there, we can just continue the conversation.", "tokens": [51452, 1396, 490, 456, 11, 321, 393, 445, 2354, 264, 3761, 13, 51582], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "122": {"start": 364.66, "end": 368.14, "text": " So we will create a new human message prompt,", "tokens": [51582, 407, 321, 486, 1884, 257, 777, 1952, 3636, 12391, 11, 51756], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "123": {"start": 368.14, "end": 369.86, "text": " we'll add that to our messages,", "tokens": [51756, 321, 603, 909, 300, 281, 527, 7897, 11, 51842], "temperature": 0.0, "avg_logprob": -0.0921643780123803, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.00020018834038637578}, "124": {"start": 369.86, "end": 372.66, "text": " and then we'll send all of those to chat GPT.", "tokens": [50364, 293, 550, 321, 603, 2845, 439, 295, 729, 281, 5081, 26039, 51, 13, 50504], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "125": {"start": 372.66, "end": 376.38, "text": " Okay, so now what was the next question I asked?", "tokens": [50504, 1033, 11, 370, 586, 437, 390, 264, 958, 1168, 286, 2351, 30, 50690], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "126": {"start": 376.38, "end": 380.94, "text": " Why do physicists believe it can produce a unified theory?", "tokens": [50690, 1545, 360, 48716, 1697, 309, 393, 5258, 257, 26787, 5261, 30, 50918], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "127": {"start": 380.94, "end": 383.54, "text": " This is talking about string theory up here.", "tokens": [50918, 639, 307, 1417, 466, 6798, 5261, 493, 510, 13, 51048], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "128": {"start": 383.54, "end": 386.62, "text": " And then it goes in and starts explaining that they believe", "tokens": [51048, 400, 550, 309, 1709, 294, 293, 3719, 13468, 300, 436, 1697, 51202], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "129": {"start": 386.62, "end": 389.3, "text": " that string theory has potential to produce a unified theory", "tokens": [51202, 300, 6798, 5261, 575, 3995, 281, 5258, 257, 26787, 5261, 51336], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "130": {"start": 389.3, "end": 391.06, "text": " because, you know, so on and so on.", "tokens": [51336, 570, 11, 291, 458, 11, 370, 322, 293, 370, 322, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "131": {"start": 391.06, "end": 392.06, "text": " Okay, cool.", "tokens": [51424, 1033, 11, 1627, 13, 51474], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "132": {"start": 392.06, "end": 395.26, "text": " Now, that is, I suppose, a core functionality", "tokens": [51474, 823, 11, 300, 307, 11, 286, 7297, 11, 257, 4965, 14980, 51634], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "133": {"start": 395.26, "end": 398.74, "text": " of Linechain's new chat features,", "tokens": [51634, 295, 14670, 11509, 311, 777, 5081, 4122, 11, 51808], "temperature": 0.0, "avg_logprob": -0.1645130407614786, "compression_ratio": 1.6679104477611941, "no_speech_prob": 7.140810339478776e-05}, "134": {"start": 398.74, "end": 400.58, "text": " but there are a few other things", "tokens": [50364, 457, 456, 366, 257, 1326, 661, 721, 50456], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "135": {"start": 400.58, "end": 403.06, "text": " that they've introduced alongside these.", "tokens": [50456, 300, 436, 600, 7268, 12385, 613, 13, 50580], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "136": {"start": 403.06, "end": 406.46000000000004, "text": " So we have a few new prompt templates.", "tokens": [50580, 407, 321, 362, 257, 1326, 777, 12391, 21165, 13, 50750], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "137": {"start": 406.46000000000004, "end": 410.98, "text": " So these new prompt templates, we have like a AI message,", "tokens": [50750, 407, 613, 777, 12391, 21165, 11, 321, 362, 411, 257, 7318, 3636, 11, 50976], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "138": {"start": 410.98, "end": 414.82, "text": " human message, and system message prompt template.", "tokens": [50976, 1952, 3636, 11, 293, 1185, 3636, 12391, 12379, 13, 51168], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "139": {"start": 414.82, "end": 417.3, "text": " And these are kind of just an extension", "tokens": [51168, 400, 613, 366, 733, 295, 445, 364, 10320, 51292], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "140": {"start": 417.3, "end": 420.74, "text": " of the original prompt templates in Linechain.", "tokens": [51292, 295, 264, 3380, 12391, 21165, 294, 14670, 11509, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "141": {"start": 420.74, "end": 423.90000000000003, "text": " But when you use them, you have a couple of functions", "tokens": [51464, 583, 562, 291, 764, 552, 11, 291, 362, 257, 1916, 295, 6828, 51622], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "142": {"start": 423.90000000000003, "end": 427.5, "text": " that will allow you to create your prompt template", "tokens": [51622, 300, 486, 2089, 291, 281, 1884, 428, 12391, 12379, 51802], "temperature": 0.0, "avg_logprob": -0.07799130967519816, "compression_ratio": 1.84375, "no_speech_prob": 5.736729144700803e-05}, "143": {"start": 427.5, "end": 430.7, "text": " and output it as a system message, AI message,", "tokens": [50364, 293, 5598, 309, 382, 257, 1185, 3636, 11, 7318, 3636, 11, 50524], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "144": {"start": 430.7, "end": 432.14, "text": " or user message.", "tokens": [50524, 420, 4195, 3636, 13, 50596], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "145": {"start": 432.14, "end": 434.94, "text": " And you can also kind of like link them all together", "tokens": [50596, 400, 291, 393, 611, 733, 295, 411, 2113, 552, 439, 1214, 50736], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "146": {"start": 434.94, "end": 437.9, "text": " to create a list of messages that you then just pass", "tokens": [50736, 281, 1884, 257, 1329, 295, 7897, 300, 291, 550, 445, 1320, 50884], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "147": {"start": 437.9, "end": 440.34, "text": " straight into your chat endpoint.", "tokens": [50884, 2997, 666, 428, 5081, 35795, 13, 51006], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "148": {"start": 440.34, "end": 445.34, "text": " Now, I'm not super aware of like a huge number", "tokens": [51006, 823, 11, 286, 478, 406, 1687, 3650, 295, 411, 257, 2603, 1230, 51256], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "149": {"start": 445.58, "end": 448.9, "text": " of reasons to use these right now,", "tokens": [51268, 295, 4112, 281, 764, 613, 558, 586, 11, 51434], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "150": {"start": 448.9, "end": 451.7, "text": " but these are part of the new features", "tokens": [51434, 457, 613, 366, 644, 295, 264, 777, 4122, 51574], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "151": {"start": 451.7, "end": 452.94, "text": " in Linechain for chat.", "tokens": [51574, 294, 14670, 11509, 337, 5081, 13, 51636], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "152": {"start": 452.94, "end": 456.02, "text": " So I figure it is important to share these.", "tokens": [51636, 407, 286, 2573, 309, 307, 1021, 281, 2073, 613, 13, 51790], "temperature": 0.0, "avg_logprob": -0.08692009077159636, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0005975324893370271}, "153": {"start": 456.02, "end": 461.02, "text": " And if it seems like something that would actually help you", "tokens": [50364, 400, 498, 309, 2544, 411, 746, 300, 576, 767, 854, 291, 50614], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "154": {"start": 461.09999999999997, "end": 463.97999999999996, "text": " with whatever it is you're building, then that's great.", "tokens": [50618, 365, 2035, 309, 307, 291, 434, 2390, 11, 550, 300, 311, 869, 13, 50762], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "155": {"start": 463.97999999999996, "end": 467.14, "text": " You now know how, or you will know how to use them.", "tokens": [50762, 509, 586, 458, 577, 11, 420, 291, 486, 458, 577, 281, 764, 552, 13, 50920], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "156": {"start": 467.14, "end": 469.14, "text": " So we'll come down to here.", "tokens": [50920, 407, 321, 603, 808, 760, 281, 510, 13, 51020], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "157": {"start": 469.14, "end": 471.09999999999997, "text": " What I'm doing is I'm making sure", "tokens": [51020, 708, 286, 478, 884, 307, 286, 478, 1455, 988, 51118], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "158": {"start": 471.09999999999997, "end": 473.58, "text": " I'm using the March model here.", "tokens": [51118, 286, 478, 1228, 264, 6129, 2316, 510, 13, 51242], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "159": {"start": 473.58, "end": 477.26, "text": " So we're going to set up our first system message,", "tokens": [51242, 407, 321, 434, 516, 281, 992, 493, 527, 700, 1185, 3636, 11, 51426], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "160": {"start": 477.26, "end": 480.09999999999997, "text": " and we're going to create a human message,", "tokens": [51426, 293, 321, 434, 516, 281, 1884, 257, 1952, 3636, 11, 51568], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "161": {"start": 480.09999999999997, "end": 481.82, "text": " our first input.", "tokens": [51568, 527, 700, 4846, 13, 51654], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "162": {"start": 481.82, "end": 485.02, "text": " Now, within this system message,", "tokens": [51654, 823, 11, 1951, 341, 1185, 3636, 11, 51814], "temperature": 0.0, "avg_logprob": -0.10846059605226678, "compression_ratio": 1.7016806722689075, "no_speech_prob": 0.00010070046118926257}, "163": {"start": 485.02, "end": 487.78, "text": " I'm saying I want the responses to be no more", "tokens": [50364, 286, 478, 1566, 286, 528, 264, 13019, 281, 312, 572, 544, 50502], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "164": {"start": 487.78, "end": 490.97999999999996, "text": " than 100 characters long, including white space.", "tokens": [50502, 813, 2319, 4342, 938, 11, 3009, 2418, 1901, 13, 50662], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "165": {"start": 490.97999999999996, "end": 492.85999999999996, "text": " And I want it to sign off every message", "tokens": [50662, 400, 286, 528, 309, 281, 1465, 766, 633, 3636, 50756], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "166": {"start": 492.85999999999996, "end": 497.21999999999997, "text": " with a random name like robot or bot rub, okay?", "tokens": [50756, 365, 257, 4974, 1315, 411, 7881, 420, 10592, 5915, 11, 1392, 30, 50974], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "167": {"start": 497.21999999999997, "end": 498.94, "text": " We're just giving it tasks to do", "tokens": [50974, 492, 434, 445, 2902, 309, 9608, 281, 360, 51060], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "168": {"start": 498.94, "end": 501.21999999999997, "text": " to see how well it follows these instructions.", "tokens": [51060, 281, 536, 577, 731, 309, 10002, 613, 9415, 13, 51174], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "169": {"start": 501.21999999999997, "end": 506.21999999999997, "text": " So run this, and now we make our first completion from this", "tokens": [51174, 407, 1190, 341, 11, 293, 586, 321, 652, 527, 700, 19372, 490, 341, 51424], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "170": {"start": 506.74, "end": 509.21999999999997, "text": " and let's see how it does with those instructions.", "tokens": [51450, 293, 718, 311, 536, 577, 309, 775, 365, 729, 9415, 13, 51574], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "171": {"start": 509.21999999999997, "end": 512.42, "text": " Okay, so the length is way out.", "tokens": [51574, 1033, 11, 370, 264, 4641, 307, 636, 484, 13, 51734], "temperature": 0.0, "avg_logprob": -0.14222474141163868, "compression_ratio": 1.569767441860465, "no_speech_prob": 9.914357360685244e-05}, "172": {"start": 512.42, "end": 515.54, "text": " We asked for 100 at maximum, it's 154.", "tokens": [50364, 492, 2351, 337, 2319, 412, 6674, 11, 309, 311, 2119, 19, 13, 50520], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "173": {"start": 515.54, "end": 520.4599999999999, "text": " And it also didn't give us a sign off there as well.", "tokens": [50520, 400, 309, 611, 994, 380, 976, 505, 257, 1465, 766, 456, 382, 731, 13, 50766], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "174": {"start": 520.4599999999999, "end": 523.9, "text": " Now, this is kind of just an issue", "tokens": [50766, 823, 11, 341, 307, 733, 295, 445, 364, 2734, 50938], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "175": {"start": 523.9, "end": 528.26, "text": " with the current version of ChatGPT, okay?", "tokens": [50938, 365, 264, 2190, 3037, 295, 27503, 38, 47, 51, 11, 1392, 30, 51156], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "176": {"start": 528.26, "end": 529.8199999999999, "text": " So with this version here,", "tokens": [51156, 407, 365, 341, 3037, 510, 11, 51234], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "177": {"start": 529.8199999999999, "end": 532.78, "text": " it's not very good at following system messages apparently,", "tokens": [51234, 309, 311, 406, 588, 665, 412, 3480, 1185, 7897, 7970, 11, 51382], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "178": {"start": 532.78, "end": 535.26, "text": " and it's kind of better to pass these instructions", "tokens": [51382, 293, 309, 311, 733, 295, 1101, 281, 1320, 613, 9415, 51506], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "179": {"start": 535.26, "end": 536.5799999999999, "text": " into your human message.", "tokens": [51506, 666, 428, 1952, 3636, 13, 51572], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "180": {"start": 536.5799999999999, "end": 539.2199999999999, "text": " But we might not want a user", "tokens": [51572, 583, 321, 1062, 406, 528, 257, 4195, 51704], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "181": {"start": 539.2199999999999, "end": 541.62, "text": " to have to specify these things.", "tokens": [51704, 281, 362, 281, 16500, 613, 721, 13, 51824], "temperature": 0.0, "avg_logprob": -0.12395047726838485, "compression_ratio": 1.5212355212355213, "no_speech_prob": 0.0008293407154269516}, "182": {"start": 541.62, "end": 543.82, "text": " So maybe this is where we can use", "tokens": [50364, 407, 1310, 341, 307, 689, 321, 393, 764, 50474], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "183": {"start": 543.82, "end": 545.86, "text": " one of these prompt templates.", "tokens": [50474, 472, 295, 613, 12391, 21165, 13, 50576], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "184": {"start": 545.86, "end": 546.74, "text": " So let's try.", "tokens": [50576, 407, 718, 311, 853, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "185": {"start": 546.74, "end": 551.18, "text": " What we're gonna do is for every human message,", "tokens": [50620, 708, 321, 434, 799, 360, 307, 337, 633, 1952, 3636, 11, 50842], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "186": {"start": 551.18, "end": 552.94, "text": " we're gonna pass it into here, right?", "tokens": [50842, 321, 434, 799, 1320, 309, 666, 510, 11, 558, 30, 50930], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "187": {"start": 552.94, "end": 554.78, "text": " So we had that question before,", "tokens": [50930, 407, 321, 632, 300, 1168, 949, 11, 51022], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "188": {"start": 554.78, "end": 556.22, "text": " hi AI, how are you?", "tokens": [51022, 4879, 7318, 11, 577, 366, 291, 30, 51094], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "189": {"start": 556.22, "end": 557.46, "text": " What is quantum physics?", "tokens": [51094, 708, 307, 13018, 10649, 30, 51156], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "190": {"start": 557.46, "end": 559.18, "text": " We'd pass it into input here.", "tokens": [51156, 492, 1116, 1320, 309, 666, 4846, 510, 13, 51242], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "191": {"start": 559.18, "end": 561.9, "text": " And what I'm going to do is after the question,", "tokens": [51242, 400, 437, 286, 478, 516, 281, 360, 307, 934, 264, 1168, 11, 51378], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "192": {"start": 561.9, "end": 564.62, "text": " I'm gonna say, can you keep the response", "tokens": [51378, 286, 478, 799, 584, 11, 393, 291, 1066, 264, 4134, 51514], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "193": {"start": 564.62, "end": 569.38, "text": " to no more than 100 characters, including white space", "tokens": [51514, 281, 572, 544, 813, 2319, 4342, 11, 3009, 2418, 1901, 51752], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "194": {"start": 569.38, "end": 571.22, "text": " and sign off with a random name?", "tokens": [51752, 293, 1465, 766, 365, 257, 4974, 1315, 30, 51844], "temperature": 0.0, "avg_logprob": -0.10191343095567491, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.0001794969866750762}, "195": {"start": 571.22, "end": 574.14, "text": " So we create our prompt like this.", "tokens": [50364, 407, 321, 1884, 527, 12391, 411, 341, 13, 50510], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "196": {"start": 574.14, "end": 576.4200000000001, "text": " So we have this line chain prompts chat,", "tokens": [50510, 407, 321, 362, 341, 1622, 5021, 41095, 5081, 11, 50624], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "197": {"start": 576.4200000000001, "end": 578.38, "text": " and we have human message prompt template.", "tokens": [50624, 293, 321, 362, 1952, 3636, 12391, 12379, 13, 50722], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "198": {"start": 578.38, "end": 580.86, "text": " And we also need to use this chat prompt template.", "tokens": [50722, 400, 321, 611, 643, 281, 764, 341, 5081, 12391, 12379, 13, 50846], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "199": {"start": 580.86, "end": 585.86, "text": " I feel like this is a little bit convoluted at the moment,", "tokens": [50846, 286, 841, 411, 341, 307, 257, 707, 857, 3754, 2308, 292, 412, 264, 1623, 11, 51096], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "200": {"start": 586.34, "end": 587.7, "text": " but this is just how it is.", "tokens": [51120, 457, 341, 307, 445, 577, 309, 307, 13, 51188], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "201": {"start": 587.7, "end": 590.94, "text": " So we're gonna go through anyway.", "tokens": [51188, 407, 321, 434, 799, 352, 807, 4033, 13, 51350], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "202": {"start": 590.94, "end": 593.0600000000001, "text": " So we have human message prompt template,", "tokens": [51350, 407, 321, 362, 1952, 3636, 12391, 12379, 11, 51456], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "203": {"start": 593.0600000000001, "end": 595.6600000000001, "text": " and we're gonna have this, okay?", "tokens": [51456, 293, 321, 434, 799, 362, 341, 11, 1392, 30, 51586], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "204": {"start": 595.6600000000001, "end": 599.26, "text": " This is just like a typical prompt template in line chain.", "tokens": [51586, 639, 307, 445, 411, 257, 7476, 12391, 12379, 294, 1622, 5021, 13, 51766], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "205": {"start": 599.26, "end": 600.98, "text": " Then once we have that human template,", "tokens": [51766, 1396, 1564, 321, 362, 300, 1952, 12379, 11, 51852], "temperature": 0.0, "avg_logprob": -0.12178943306207657, "compression_ratio": 2.057777777777778, "no_speech_prob": 4.539538349490613e-05}, "206": {"start": 600.98, "end": 605.02, "text": " we need to pass it to this chat prompt template", "tokens": [50364, 321, 643, 281, 1320, 309, 281, 341, 5081, 12391, 12379, 50566], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "207": {"start": 605.02, "end": 607.14, "text": " and from messages, right?", "tokens": [50566, 293, 490, 7897, 11, 558, 30, 50672], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "208": {"start": 607.14, "end": 610.82, "text": " And then in there, we pass in like a list", "tokens": [50672, 400, 550, 294, 456, 11, 321, 1320, 294, 411, 257, 1329, 50856], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "209": {"start": 610.82, "end": 612.98, "text": " of whatever messages we want, right?", "tokens": [50856, 295, 2035, 7897, 321, 528, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "210": {"start": 612.98, "end": 615.5, "text": " So I will give you another example soon,", "tokens": [50964, 407, 286, 486, 976, 291, 1071, 1365, 2321, 11, 51090], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "211": {"start": 615.5, "end": 618.86, "text": " but we can also pass multiple messages here,", "tokens": [51090, 457, 321, 393, 611, 1320, 3866, 7897, 510, 11, 51258], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "212": {"start": 618.86, "end": 622.1800000000001, "text": " like system message, human message, AI message, and so on,", "tokens": [51258, 411, 1185, 3636, 11, 1952, 3636, 11, 7318, 3636, 11, 293, 370, 322, 11, 51424], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "213": {"start": 622.1800000000001, "end": 627.1, "text": " which I found some way of kind of using that.", "tokens": [51424, 597, 286, 1352, 512, 636, 295, 733, 295, 1228, 300, 13, 51670], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "214": {"start": 627.1, "end": 630.26, "text": " So I mean, I think that's kind of interesting at least.", "tokens": [51670, 407, 286, 914, 11, 286, 519, 300, 311, 733, 295, 1880, 412, 1935, 13, 51828], "temperature": 0.0, "avg_logprob": -0.09939396381378174, "compression_ratio": 1.7347826086956522, "no_speech_prob": 5.062967829871923e-05}, "215": {"start": 630.26, "end": 631.9, "text": " So we format that with some input.", "tokens": [50364, 407, 321, 7877, 300, 365, 512, 4846, 13, 50446], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "216": {"start": 631.9, "end": 634.66, "text": " So we pass in this input here,", "tokens": [50446, 407, 321, 1320, 294, 341, 4846, 510, 11, 50584], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "217": {"start": 634.66, "end": 636.9399999999999, "text": " how AI, how you, what is quantum physics?", "tokens": [50584, 577, 7318, 11, 577, 291, 11, 437, 307, 13018, 10649, 30, 50698], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "218": {"start": 636.9399999999999, "end": 639.06, "text": " And let's see what we get from that.", "tokens": [50698, 400, 718, 311, 536, 437, 321, 483, 490, 300, 13, 50804], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "219": {"start": 639.06, "end": 642.42, "text": " So we get this chat prompt value object,", "tokens": [50804, 407, 321, 483, 341, 5081, 12391, 2158, 2657, 11, 50972], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "220": {"start": 642.42, "end": 646.14, "text": " and it has a list of messages in there.", "tokens": [50972, 293, 309, 575, 257, 1329, 295, 7897, 294, 456, 13, 51158], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "221": {"start": 646.14, "end": 648.8199999999999, "text": " First message and the only message is,", "tokens": [51158, 2386, 3636, 293, 264, 787, 3636, 307, 11, 51292], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "222": {"start": 648.8199999999999, "end": 651.3, "text": " hi AI, how are you, what is quantum physics?", "tokens": [51292, 4879, 7318, 11, 577, 366, 291, 11, 437, 307, 13018, 10649, 30, 51416], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "223": {"start": 651.3, "end": 652.74, "text": " Right, so that's our input.", "tokens": [51416, 1779, 11, 370, 300, 311, 527, 4846, 13, 51488], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "224": {"start": 652.74, "end": 654.34, "text": " And then we have, can you keep the response", "tokens": [51488, 400, 550, 321, 362, 11, 393, 291, 1066, 264, 4134, 51568], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "225": {"start": 654.34, "end": 656.86, "text": " to no more than 100 characters including white space,", "tokens": [51568, 281, 572, 544, 813, 2319, 4342, 3009, 2418, 1901, 11, 51694], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "226": {"start": 656.86, "end": 658.7, "text": " sign off, so on and so on, right?", "tokens": [51694, 1465, 766, 11, 370, 322, 293, 370, 322, 11, 558, 30, 51786], "temperature": 0.0, "avg_logprob": -0.147181991144275, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.746422827243805e-05}, "227": {"start": 658.7, "end": 663.22, "text": " So that is our template that is being applied based on this.", "tokens": [50364, 407, 300, 307, 527, 12379, 300, 307, 885, 6456, 2361, 322, 341, 13, 50590], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "228": {"start": 663.22, "end": 664.5400000000001, "text": " All right, cool.", "tokens": [50590, 1057, 558, 11, 1627, 13, 50656], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "229": {"start": 665.38, "end": 667.26, "text": " Now, and we come down to here,", "tokens": [50698, 823, 11, 293, 321, 808, 760, 281, 510, 11, 50792], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "230": {"start": 667.26, "end": 671.46, "text": " and to use our human message prompt template", "tokens": [50792, 293, 281, 764, 527, 1952, 3636, 12391, 12379, 51002], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "231": {"start": 671.46, "end": 676.1, "text": " as a typical message or human message,", "tokens": [51002, 382, 257, 7476, 3636, 420, 1952, 3636, 11, 51234], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "232": {"start": 676.1, "end": 679.94, "text": " we actually need to use this here, right?", "tokens": [51234, 321, 767, 643, 281, 764, 341, 510, 11, 558, 30, 51426], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "233": {"start": 679.94, "end": 683.9000000000001, "text": " So we take our chat prompt value, which we create here,", "tokens": [51426, 407, 321, 747, 527, 5081, 12391, 2158, 11, 597, 321, 1884, 510, 11, 51624], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "234": {"start": 683.9000000000001, "end": 685.1800000000001, "text": " and we can see here,", "tokens": [51624, 293, 321, 393, 536, 510, 11, 51688], "temperature": 0.0, "avg_logprob": -0.1615022341410319, "compression_ratio": 1.7374301675977655, "no_speech_prob": 0.000206588621949777}, "235": {"start": 685.18, "end": 690.02, "text": " and we can either pass it as two messages,", "tokens": [50364, 293, 321, 393, 2139, 1320, 309, 382, 732, 7897, 11, 50606], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "236": {"start": 690.02, "end": 692.6999999999999, "text": " that will give us the format that we need", "tokens": [50606, 300, 486, 976, 505, 264, 7877, 300, 321, 643, 50740], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "237": {"start": 692.6999999999999, "end": 695.06, "text": " in order to pass it to chat GPT,", "tokens": [50740, 294, 1668, 281, 1320, 309, 281, 5081, 26039, 51, 11, 50858], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "238": {"start": 695.06, "end": 698.2199999999999, "text": " or we can just create a string out of it, okay?", "tokens": [50858, 420, 321, 393, 445, 1884, 257, 6798, 484, 295, 309, 11, 1392, 30, 51016], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "239": {"start": 698.2199999999999, "end": 702.62, "text": " So this would, I suppose, be pretty much the same as,", "tokens": [51016, 407, 341, 576, 11, 286, 7297, 11, 312, 1238, 709, 264, 912, 382, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "240": {"start": 702.62, "end": 704.42, "text": " like, using an F string.", "tokens": [51236, 411, 11, 1228, 364, 479, 6798, 13, 51326], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "241": {"start": 704.42, "end": 705.9, "text": " The only thing that's added onto there", "tokens": [51326, 440, 787, 551, 300, 311, 3869, 3911, 456, 51400], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "242": {"start": 705.9, "end": 708.8599999999999, "text": " is we have this, like, human, right?", "tokens": [51400, 307, 321, 362, 341, 11, 411, 11, 1952, 11, 558, 30, 51548], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "243": {"start": 708.8599999999999, "end": 712.06, "text": " Otherwise, it's literally just, like,", "tokens": [51548, 10328, 11, 309, 311, 3736, 445, 11, 411, 11, 51708], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "244": {"start": 712.06, "end": 714.3, "text": " taking this and converting it into a string.", "tokens": [51708, 1940, 341, 293, 29942, 309, 666, 257, 6798, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1375761188444544, "compression_ratio": 1.625, "no_speech_prob": 0.0009691251907497644}, "245": {"start": 714.3, "end": 717.06, "text": " Okay, so let's see if this approach works.", "tokens": [50364, 1033, 11, 370, 718, 311, 536, 498, 341, 3109, 1985, 13, 50502], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "246": {"start": 717.06, "end": 719.6999999999999, "text": " Here, I'm just kind of throwing it all together.", "tokens": [50502, 1692, 11, 286, 478, 445, 733, 295, 10238, 309, 439, 1214, 13, 50634], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "247": {"start": 719.6999999999999, "end": 723.4599999999999, "text": " So we have the chat prompt, the input, hi, hi,", "tokens": [50634, 407, 321, 362, 264, 5081, 12391, 11, 264, 4846, 11, 4879, 11, 4879, 11, 50822], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "248": {"start": 723.4599999999999, "end": 724.3, "text": " how are you doing?", "tokens": [50822, 577, 366, 291, 884, 30, 50864], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "249": {"start": 724.3, "end": 727.54, "text": " That's going to create this,", "tokens": [50864, 663, 311, 516, 281, 1884, 341, 11, 51026], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "250": {"start": 727.54, "end": 729.66, "text": " and then I'm going to convert two messages", "tokens": [51026, 293, 550, 286, 478, 516, 281, 7620, 732, 7897, 51132], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "251": {"start": 729.66, "end": 731.02, "text": " and take the first message,", "tokens": [51132, 293, 747, 264, 700, 3636, 11, 51200], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "252": {"start": 731.02, "end": 732.6999999999999, "text": " which is the only message in there,", "tokens": [51200, 597, 307, 264, 787, 3636, 294, 456, 11, 51284], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "253": {"start": 732.6999999999999, "end": 736.4599999999999, "text": " which is essentially going to give us this human message,", "tokens": [51284, 597, 307, 4476, 516, 281, 976, 505, 341, 1952, 3636, 11, 51472], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "254": {"start": 736.4599999999999, "end": 737.3, "text": " okay?", "tokens": [51472, 1392, 30, 51514], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "255": {"start": 737.3, "end": 740.38, "text": " And did I, can you keep the response", "tokens": [51514, 400, 630, 286, 11, 393, 291, 1066, 264, 4134, 51668], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "256": {"start": 740.38, "end": 743.18, "text": " to no more than 100 characters?", "tokens": [51668, 281, 572, 544, 813, 2319, 4342, 30, 51808], "temperature": 0.0, "avg_logprob": -0.09692959522637795, "compression_ratio": 1.6575875486381324, "no_speech_prob": 5.1435737987048924e-05}, "257": {"start": 743.18, "end": 744.8199999999999, "text": " And then here, I put 60 characters,", "tokens": [50364, 400, 550, 510, 11, 286, 829, 4060, 4342, 11, 50446], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "258": {"start": 744.8199999999999, "end": 748.9399999999999, "text": " so maybe I just put 100 here,", "tokens": [50446, 370, 1310, 286, 445, 829, 2319, 510, 11, 50652], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "259": {"start": 748.9399999999999, "end": 750.9399999999999, "text": " and we'll try 60 later as well.", "tokens": [50652, 293, 321, 603, 853, 4060, 1780, 382, 731, 13, 50752], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "260": {"start": 750.9399999999999, "end": 752.42, "text": " So let's run that.", "tokens": [50752, 407, 718, 311, 1190, 300, 13, 50826], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "261": {"start": 752.42, "end": 755.02, "text": " All right, so you can see now it's listening.", "tokens": [50826, 1057, 558, 11, 370, 291, 393, 536, 586, 309, 311, 4764, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "262": {"start": 755.02, "end": 757.8199999999999, "text": " So we said 100 characters here, didn't really work,", "tokens": [50956, 407, 321, 848, 2319, 4342, 510, 11, 994, 380, 534, 589, 11, 51096], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "263": {"start": 757.8199999999999, "end": 761.62, "text": " but then we did, we've also added it into this user", "tokens": [51096, 457, 550, 321, 630, 11, 321, 600, 611, 3869, 309, 666, 341, 4195, 51286], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "264": {"start": 761.62, "end": 763.18, "text": " or human message here,", "tokens": [51286, 420, 1952, 3636, 510, 11, 51364], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "265": {"start": 763.18, "end": 765.4599999999999, "text": " and now it's sticking to that, right?", "tokens": [51364, 293, 586, 309, 311, 13465, 281, 300, 11, 558, 30, 51478], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "266": {"start": 765.4599999999999, "end": 767.2199999999999, "text": " So length is good, let's keep going,", "tokens": [51478, 407, 4641, 307, 665, 11, 718, 311, 1066, 516, 11, 51566], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "267": {"start": 767.2199999999999, "end": 770.02, "text": " and we also have this signed off with botrub.", "tokens": [51566, 293, 321, 611, 362, 341, 8175, 766, 365, 10592, 81, 836, 13, 51706], "temperature": 0.0, "avg_logprob": -0.14156527886023887, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.00029130157781764865}, "268": {"start": 770.02, "end": 771.9, "text": " So that is working.", "tokens": [50364, 407, 300, 307, 1364, 13, 50458], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "269": {"start": 771.9, "end": 774.86, "text": " By adding those instructions into the user message,", "tokens": [50458, 3146, 5127, 729, 9415, 666, 264, 4195, 3636, 11, 50606], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "270": {"start": 774.86, "end": 776.22, "text": " we're getting better results.", "tokens": [50606, 321, 434, 1242, 1101, 3542, 13, 50674], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "271": {"start": 776.22, "end": 777.54, "text": " Okay, cool.", "tokens": [50674, 1033, 11, 1627, 13, 50740], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "272": {"start": 777.54, "end": 778.78, "text": " In my last attempt,", "tokens": [50740, 682, 452, 1036, 5217, 11, 50802], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "273": {"start": 778.78, "end": 781.98, "text": " I actually got slightly over the character limit apparently.", "tokens": [50802, 286, 767, 658, 4748, 670, 264, 2517, 4948, 7970, 13, 50962], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "274": {"start": 781.98, "end": 783.8199999999999, "text": " So I mean, we can run this again,", "tokens": [50962, 407, 286, 914, 11, 321, 393, 1190, 341, 797, 11, 51054], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "275": {"start": 783.8199999999999, "end": 787.02, "text": " and okay, so we've set the time to zero here,", "tokens": [51054, 293, 1392, 11, 370, 321, 600, 992, 264, 565, 281, 4018, 510, 11, 51214], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "276": {"start": 787.02, "end": 788.34, "text": " and because of that,", "tokens": [51214, 293, 570, 295, 300, 11, 51280], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "277": {"start": 788.34, "end": 791.6999999999999, "text": " we would expect the output to be the same every single time,", "tokens": [51280, 321, 576, 2066, 264, 5598, 281, 312, 264, 912, 633, 2167, 565, 11, 51448], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "278": {"start": 791.6999999999999, "end": 793.1, "text": " so it's deterministic.", "tokens": [51448, 370, 309, 311, 15957, 3142, 13, 51518], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "279": {"start": 793.1, "end": 795.54, "text": " So quantum physics is very small scale,", "tokens": [51518, 407, 13018, 10649, 307, 588, 1359, 4373, 11, 51640], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "280": {"start": 795.54, "end": 797.9399999999999, "text": " I think it's every time it's outputting the same.", "tokens": [51640, 286, 519, 309, 311, 633, 565, 309, 311, 5598, 783, 264, 912, 13, 51760], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "281": {"start": 797.9399999999999, "end": 798.78, "text": " Okay, cool.", "tokens": [51760, 1033, 11, 1627, 13, 51802], "temperature": 0.0, "avg_logprob": -0.14211988787279062, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.0007551517337560654}, "282": {"start": 798.78, "end": 801.98, "text": " And then let's continue with this.", "tokens": [50364, 400, 550, 718, 311, 2354, 365, 341, 13, 50524], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "283": {"start": 801.98, "end": 803.4599999999999, "text": " So I want to show you,", "tokens": [50524, 407, 286, 528, 281, 855, 291, 11, 50598], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "284": {"start": 803.4599999999999, "end": 805.98, "text": " we can use this prompt templating method", "tokens": [50598, 321, 393, 764, 341, 12391, 9100, 990, 3170, 50724], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "285": {"start": 805.98, "end": 808.62, "text": " in order to build a initial set of messages", "tokens": [50724, 294, 1668, 281, 1322, 257, 5883, 992, 295, 7897, 50856], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "286": {"start": 808.62, "end": 812.06, "text": " that we can basically use as like examples,", "tokens": [50856, 300, 321, 393, 1936, 764, 382, 411, 5110, 11, 51028], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "287": {"start": 812.06, "end": 816.02, "text": " like few shot training for our chat model.", "tokens": [51028, 411, 1326, 3347, 3097, 337, 527, 5081, 2316, 13, 51226], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "288": {"start": 816.02, "end": 818.54, "text": " So what we can do like here,", "tokens": [51226, 407, 437, 321, 393, 360, 411, 510, 11, 51352], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "289": {"start": 818.54, "end": 822.06, "text": " we've done a hundred characters, right?", "tokens": [51352, 321, 600, 1096, 257, 3262, 4342, 11, 558, 30, 51528], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "290": {"start": 822.06, "end": 824.1, "text": " Maybe we can go even lower,", "tokens": [51528, 2704, 321, 393, 352, 754, 3126, 11, 51630], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "291": {"start": 824.1, "end": 825.4599999999999, "text": " but maybe in that case,", "tokens": [51630, 457, 1310, 294, 300, 1389, 11, 51698], "temperature": 0.0, "avg_logprob": -0.08150953405043658, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.00020986171148251742}, "292": {"start": 825.46, "end": 829.38, "text": " we might need to give some examples to the system, right?", "tokens": [50364, 321, 1062, 643, 281, 976, 512, 5110, 281, 264, 1185, 11, 558, 30, 50560], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "293": {"start": 829.38, "end": 831.26, "text": " So let's do that.", "tokens": [50560, 407, 718, 311, 360, 300, 13, 50654], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "294": {"start": 831.26, "end": 833.9000000000001, "text": " We're going to have this character limit,", "tokens": [50654, 492, 434, 516, 281, 362, 341, 2517, 4948, 11, 50786], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "295": {"start": 833.9000000000001, "end": 837.34, "text": " and we're going to have this sign off inputs or variables.", "tokens": [50786, 293, 321, 434, 516, 281, 362, 341, 1465, 766, 15743, 420, 9102, 13, 50958], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "296": {"start": 837.34, "end": 838.5400000000001, "text": " For the human message,", "tokens": [50958, 1171, 264, 1952, 3636, 11, 51018], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "297": {"start": 838.5400000000001, "end": 843.5400000000001, "text": " we're just going to pass in the input there, right?", "tokens": [51018, 321, 434, 445, 516, 281, 1320, 294, 264, 4846, 456, 11, 558, 30, 51268], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "298": {"start": 843.74, "end": 844.74, "text": " So for this first,", "tokens": [51278, 407, 337, 341, 700, 11, 51328], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "299": {"start": 844.74, "end": 846.9000000000001, "text": " we're not going to pass in those instructions,", "tokens": [51328, 321, 434, 406, 516, 281, 1320, 294, 729, 9415, 11, 51436], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "300": {"start": 846.9000000000001, "end": 849.38, "text": " because we're actually going to create this human message,", "tokens": [51436, 570, 321, 434, 767, 516, 281, 1884, 341, 1952, 3636, 11, 51560], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "301": {"start": 849.38, "end": 851.58, "text": " and we're also going to create following AI message", "tokens": [51560, 293, 321, 434, 611, 516, 281, 1884, 3480, 7318, 3636, 51670], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "302": {"start": 851.58, "end": 854.6600000000001, "text": " as an example to the chat bot", "tokens": [51670, 382, 364, 1365, 281, 264, 5081, 10592, 51824], "temperature": 0.0, "avg_logprob": -0.12128528716072204, "compression_ratio": 1.9324894514767932, "no_speech_prob": 0.007112193387001753}, "303": {"start": 854.66, "end": 858.2199999999999, "text": " as to how it should respond, okay?", "tokens": [50364, 382, 281, 577, 309, 820, 4196, 11, 1392, 30, 50542], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "304": {"start": 858.2199999999999, "end": 859.98, "text": " And we put all of these together.", "tokens": [50542, 400, 321, 829, 439, 295, 613, 1214, 13, 50630], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "305": {"start": 859.98, "end": 862.26, "text": " So we have the system template,", "tokens": [50630, 407, 321, 362, 264, 1185, 12379, 11, 50744], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "306": {"start": 862.26, "end": 864.54, "text": " the human template, and the AI template.", "tokens": [50744, 264, 1952, 12379, 11, 293, 264, 7318, 12379, 13, 50858], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "307": {"start": 864.54, "end": 867.02, "text": " Like know that we're using AI message prompt template,", "tokens": [50858, 1743, 458, 300, 321, 434, 1228, 7318, 3636, 12391, 12379, 11, 50982], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "308": {"start": 867.02, "end": 868.3, "text": " human message prompt template,", "tokens": [50982, 1952, 3636, 12391, 12379, 11, 51046], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "309": {"start": 868.3, "end": 870.98, "text": " and system message prompt template for each of those.", "tokens": [51046, 293, 1185, 3636, 12391, 12379, 337, 1184, 295, 729, 13, 51180], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "310": {"start": 870.98, "end": 874.8199999999999, "text": " And what we do is create a list of messages.", "tokens": [51180, 400, 437, 321, 360, 307, 1884, 257, 1329, 295, 7897, 13, 51372], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "311": {"start": 874.8199999999999, "end": 876.8199999999999, "text": " So it goes off to the system message first,", "tokens": [51372, 407, 309, 1709, 766, 281, 264, 1185, 3636, 700, 11, 51472], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "312": {"start": 876.8199999999999, "end": 878.54, "text": " the human message for second,", "tokens": [51472, 264, 1952, 3636, 337, 1150, 11, 51558], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "313": {"start": 878.54, "end": 880.8199999999999, "text": " and the AI message third.", "tokens": [51558, 293, 264, 7318, 3636, 2636, 13, 51672], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "314": {"start": 880.8199999999999, "end": 883.22, "text": " And these are the templates, right?", "tokens": [51672, 400, 613, 366, 264, 21165, 11, 558, 30, 51792], "temperature": 0.0, "avg_logprob": -0.11736528457157196, "compression_ratio": 2.008695652173913, "no_speech_prob": 0.0003405149618629366}, "315": {"start": 883.22, "end": 886.26, "text": " So what we then do is we take our chat prompt,", "tokens": [50364, 407, 437, 321, 550, 360, 307, 321, 747, 527, 5081, 12391, 11, 50516], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "316": {"start": 886.26, "end": 887.98, "text": " which is a list of these,", "tokens": [50516, 597, 307, 257, 1329, 295, 613, 11, 50602], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "317": {"start": 887.98, "end": 890.78, "text": " and we format that prompt with our inputs.", "tokens": [50602, 293, 321, 7877, 300, 12391, 365, 527, 15743, 13, 50742], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "318": {"start": 890.78, "end": 892.14, "text": " So we have the character limit,", "tokens": [50742, 407, 321, 362, 264, 2517, 4948, 11, 50810], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "319": {"start": 892.14, "end": 893.46, "text": " which we're going to set to 50,", "tokens": [50810, 597, 321, 434, 516, 281, 992, 281, 2625, 11, 50876], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "320": {"start": 893.46, "end": 896.46, "text": " so half of what we had before, making it harder.", "tokens": [50876, 370, 1922, 295, 437, 321, 632, 949, 11, 1455, 309, 6081, 13, 51026], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "321": {"start": 896.46, "end": 897.3000000000001, "text": " We're going to say the sign off", "tokens": [51026, 492, 434, 516, 281, 584, 264, 1465, 766, 51068], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "322": {"start": 897.3000000000001, "end": 900.1800000000001, "text": " has to be this robot robot,", "tokens": [51068, 575, 281, 312, 341, 7881, 7881, 11, 51212], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "323": {"start": 900.1800000000001, "end": 904.22, "text": " and the input is going to be the same as before.", "tokens": [51212, 293, 264, 4846, 307, 516, 281, 312, 264, 912, 382, 949, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "324": {"start": 904.22, "end": 907.22, "text": " And then we're giving an example response, right?", "tokens": [51414, 400, 550, 321, 434, 2902, 364, 1365, 4134, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "325": {"start": 907.22, "end": 909.82, "text": " So good is physics, small things.", "tokens": [51564, 407, 665, 307, 10649, 11, 1359, 721, 13, 51694], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "326": {"start": 909.82, "end": 913.14, "text": " That example response is going to automatically", "tokens": [51694, 663, 1365, 4134, 307, 516, 281, 6772, 51860], "temperature": 0.0, "avg_logprob": -0.14521831936306423, "compression_ratio": 1.817829457364341, "no_speech_prob": 4.2642455809982494e-05}, "327": {"start": 914.06, "end": 914.9, "text": " have the sign off added to it.", "tokens": [50410, 362, 264, 1465, 766, 3869, 281, 309, 13, 50452], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "328": {"start": 914.9, "end": 917.06, "text": " All right, so let's run this.", "tokens": [50452, 1057, 558, 11, 370, 718, 311, 1190, 341, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "329": {"start": 917.06, "end": 917.9, "text": " Let's see what we get.", "tokens": [50560, 961, 311, 536, 437, 321, 483, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "330": {"start": 917.9, "end": 921.1, "text": " So system message, you are helpful assistant.", "tokens": [50602, 407, 1185, 3636, 11, 291, 366, 4961, 10994, 13, 50762], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "331": {"start": 921.1, "end": 924.14, "text": " You keep responses, no one's 50 characters long.", "tokens": [50762, 509, 1066, 13019, 11, 572, 472, 311, 2625, 4342, 938, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "332": {"start": 924.14, "end": 926.78, "text": " You sign off every message with robot robot,", "tokens": [50914, 509, 1465, 766, 633, 3636, 365, 7881, 7881, 11, 51046], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "333": {"start": 926.78, "end": 930.26, "text": " so we can see where those are being added there.", "tokens": [51046, 370, 321, 393, 536, 689, 729, 366, 885, 3869, 456, 13, 51220], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "334": {"start": 930.26, "end": 933.1, "text": " Human message, how yeah, AI, how you,", "tokens": [51220, 10294, 3636, 11, 577, 1338, 11, 7318, 11, 577, 291, 11, 51362], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "335": {"start": 933.1, "end": 934.38, "text": " what is quantum physics?", "tokens": [51362, 437, 307, 13018, 10649, 30, 51426], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "336": {"start": 934.38, "end": 935.54, "text": " So it's, you know,", "tokens": [51426, 407, 309, 311, 11, 291, 458, 11, 51484], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "337": {"start": 935.54, "end": 937.8199999999999, "text": " because we're just passing the input in there.", "tokens": [51484, 570, 321, 434, 445, 8437, 264, 4846, 294, 456, 13, 51598], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "338": {"start": 937.8199999999999, "end": 939.74, "text": " And then we have the AI message, good.", "tokens": [51598, 400, 550, 321, 362, 264, 7318, 3636, 11, 665, 13, 51694], "temperature": 0.0, "avg_logprob": -0.2550339662939086, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.9410788303939626e-05}, "339": {"start": 939.74, "end": 944.74, "text": " It's physics of small things, robot robot, okay?", "tokens": [50364, 467, 311, 10649, 295, 1359, 721, 11, 7881, 7881, 11, 1392, 30, 50614], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "340": {"start": 944.94, "end": 945.9, "text": " Very short answer.", "tokens": [50624, 4372, 2099, 1867, 13, 50672], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "341": {"start": 945.9, "end": 950.34, "text": " And let's just see if that helps the system", "tokens": [50672, 400, 718, 311, 445, 536, 498, 300, 3665, 264, 1185, 50894], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "342": {"start": 950.34, "end": 952.54, "text": " produce just very short answers.", "tokens": [50894, 5258, 445, 588, 2099, 6338, 13, 51004], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "343": {"start": 952.54, "end": 957.54, "text": " So we run this and we get atoms, electrons, photons,", "tokens": [51004, 407, 321, 1190, 341, 293, 321, 483, 16871, 11, 14265, 11, 40209, 11, 51254], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "344": {"start": 957.66, "end": 958.82, "text": " and then it does a sign off.", "tokens": [51260, 293, 550, 309, 775, 257, 1465, 766, 13, 51318], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "345": {"start": 958.82, "end": 961.22, "text": " So I think that's a pretty good response.", "tokens": [51318, 407, 286, 519, 300, 311, 257, 1238, 665, 4134, 13, 51438], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "346": {"start": 961.22, "end": 963.26, "text": " Let's try again, right?", "tokens": [51438, 961, 311, 853, 797, 11, 558, 30, 51540], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "347": {"start": 963.26, "end": 965.86, "text": " So here we go slightly over.", "tokens": [51540, 407, 510, 321, 352, 4748, 670, 13, 51670], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "348": {"start": 965.86, "end": 969.42, "text": " So we get like four characters over there.", "tokens": [51670, 407, 321, 483, 411, 1451, 4342, 670, 456, 13, 51848], "temperature": 0.0, "avg_logprob": -0.15128423549510814, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.002114634495228529}, "349": {"start": 969.42, "end": 971.78, "text": " So maybe we can be more strict again.", "tokens": [50364, 407, 1310, 321, 393, 312, 544, 10910, 797, 13, 50482], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "350": {"start": 971.78, "end": 976.3399999999999, "text": " So what we can do is we add in that template", "tokens": [50482, 407, 437, 321, 393, 360, 307, 321, 909, 294, 300, 12379, 50710], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "351": {"start": 976.3399999999999, "end": 978.9, "text": " that we used before where we add in the answer", "tokens": [50710, 300, 321, 1143, 949, 689, 321, 909, 294, 264, 1867, 50838], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "352": {"start": 978.9, "end": 983.0999999999999, "text": " in less than the character limit, including white space.", "tokens": [50838, 294, 1570, 813, 264, 2517, 4948, 11, 3009, 2418, 1901, 13, 51048], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "353": {"start": 983.0999999999999, "end": 987.02, "text": " Okay, we're going to add that to our human message.", "tokens": [51048, 1033, 11, 321, 434, 516, 281, 909, 300, 281, 527, 1952, 3636, 13, 51244], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "354": {"start": 987.02, "end": 990.02, "text": " So we're gonna create the human message like this.", "tokens": [51244, 407, 321, 434, 799, 1884, 264, 1952, 3636, 411, 341, 13, 51394], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "355": {"start": 990.02, "end": 993.5799999999999, "text": " So the chat prompt template and so on and so on.", "tokens": [51394, 407, 264, 5081, 12391, 12379, 293, 370, 322, 293, 370, 322, 13, 51572], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "356": {"start": 993.5799999999999, "end": 994.42, "text": " Okay, cool.", "tokens": [51572, 1033, 11, 1627, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "357": {"start": 994.42, "end": 997.26, "text": " So is it like particle physics as far as before?", "tokens": [51614, 407, 307, 309, 411, 12359, 10649, 382, 1400, 382, 949, 30, 51756], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "358": {"start": 997.26, "end": 998.0999999999999, "text": " Yeah.", "tokens": [51756, 865, 13, 51798], "temperature": 0.0, "avg_logprob": -0.11094532341792665, "compression_ratio": 1.7920353982300885, "no_speech_prob": 6.301607209024951e-05}, "359": {"start": 998.1, "end": 998.94, "text": " So we're asking the same question,", "tokens": [50364, 407, 321, 434, 3365, 264, 912, 1168, 11, 50406], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "360": {"start": 998.94, "end": 1001.02, "text": " but we're adding that on to the end.", "tokens": [50406, 457, 321, 434, 5127, 300, 322, 281, 264, 917, 13, 50510], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "361": {"start": 1001.02, "end": 1003.46, "text": " So is it like particle physics answering less than 50", "tokens": [50510, 407, 307, 309, 411, 12359, 10649, 13430, 1570, 813, 2625, 50632], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "362": {"start": 1003.46, "end": 1005.1800000000001, "text": " characters, including white space?", "tokens": [50632, 4342, 11, 3009, 2418, 1901, 30, 50718], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "363": {"start": 1005.1800000000001, "end": 1008.5, "text": " Then what I'm going to do is so within the messages", "tokens": [50718, 1396, 437, 286, 478, 516, 281, 360, 307, 370, 1951, 264, 7897, 50884], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "364": {"start": 1008.5, "end": 1012.82, "text": " right now, we have this query that we created before", "tokens": [50884, 558, 586, 11, 321, 362, 341, 14581, 300, 321, 2942, 949, 51100], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "365": {"start": 1012.82, "end": 1014.46, "text": " where we need to replace that query", "tokens": [51100, 689, 321, 643, 281, 7406, 300, 14581, 51182], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "366": {"start": 1014.46, "end": 1016.5400000000001, "text": " with our new modified query.", "tokens": [51182, 365, 527, 777, 15873, 14581, 13, 51286], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "367": {"start": 1016.5400000000001, "end": 1020.66, "text": " So I'm going to remove the most recent message in messages.", "tokens": [51286, 407, 286, 478, 516, 281, 4159, 264, 881, 5162, 3636, 294, 7897, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "368": {"start": 1020.66, "end": 1024.3, "text": " And now I'm going to send it with this new human prompt", "tokens": [51492, 400, 586, 286, 478, 516, 281, 2845, 309, 365, 341, 777, 1952, 12391, 51674], "temperature": 0.0, "avg_logprob": -0.1642158637612553, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.00035691820085048676}, "369": {"start": 1024.34, "end": 1028.6599999999999, "text": " value, which is this kind of new version", "tokens": [50366, 2158, 11, 597, 307, 341, 733, 295, 777, 3037, 50582], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "370": {"start": 1028.6599999999999, "end": 1030.62, "text": " with those instructions added to the end.", "tokens": [50582, 365, 729, 9415, 3869, 281, 264, 917, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "371": {"start": 1030.62, "end": 1033.22, "text": " So let's have a look, make sure we have the right form.", "tokens": [50680, 407, 718, 311, 362, 257, 574, 11, 652, 988, 321, 362, 264, 558, 1254, 13, 50810], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "372": {"start": 1033.22, "end": 1036.7, "text": " So system, human, AI, human, AI.", "tokens": [50810, 407, 1185, 11, 1952, 11, 7318, 11, 1952, 11, 7318, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "373": {"start": 1036.7, "end": 1039.34, "text": " That's the last correct response we got from the AI.", "tokens": [50984, 663, 311, 264, 1036, 3006, 4134, 321, 658, 490, 264, 7318, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "374": {"start": 1039.34, "end": 1042.26, "text": " And now we have the new modified human message.", "tokens": [51116, 400, 586, 321, 362, 264, 777, 15873, 1952, 3636, 13, 51262], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "375": {"start": 1042.26, "end": 1043.1, "text": " Okay, cool.", "tokens": [51262, 1033, 11, 1627, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "376": {"start": 1043.1, "end": 1046.26, "text": " So is it like this answering less than 50 characters?", "tokens": [51304, 407, 307, 309, 411, 341, 13430, 1570, 813, 2625, 4342, 30, 51462], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "377": {"start": 1046.26, "end": 1050.3, "text": " And now we pass that through our chat system again,", "tokens": [51462, 400, 586, 321, 1320, 300, 807, 527, 5081, 1185, 797, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "378": {"start": 1050.3, "end": 1052.94, "text": " and we get way shorter.", "tokens": [51664, 293, 321, 483, 636, 11639, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1454317910330636, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01910032145678997}, "379": {"start": 1052.94, "end": 1055.22, "text": " So 28 is like, yes, similar.", "tokens": [50364, 407, 7562, 307, 411, 11, 2086, 11, 2531, 13, 50478], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "380": {"start": 1055.22, "end": 1058.06, "text": " Because we're saying, we're telling it", "tokens": [50478, 1436, 321, 434, 1566, 11, 321, 434, 3585, 309, 50620], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "381": {"start": 1058.06, "end": 1060.54, "text": " in the most recent query again,", "tokens": [50620, 294, 264, 881, 5162, 14581, 797, 11, 50744], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "382": {"start": 1060.54, "end": 1062.8200000000002, "text": " like you need to answer in less than 50 characters.", "tokens": [50744, 411, 291, 643, 281, 1867, 294, 1570, 813, 2625, 4342, 13, 50858], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "383": {"start": 1062.8200000000002, "end": 1065.8600000000001, "text": " All right, so what I mentioned before is that maybe", "tokens": [50858, 1057, 558, 11, 370, 437, 286, 2835, 949, 307, 300, 1310, 51010], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "384": {"start": 1065.8600000000001, "end": 1067.42, "text": " this is a little bit convoluted.", "tokens": [51010, 341, 307, 257, 707, 857, 3754, 2308, 292, 13, 51088], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "385": {"start": 1067.42, "end": 1071.18, "text": " And that's not to say that there aren't use cases for this.", "tokens": [51088, 400, 300, 311, 406, 281, 584, 300, 456, 3212, 380, 764, 3331, 337, 341, 13, 51276], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "386": {"start": 1071.18, "end": 1075.54, "text": " It's just that it would be unfair of me to tell you", "tokens": [51276, 467, 311, 445, 300, 309, 576, 312, 17019, 295, 385, 281, 980, 291, 51494], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "387": {"start": 1075.54, "end": 1077.66, "text": " all of this and be like, this is how you use it.", "tokens": [51494, 439, 295, 341, 293, 312, 411, 11, 341, 307, 577, 291, 764, 309, 13, 51600], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "388": {"start": 1077.66, "end": 1081.06, "text": " And then like just miss something that could make things", "tokens": [51600, 400, 550, 411, 445, 1713, 746, 300, 727, 652, 721, 51770], "temperature": 0.0, "avg_logprob": -0.1504584679236779, "compression_ratio": 1.6509090909090909, "no_speech_prob": 0.0007316223345696926}, "389": {"start": 1081.06, "end": 1084.46, "text": " much easier in most, at least most use cases", "tokens": [50364, 709, 3571, 294, 881, 11, 412, 1935, 881, 764, 3331, 50534], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "390": {"start": 1084.46, "end": 1087.62, "text": " or simply use cases or something along those lines.", "tokens": [50534, 420, 2935, 764, 3331, 420, 746, 2051, 729, 3876, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "391": {"start": 1087.62, "end": 1089.8999999999999, "text": " Right, so I would say it's arguable as to whether", "tokens": [50692, 1779, 11, 370, 286, 576, 584, 309, 311, 10171, 712, 382, 281, 1968, 50806], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "392": {"start": 1089.8999999999999, "end": 1091.8999999999999, "text": " all of the above that we just did", "tokens": [50806, 439, 295, 264, 3673, 300, 321, 445, 630, 50906], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "393": {"start": 1091.8999999999999, "end": 1094.62, "text": " would be any simpler than using an F string.", "tokens": [50906, 576, 312, 604, 18587, 813, 1228, 364, 479, 6798, 13, 51042], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "394": {"start": 1094.62, "end": 1096.7, "text": " So we have this input.", "tokens": [51042, 407, 321, 362, 341, 4846, 13, 51146], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "395": {"start": 1096.7, "end": 1099.1, "text": " Okay, cool, is it like particle physics?", "tokens": [51146, 1033, 11, 1627, 11, 307, 309, 411, 12359, 10649, 30, 51266], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "396": {"start": 1099.1, "end": 1101.5, "text": " That's our most recent question, right?", "tokens": [51266, 663, 311, 527, 881, 5162, 1168, 11, 558, 30, 51386], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "397": {"start": 1101.5, "end": 1103.06, "text": " And we can just use an F string, right?", "tokens": [51386, 400, 321, 393, 445, 764, 364, 479, 6798, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "398": {"start": 1103.06, "end": 1104.8999999999999, "text": " So we have the F string here,", "tokens": [51464, 407, 321, 362, 264, 479, 6798, 510, 11, 51556], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "399": {"start": 1104.8999999999999, "end": 1108.22, "text": " and we have the human message, the content.", "tokens": [51556, 293, 321, 362, 264, 1952, 3636, 11, 264, 2701, 13, 51722], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "400": {"start": 1108.22, "end": 1110.82, "text": " And then we just say answering less than", "tokens": [51722, 400, 550, 321, 445, 584, 13430, 1570, 813, 51852], "temperature": 0.0, "avg_logprob": -0.1376307645290018, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0028865470085293055}, "401": {"start": 1110.86, "end": 1112.8999999999999, "text": " the character limit, which is set here,", "tokens": [50366, 264, 2517, 4948, 11, 597, 307, 992, 510, 11, 50468], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "402": {"start": 1112.8999999999999, "end": 1115.9399999999998, "text": " characters, including white space, right?", "tokens": [50468, 4342, 11, 3009, 2418, 1901, 11, 558, 30, 50620], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "403": {"start": 1115.9399999999998, "end": 1119.3799999999999, "text": " And the result of that is basically the same", "tokens": [50620, 400, 264, 1874, 295, 300, 307, 1936, 264, 912, 50792], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "404": {"start": 1119.3799999999999, "end": 1121.4199999999998, "text": " that we have this, right?", "tokens": [50792, 300, 321, 362, 341, 11, 558, 30, 50894], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "405": {"start": 1121.4199999999998, "end": 1125.5, "text": " That's the same as all of this code here.", "tokens": [50894, 663, 311, 264, 912, 382, 439, 295, 341, 3089, 510, 13, 51098], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "406": {"start": 1125.5, "end": 1129.62, "text": " So now all of this code, is that right?", "tokens": [51098, 407, 586, 439, 295, 341, 3089, 11, 307, 300, 558, 30, 51304], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "407": {"start": 1129.62, "end": 1132.06, "text": " Yeah, plus this.", "tokens": [51304, 865, 11, 1804, 341, 13, 51426], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "408": {"start": 1132.06, "end": 1135.9399999999998, "text": " So it depends, I don't know, it depends on your use case,", "tokens": [51426, 407, 309, 5946, 11, 286, 500, 380, 458, 11, 309, 5946, 322, 428, 764, 1389, 11, 51620], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "409": {"start": 1135.9399999999998, "end": 1138.34, "text": " like what you're doing, how you prefer to write this.", "tokens": [51620, 411, 437, 291, 434, 884, 11, 577, 291, 4382, 281, 2464, 341, 13, 51740], "temperature": 0.0, "avg_logprob": -0.14830577045405677, "compression_ratio": 1.6962616822429906, "no_speech_prob": 0.00011589897621888667}, "410": {"start": 1138.3799999999999, "end": 1141.1799999999998, "text": " But just be aware that you can also do this", "tokens": [50366, 583, 445, 312, 3650, 300, 291, 393, 611, 360, 341, 50506], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "411": {"start": 1141.1799999999998, "end": 1142.8999999999999, "text": " and you get the same result.", "tokens": [50506, 293, 291, 483, 264, 912, 1874, 13, 50592], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "412": {"start": 1142.8999999999999, "end": 1146.5, "text": " So we can see again, popping the last message", "tokens": [50592, 407, 321, 393, 536, 797, 11, 18374, 264, 1036, 3636, 50772], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "413": {"start": 1146.5, "end": 1150.86, "text": " to remove the one that we created using the prompt template.", "tokens": [50772, 281, 4159, 264, 472, 300, 321, 2942, 1228, 264, 12391, 12379, 13, 50990], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "414": {"start": 1150.86, "end": 1152.3, "text": " And then I'm adding the one that we created", "tokens": [50990, 400, 550, 286, 478, 5127, 264, 472, 300, 321, 2942, 51062], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "415": {"start": 1152.3, "end": 1155.1399999999999, "text": " using the F string approach, and we get this.", "tokens": [51062, 1228, 264, 479, 6798, 3109, 11, 293, 321, 483, 341, 13, 51204], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "416": {"start": 1155.1399999999999, "end": 1156.5, "text": " All right, it's the same thing.", "tokens": [51204, 1057, 558, 11, 309, 311, 264, 912, 551, 13, 51272], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "417": {"start": 1156.5, "end": 1157.98, "text": " There's no difference there.", "tokens": [51272, 821, 311, 572, 2649, 456, 13, 51346], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "418": {"start": 1157.98, "end": 1160.58, "text": " We can process it through chat GPT again,", "tokens": [51346, 492, 393, 1399, 309, 807, 5081, 26039, 51, 797, 11, 51476], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "419": {"start": 1160.58, "end": 1162.78, "text": " and we'll get the same response.", "tokens": [51476, 293, 321, 603, 483, 264, 912, 4134, 13, 51586], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "420": {"start": 1162.78, "end": 1165.6599999999999, "text": " Okay, so just wanted to make you aware of that.", "tokens": [51586, 1033, 11, 370, 445, 1415, 281, 652, 291, 3650, 295, 300, 13, 51730], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "421": {"start": 1165.6599999999999, "end": 1168.26, "text": " But yeah, that's it for this video.", "tokens": [51730, 583, 1338, 11, 300, 311, 309, 337, 341, 960, 13, 51860], "temperature": 0.0, "avg_logprob": -0.15951130914349929, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.0013441999908536673}, "422": {"start": 1169.1, "end": 1170.66, "text": " We've covered, I think the vast majority", "tokens": [50406, 492, 600, 5343, 11, 286, 519, 264, 8369, 6286, 50484], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "423": {"start": 1170.66, "end": 1174.78, "text": " of the new chat features within Limechain.", "tokens": [50484, 295, 264, 777, 5081, 4122, 1951, 441, 1312, 11509, 13, 50690], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "424": {"start": 1174.78, "end": 1177.3799999999999, "text": " And naturally, like we saw at the end there,", "tokens": [50690, 400, 8195, 11, 411, 321, 1866, 412, 264, 917, 456, 11, 50820], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "425": {"start": 1177.3799999999999, "end": 1178.98, "text": " we don't need to use all of them,", "tokens": [50820, 321, 500, 380, 643, 281, 764, 439, 295, 552, 11, 50900], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "426": {"start": 1178.98, "end": 1180.54, "text": " like the prompt templates.", "tokens": [50900, 411, 264, 12391, 21165, 13, 50978], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "427": {"start": 1180.54, "end": 1183.02, "text": " You can use, of course, if you have reason to,", "tokens": [50978, 509, 393, 764, 11, 295, 1164, 11, 498, 291, 362, 1778, 281, 11, 51102], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "428": {"start": 1183.02, "end": 1186.14, "text": " but it isn't needed if you have a simpler approach", "tokens": [51102, 457, 309, 1943, 380, 2978, 498, 291, 362, 257, 18587, 3109, 51258], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "429": {"start": 1186.14, "end": 1187.5, "text": " to doing these things.", "tokens": [51258, 281, 884, 613, 721, 13, 51326], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "430": {"start": 1187.5, "end": 1191.98, "text": " But yeah, it's cool to see this being implemented", "tokens": [51326, 583, 1338, 11, 309, 311, 1627, 281, 536, 341, 885, 12270, 51550], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "431": {"start": 1191.98, "end": 1194.86, "text": " in Limechain, and although I haven't been through it yet,", "tokens": [51550, 294, 441, 1312, 11509, 11, 293, 4878, 286, 2378, 380, 668, 807, 309, 1939, 11, 51694], "temperature": 0.0, "avg_logprob": -0.16772332738657467, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0001970092998817563}, "432": {"start": 1194.86, "end": 1198.8999999999999, "text": " I'm hoping that there will be good integrations", "tokens": [50364, 286, 478, 7159, 300, 456, 486, 312, 665, 3572, 763, 50566], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "433": {"start": 1198.8999999999999, "end": 1201.82, "text": " of these new chat features with like", "tokens": [50566, 295, 613, 777, 5081, 4122, 365, 411, 50712], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "434": {"start": 1201.82, "end": 1204.5, "text": " their conversation memory, their retrieval augmentation,", "tokens": [50712, 641, 3761, 4675, 11, 641, 19817, 3337, 14501, 19631, 11, 50846], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "435": {"start": 1204.5, "end": 1206.26, "text": " and everything else within Limechain,", "tokens": [50846, 293, 1203, 1646, 1951, 441, 1312, 11509, 11, 50934], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "436": {"start": 1206.26, "end": 1208.62, "text": " which is that that's where the value", "tokens": [50934, 597, 307, 300, 300, 311, 689, 264, 2158, 51052], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "437": {"start": 1208.62, "end": 1209.82, "text": " of this sort of thing will come in.", "tokens": [51052, 295, 341, 1333, 295, 551, 486, 808, 294, 13, 51112], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "438": {"start": 1209.82, "end": 1211.82, "text": " Right now, it's kind of like a simple wrapper", "tokens": [51112, 1779, 586, 11, 309, 311, 733, 295, 411, 257, 2199, 46906, 51212], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "439": {"start": 1211.82, "end": 1215.3, "text": " on top of OpenAI's chat completion endpoint,", "tokens": [51212, 322, 1192, 295, 7238, 48698, 311, 5081, 19372, 35795, 11, 51386], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "440": {"start": 1215.3, "end": 1220.26, "text": " but hopefully with all of the agent's conversation memory", "tokens": [51386, 457, 4696, 365, 439, 295, 264, 9461, 311, 3761, 4675, 51634], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "441": {"start": 1220.26, "end": 1223.6599999999999, "text": " and retrieval augmentation components that Limechain offers,", "tokens": [51634, 293, 19817, 3337, 14501, 19631, 6677, 300, 441, 1312, 11509, 7736, 11, 51804], "temperature": 0.0, "avg_logprob": -0.14149706587832198, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.007563426624983549}, "442": {"start": 1223.66, "end": 1226.02, "text": " we'll get a tight integration between those,", "tokens": [50364, 321, 603, 483, 257, 4524, 10980, 1296, 729, 11, 50482], "temperature": 0.0, "avg_logprob": -0.14264678955078125, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0002990570792462677}, "443": {"start": 1226.02, "end": 1227.94, "text": " and that's why this will be useful.", "tokens": [50482, 293, 300, 311, 983, 341, 486, 312, 4420, 13, 50578], "temperature": 0.0, "avg_logprob": -0.14264678955078125, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0002990570792462677}, "444": {"start": 1227.94, "end": 1231.5, "text": " So that's it for this video.", "tokens": [50578, 407, 300, 311, 309, 337, 341, 960, 13, 50756], "temperature": 0.0, "avg_logprob": -0.14264678955078125, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0002990570792462677}, "445": {"start": 1231.5, "end": 1233.94, "text": " I hope all of this has been useful and interesting.", "tokens": [50756, 286, 1454, 439, 295, 341, 575, 668, 4420, 293, 1880, 13, 50878], "temperature": 0.0, "avg_logprob": -0.14264678955078125, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0002990570792462677}, "446": {"start": 1233.94, "end": 1236.3400000000001, "text": " But for now, thank you very much for watching,", "tokens": [50878, 583, 337, 586, 11, 1309, 291, 588, 709, 337, 1976, 11, 50998], "temperature": 0.0, "avg_logprob": -0.14264678955078125, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0002990570792462677}, "447": {"start": 1236.3400000000001, "end": 1238.9, "text": " and I will see you again in the next one.", "tokens": [50998, 293, 286, 486, 536, 291, 797, 294, 264, 958, 472, 13, 51126], "temperature": 0.0, "avg_logprob": -0.14264678955078125, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0002990570792462677}, "448": {"start": 1238.9, "end": 1239.74, "text": " Bye.", "tokens": [51126, 4621, 13, 51168], "temperature": 0.0, "avg_logprob": -0.14264678955078125, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0002990570792462677}}